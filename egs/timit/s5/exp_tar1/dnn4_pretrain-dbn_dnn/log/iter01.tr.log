nsclab-gpu
nnet-train-frmshuff --cross-validate=false --randomize=true --verbose=0 --minibatch-size=256 --randomizer-size=32768 --randomizer-seed=777 --learn-rate=0.008 --momentum=0 --l1-penalty=0 --l2-penalty=0 --feature-transform=exp/dnn4_pretrain-dbn_dnn/final.feature_transform 'ark:copy-feats scp:exp/dnn4_pretrain-dbn_dnn/train.scp ark:- |' 'ark:ali-to-pdf exp/tri3_ali/final.mdl "ark:gunzip -c exp/tri3_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- |' exp/dnn4_pretrain-dbn_dnn/nnet_dbn_dnn.init exp/dnn4_pretrain-dbn_dnn/nnet/nnet_dbn_dnn_iter01 
WARNING (nnet-train-frmshuff[5.5.294-06484]:SelectGpuId():cu-device.cc:221) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:349) Selecting from 2 GPUs
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:364) cudaSetDevice(0): TITAN Xp	free:12023M, used:173M, total:12196M, free/total:0.985815
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:364) cudaSetDevice(1): TITAN Xp	free:11887M, used:305M, total:12192M, free/total:0.974986
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:411) Trying to select device: 0 (automatically), mem_ratio: 0.985815
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:430) Success selecting device 0 free mem ratio: 0.985815
LOG (nnet-train-frmshuff[5.5.294-06484]:FinalizeActiveGpu():cu-device.cc:284) The active GPU is [0]: TITAN Xp	free:11957M, used:239M, total:12196M, free/total:0.980403 version 6.1
copy-feats scp:exp/dnn4_pretrain-dbn_dnn/train.scp ark:- 
LOG (nnet-train-frmshuff[5.5.294-06484]:Init():nnet-randomizer.cc:32) Seeding by srand with : 777
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:163) TRAINING STARTED
ali-to-post ark:- ark:- 
ali-to-pdf exp/tri3_ali/final.mdl 'ark:gunzip -c exp/tri3_ali/ali.*.gz |' ark:- 
LOG (ali-to-pdf[5.5.294-06484]:main():ali-to-pdf.cc:68) Converted 1232 alignments to pdf sequences.
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:354) ### After 0 frames,
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:355) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -7.2475, max 7.77538, mean -0.00464958, stddev 0.983373, skewness 0.0162194, kurtosis 1.92828 ) 
[1] output of <AffineTransform> ( min -20.9443, max 10.6867, mean -3.19669, stddev 2.30749, skewness 0.163396, kurtosis 1.89454 ) 
[2] output of <Sigmoid> ( min 8.01691e-10, max 0.999977, mean 0.128511, stddev 0.217238, skewness 2.40099, kurtosis 5.14564 ) 
[3] output of <AffineTransform> ( min -22.2279, max 15.1594, mean -3.58254, stddev 2.33118, skewness -0.0398662, kurtosis 3.04026 ) 
[4] output of <Sigmoid> ( min 2.22108e-10, max 1, mean 0.0971144, stddev 0.182579, skewness 3.10966, kurtosis 9.84772 ) 
[5] output of <AffineTransform> ( min -14.8363, max 11.7586, mean -3.44504, stddev 1.95095, skewness 0.769257, kurtosis 2.78233 ) 
[6] output of <Sigmoid> ( min 3.60319e-07, max 0.999992, mean 0.0926256, stddev 0.181586, skewness 3.2483, kurtosis 10.6549 ) 
[7] output of <AffineTransform> ( min -20.2356, max 12.4, mean -3.65322, stddev 1.96572, skewness 0.713704, kurtosis 4.63417 ) 
[8] output of <Sigmoid> ( min 1.62853e-09, max 0.999996, mean 0.0794729, stddev 0.170712, skewness 3.64285, kurtosis 13.5843 ) 
[9] output of <AffineTransform> ( min -14.649, max 11.9136, mean -3.77271, stddev 1.79168, skewness 1.73148, kurtosis 6.08882 ) 
[10] output of <Sigmoid> ( min 4.34517e-07, max 0.999993, mean 0.0714849, stddev 0.175439, skewness 3.83222, kurtosis 14.5516 ) 
[11] output of <AffineTransform> ( min -18.2768, max 11.9432, mean -3.87977, stddev 1.83008, skewness 1.37501, kurtosis 7.33025 ) 
[12] output of <Sigmoid> ( min 1.15476e-08, max 0.999994, mean 0.0649632, stddev 0.165639, skewness 4.1321, kurtosis 17.1577 ) 
[13] output of <AffineTransform> ( min -2.88451, max 3.36018, mean -0.00463055, stddev 0.551117, skewness 0.0121988, kurtosis 0.323953 ) 
[14] output of <Softmax> ( min 2.79982e-05, max 0.0137095, mean 0.000625003, stddev 0.000377305, skewness 2.43453, kurtosis 15.4236 ) 
### END FORWARD

LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:357) 
### BACKWARD PROPAGATION BUFFER CONTENT :
[0] diff of <Input>  ( min -0.12145, max 0.110637, mean -0.000115566, stddev 0.00673531, skewness -0.185809, kurtosis 13.5776 ) 
[1] diff-output of <AffineTransform> ( min -0.0571138, max 0.0514321, mean 5.68628e-05, stddev 0.00183588, skewness 1.13119, kurtosis 49.3182 ) 
[2] diff-output of <Sigmoid> ( min -0.303665, max 0.232118, mean -4.83828e-05, stddev 0.014648, skewness 0.0512729, kurtosis 8.88262 ) 
[3] diff-output of <AffineTransform> ( min -0.0605039, max 0.0530709, mean 4.78241e-05, stddev 0.00176933, skewness 1.03868, kurtosis 54.6824 ) 
[4] diff-output of <Sigmoid> ( min -0.242232, max 0.237179, mean -0.000296806, stddev 0.016198, skewness 0.0256411, kurtosis 5.48414 ) 
[5] diff-output of <AffineTransform> ( min -0.0533427, max 0.0464259, mean 4.83016e-05, stddev 0.00178785, skewness 0.984534, kurtosis 45.9636 ) 
[6] diff-output of <Sigmoid> ( min -0.222823, max 0.187679, mean -0.000221276, stddev 0.0157896, skewness 0.0540522, kurtosis 4.47846 ) 
[7] diff-output of <AffineTransform> ( min -0.0342069, max 0.0429927, mean 4.71009e-05, stddev 0.00187954, skewness 0.914593, kurtosis 41.1638 ) 
[8] diff-output of <Sigmoid> ( min -0.170574, max 0.193208, mean -0.000247107, stddev 0.017607, skewness 0.0630503, kurtosis 3.6106 ) 
[9] diff-output of <AffineTransform> ( min -0.0428526, max 0.0507962, mean 4.23783e-05, stddev 0.00217655, skewness 0.751255, kurtosis 36.2662 ) 
[10] diff-output of <Sigmoid> ( min -0.263961, max 0.20809, mean -0.000362575, stddev 0.0236695, skewness 0.0328185, kurtosis 3.19099 ) 
[11] diff-output of <AffineTransform> ( min -0.0777144, max 0.0837751, mean 6.59045e-05, stddev 0.0058469, skewness 0.235584, kurtosis 27.0473 ) 
[12] diff-output of <Sigmoid> ( min -0.437458, max 0.476127, mean 0.000590614, stddev 0.0967934, skewness 0.00101688, kurtosis 0.0240395 ) 
[13] diff-output of <AffineTransform> ( min -0.999872, max 0.0137095, mean 2.81725e-10, stddev 0.0249902, skewness -39.9717, kurtosis 1595.49 ) 
[14] diff-output of <Softmax> ( min -0.999872, max 0.0137095, mean 2.81725e-10, stddev 0.0249902, skewness -39.9717, kurtosis 1595.49 ) 
### END BACKWARD


LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:358) 
### GRADIENT STATS :
Component 1 : <AffineTransform>, 
  linearity_grad ( min -0.246599, max 0.273867, mean 0.000266384, stddev 0.0306539, skewness 0.0215614, kurtosis 1.82991 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.163224, max 0.144094, mean 0.0145568, stddev 0.0353225, skewness 0.170401, kurtosis 1.32567 ) , lr-coef 1
Component 2 : <Sigmoid>, 
Component 3 : <AffineTransform>, 
  linearity_grad ( min -0.129093, max 0.106184, mean 0.00158749, stddev 0.00800653, skewness 0.491427, kurtosis 6.60239 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.160515, max 0.142136, mean 0.012243, stddev 0.0337879, skewness -0.0655219, kurtosis 1.62238 ) , lr-coef 1
Component 4 : <Sigmoid>, 
Component 5 : <AffineTransform>, 
  linearity_grad ( min -0.0938635, max 0.104336, mean 0.00120042, stddev 0.0066146, skewness 0.627537, kurtosis 7.53139 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.139835, max 0.171869, mean 0.0123652, stddev 0.0347284, skewness 0.333298, kurtosis 1.33964 ) , lr-coef 1
Component 6 : <Sigmoid>, 
Component 7 : <AffineTransform>, 
  linearity_grad ( min -0.148692, max 0.128642, mean 0.0011445, stddev 0.00699393, skewness 0.725156, kurtosis 11.376 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.161732, max 0.180656, mean 0.0120578, stddev 0.0376716, skewness 0.176132, kurtosis 1.6835 ) , lr-coef 1
Component 8 : <Sigmoid>, 
Component 9 : <AffineTransform>, 
  linearity_grad ( min -0.157616, max 0.128473, mean 0.000870958, stddev 0.0074815, skewness 0.879796, kurtosis 13.7699 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.17303, max 0.236413, mean 0.0108489, stddev 0.0438198, skewness 0.576457, kurtosis 2.54295 ) , lr-coef 1
Component 10 : <Sigmoid>, 
Component 11 : <AffineTransform>, 
  linearity_grad ( min -0.698681, max 0.608564, mean 0.00128064, stddev 0.0215977, skewness 0.500426, kurtosis 44.4042 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.686447, max 0.770523, mean 0.0168716, stddev 0.12845, skewness 0.396058, kurtosis 3.62964 ) , lr-coef 1
Component 12 : <Sigmoid>, 
Component 13 : <AffineTransform>, 
  linearity_grad ( min -16.6357, max 0.186225, mean 2.9309e-09, stddev 0.0958442, skewness -42.0551, kurtosis 4355.23 ) , lr-coef 1, max-norm 0
  bias_grad ( min -16.8701, max 0.461961, mean 1.13249e-08, stddev 0.685045, skewness -14.4393, kurtosis 308.121 ) , lr-coef 1
Component 14 : <Softmax>, 
### END GRADIENT

LOG (ali-to-post[5.5.294-06484]:main():ali-to-post.cc:73) Converted 1232 alignments.
LOG (copy-feats[5.5.294-06484]:main():copy-feats.cc:143) Copied 1112 feature matrices.
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:384) ### After 338432 frames,
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:385) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -7.50193, max 6.77601, mean -0.00405738, stddev 0.997423, skewness -0.00626004, kurtosis 2.0657 ) 
[1] output of <AffineTransform> ( min -24.799, max 17.1399, mean -3.20294, stddev 2.91811, skewness 0.139217, kurtosis 1.73961 ) 
[2] output of <Sigmoid> ( min 1.69797e-11, max 1, mean 0.159006, stddev 0.25754, skewness 1.98141, kurtosis 2.86317 ) 
[3] output of <AffineTransform> ( min -30.8775, max 15.3869, mean -3.76545, stddev 2.56365, skewness -0.00354602, kurtosis 3.22825 ) 
[4] output of <Sigmoid> ( min 3.89118e-14, max 1, mean 0.0998513, stddev 0.192914, skewness 2.95697, kurtosis 8.65113 ) 
[5] output of <AffineTransform> ( min -15.1631, max 12.6651, mean -3.23079, stddev 2.05202, skewness 0.773521, kurtosis 2.99162 ) 
[6] output of <Sigmoid> ( min 2.59867e-07, max 0.999997, mean 0.108694, stddev 0.195989, skewness 2.8864, kurtosis 8.16887 ) 
[7] output of <AffineTransform> ( min -25.5523, max 16.911, mean -3.22957, stddev 2.3074, skewness 0.610862, kurtosis 3.84449 ) 
[8] output of <Sigmoid> ( min 7.99403e-12, max 1, mean 0.120844, stddev 0.21629, skewness 2.59621, kurtosis 6.17497 ) 
[9] output of <AffineTransform> ( min -15.871, max 15.8112, mean -3.12323, stddev 2.58996, skewness 1.49218, kurtosis 3.55885 ) 
[10] output of <Sigmoid> ( min 1.28027e-07, max 1, mean 0.142231, stddev 0.262241, skewness 2.21098, kurtosis 3.67802 ) 
[11] output of <AffineTransform> ( min -31.761, max 19.0687, mean -3.40278, stddev 3.10226, skewness 1.12943, kurtosis 3.9204 ) 
[12] output of <Sigmoid> ( min 1.60832e-14, max 1, mean 0.143732, stddev 0.278984, skewness 2.15258, kurtosis 3.23134 ) 
[13] output of <AffineTransform> ( min -10.2218, max 16.1606, mean -0.0108618, stddev 2.49321, skewness 0.678249, kurtosis 1.33476 ) 
[14] output of <Softmax> ( min 2.42035e-11, max 0.957527, mean 0.000624869, stddev 0.014643, skewness 42.1624, kurtosis 2036.16 ) 
### END FORWARD

LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:387) 
### BACKWARD PROPAGATION BUFFER CONTENT :
[0] diff of <Input>  ( min -0.521253, max 0.864974, mean -0.000376931, stddev 0.0310662, skewness -0.213614, kurtosis 26.2761 ) 
[1] diff-output of <AffineTransform> ( min -0.264475, max 0.194234, mean -1.50486e-05, stddev 0.00687248, skewness -1.03219, kurtosis 87.5883 ) 
[2] diff-output of <Sigmoid> ( min -1.47389, max 1.75292, mean -0.000413876, stddev 0.0572475, skewness -0.0963045, kurtosis 20.3855 ) 
[3] diff-output of <AffineTransform> ( min -0.235934, max 0.269432, mean -1.99629e-05, stddev 0.0068638, skewness -0.319919, kurtosis 94.9474 ) 
[4] diff-output of <Sigmoid> ( min -1.26017, max 1.50603, mean -1.83758e-05, stddev 0.0673659, skewness 0.00739981, kurtosis 11.6633 ) 
[5] diff-output of <AffineTransform> ( min -0.303844, max 0.215911, mean -3.37566e-05, stddev 0.00682637, skewness -0.130323, kurtosis 75.4056 ) 
[6] diff-output of <Sigmoid> ( min -1.21541, max 1.63312, mean -0.000283988, stddev 0.0599158, skewness -0.00251851, kurtosis 14.7327 ) 
[7] diff-output of <AffineTransform> ( min -0.227616, max 0.236459, mean -3.04642e-05, stddev 0.00625077, skewness 0.062557, kurtosis 67.6166 ) 
[8] diff-output of <Sigmoid> ( min -0.989209, max 1.41979, mean -0.00024875, stddev 0.052063, skewness 0.0339974, kurtosis 12.2991 ) 
[9] diff-output of <AffineTransform> ( min -0.181665, max 0.191365, mean -2.58788e-05, stddev 0.00548805, skewness -0.0571839, kurtosis 54.7757 ) 
[10] diff-output of <Sigmoid> ( min -0.756071, max 0.948613, mean -0.000104649, stddev 0.0445696, skewness -0.043676, kurtosis 12.5076 ) 
[11] diff-output of <AffineTransform> ( min -0.146075, max 0.188224, mean -7.38271e-06, stddev 0.00748879, skewness 0.093135, kurtosis 32.9252 ) 
[12] diff-output of <Sigmoid> ( min -0.920799, max 0.782183, mean 0.000440001, stddev 0.0820748, skewness -0.00586856, kurtosis 2.44427 ) 
[13] diff-output of <AffineTransform> ( min -0.999994, max 0.903999, mean -7.59144e-09, stddev 0.0192805, skewness -30.4528, kurtosis 1741.7 ) 
[14] diff-output of <Softmax> ( min -0.999994, max 0.903999, mean -7.59144e-09, stddev 0.0192805, skewness -30.4528, kurtosis 1741.7 ) 
### END BACKWARD


LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:388) 
### GRADIENT STATS :
Component 1 : <AffineTransform>, 
  linearity_grad ( min -0.891785, max 0.926003, mean -0.000515592, stddev 0.10899, skewness -0.00186796, kurtosis 3.05523 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.569911, max 0.49276, mean -0.00385243, stddev 0.119204, skewness -0.153941, kurtosis 1.59979 ) , lr-coef 1
Component 2 : <Sigmoid>, 
Component 3 : <AffineTransform>, 
  linearity_grad ( min -0.474569, max 0.561117, mean -0.000784184, stddev 0.0344929, skewness 0.000860945, kurtosis 10.0123 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.555798, max 0.550269, mean -0.00511053, stddev 0.123336, skewness -0.0531821, kurtosis 2.60997 ) , lr-coef 1
Component 4 : <Sigmoid>, 
Component 5 : <AffineTransform>, 
  linearity_grad ( min -0.440622, max 0.498233, mean -0.000869781, stddev 0.0250167, skewness 0.0660174, kurtosis 15.101 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.563859, max 0.672872, mean -0.00864172, stddev 0.126226, skewness 0.264478, kurtosis 3.5748 ) , lr-coef 1
Component 6 : <Sigmoid>, 
Component 7 : <AffineTransform>, 
  linearity_grad ( min -0.335913, max 0.487468, mean -0.000818589, stddev 0.0233719, skewness 0.0764393, kurtosis 13.8901 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.455746, max 0.470458, mean -0.00779882, stddev 0.112425, skewness 0.0812786, kurtosis 2.3837 ) , lr-coef 1
Component 8 : <Sigmoid>, 
Component 9 : <AffineTransform>, 
  linearity_grad ( min -0.304271, max 0.35448, mean -0.000748146, stddev 0.0225595, skewness -0.0545745, kurtosis 10.9728 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.441954, max 0.423693, mean -0.00662492, stddev 0.0964497, skewness -0.00450248, kurtosis 3.34394 ) , lr-coef 1
Component 10 : <Sigmoid>, 
Component 11 : <AffineTransform>, 
  linearity_grad ( min -0.404629, max 0.434625, mean 0.000107584, stddev 0.0369139, skewness 0.0896774, kurtosis 9.07776 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.453226, max 0.593016, mean -0.00189, stddev 0.126427, skewness 0.188836, kurtosis 2.17367 ) , lr-coef 1
Component 12 : <Sigmoid>, 
Component 13 : <AffineTransform>, 
  linearity_grad ( min -4.28689, max 3.25612, mean -1.42433e-08, stddev 0.101822, skewness -5.19892, kurtosis 95.6677 ) , lr-coef 1, max-norm 0
  bias_grad ( min -4.1717, max 2.05949, mean 2.98023e-09, stddev 0.333111, skewness -2.44208, kurtosis 22.6536 ) , lr-coef 1
Component 14 : <Softmax>, 
### END GRADIENT

LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:395) Done 1112 files, 0 with no tgt_mats, 0 with other errors. [TRAINING, RANDOMIZED, 0.0780168 min, processing 72298.9 frames per sec; i/o time 5.13547%]
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:405) PER-CLASS PERFORMANCE:
@@@ Frames per-class: [ 14106 318 182 101 16 23 147 394 174 74 294 47 870 141 67 129 255 114 165 74 149 213 181 183 408 103 94 245 56 199 233 219 209 78 485 234 501 261 303 129 32 34 202 163 535 145 178 160 16656 5473 1407 385 440 102 243 122 303 118 380 0 107 271 58 97 93 86 243 57 294 191 144 415 282 143 874 110 864 152 49 13 569 154 448 222 342 20 710 155 141 68 76 139 255 166 278 70 165 257 182 92 99 60 29 327 272 85 199 196 15 111 124 263 142 460 128 77 124 98 17 288 88 277 175 145 85 235 203 106 16 130 103 96 261 105 225 47 654 102 126 63 230 68 627 94 106 150 211 42 124 289 144 100 161 98 300 136 229 44 206 11 193 224 432 114 59 162 165 82 201 60 143 259 59 211 112 0 74 325 188 0 308 89 60 48 267 60 114 213 217 233 69 84 34 171 117 241 110 199 148 224 208 100 318 174 185 100 39 85 279 178 263 182 189 160 112 158 150 96 122 83 71 465 110 243 156 322 191 131 401 380 136 112 84 62 69 94 171 202 164 237 323 105 117 1041 218 21 103 173 175 146 274 69 106 143 83 59 358 320 29 312 180 310 237 470 549 136 85 158 80 245 175 26 156 156 198 132 23 84 27 70 350 301 265 162 4 208 159 83 111 119 62 231 365 304 54 85 79 76 168 163 124 705 189 106 48 121 19 123 117 137 169 141 132 456 113 108 147 127 25 491 355 389 168 188 199 87 83 96 64 68 88 197 54 111 190 205 366 44 139 111 166 116 142 17 246 129 132 136 118 101 283 141 319 231 267 110 156 104 38 203 22 154 518 287 30 115 144 23 375 214 888 167 91 50 151 941 147 529 88 198 86 42 20 103 215 7 131 37 106 81 22 12 221 160 126 219 148 121 262 96 292 133 232 397 251 83 68 117 120 105 280 174 760 20 61 190 113 277 70 233 90 65 10 85 200 246 146 190 262 32 169 140 133 203 48 20 191 113 261 70 14 67 149 131 195 140 173 225 118 142 479 20 65 121 644 103 272 567 0 296 110 125 160 72 193 209 23 154 136 123 42 101 71 350 71 132 225 118 52 133 131 151 84 16 351 136 37 102 80 155 336 499 610 12 156 207 219 45 193 336 298 26 79 88 241 108 284 67 113 217 108 268 207 177 338 0 305 120 149 24 75 211 69 271 255 39 24 38 172 143 166 104 74 156 223 90 72 29 82 110 165 110 21 127 56 70 231 501 270 74 7 137 123 314 232 395 203 187 41 160 122 17 81 166 128 40 126 167 271 254 173 133 146 338 236 381 113 188 153 16 31 122 164 152 22 403 94 16 266 540 9 424 185 271 152 51 169 517 345 194 95 131 53 165 217 206 266 69 37 89 10 229 185 91 95 46 196 393 238 61 98 65 90 158 275 81 102 141 157 238 200 153 34 93 114 536 130 350 119 108 80 155 62 182 91 71 357 98 68 287 22 338 165 148 65 150 43 19 475 257 27 112 101 300 81 75 49 62 166 121 115 96 155 149 114 163 60 77 150 33 195 123 137 225 13 96 1 371 772 359 185 304 20 19 266 123 818 63 1086 46 67 11 30 81 10 43 201 243 244 188 190 152 28 79 20 11 286 81 50 67 51 394 74 59 133 106 100 187 12 118 214 93 178 180 230 63 181 177 200 223 108 128 194 131 308 105 338 145 9 47 366 206 283 41 270 145 334 161 60 200 224 66 501 255 295 186 156 187 52 224 200 154 11 230 236 109 237 192 144 232 96 14 257 201 241 13 135 54 40 360 65 120 202 78 92 300 194 309 191 175 92 336 191 76 30 198 155 34 242 103 151 159 81 34 239 79 193 187 894 389 136 108 191 100 92 356 115 212 250 415 139 101 304 13 96 177 271 105 284 17 199 129 1 184 412 238 105 621 144 87 465 395 487 106 311 221 12 120 85 137 17 153 113 92 58 238 214 264 478 417 92 505 132 126 271 46 78 163 182 80 434 59 158 155 94 295 460 167 110 239 370 337 231 178 243 94 209 244 6 572 148 133 853 101 140 298 137 110 210 495 597 87 244 98 307 351 125 50 122 130 203 116 71 317 281 439 117 229 237 160 74 14 68 0 179 352 232 39 133 161 95 239 159 52 55 475 149 208 139 238 143 13 107 166 174 77 63 265 134 130 27 75 250 167 217 98 105 107 7 35 22 33 161 244 236 88 96 74 173 431 163 277 184 154 69 301 65 155 55 235 28 214 170 218 43 55 187 121 171 187 83 253 141 91 98 50 139 63 314 513 250 377 20 20 151 50 93 210 102 8 75 294 127 188 138 219 387 206 58 42 144 160 500 297 122 109 157 150 293 122 184 135 454 256 639 430 514 276 260 11 205 603 30 294 400 314 275 1369 13 135 174 545 623 167 212 18 228 189 8 269 378 188 368 54 407 110 66 152 72 182 17 70 15 364 187 435 117 417 215 453 278 238 307 229 170 222 514 1659 385 27 29 135 197 165 95 257 265 225 234 324 927 460 300 79 209 290 73 105 104 301 57 72 71 288 277 257 196 163 88 149 159 249 147 133 209 222 383 280 233 81 385 205 66 221 90 97 206 21 51 102 157 112 55 247 289 199 94 253 66 209 60 71 91 216 394 303 128 297 867 202 154 463 68 187 379 101 293 182 46 44 166 97 74 114 137 135 144 261 159 184 429 65 188 148 128 153 25 51 234 276 278 124 105 40 231 49 271 475 138 472 70 108 64 340 343 67 57 83 126 354 312 217 173 1139 101 283 121 161 41 25 133 75 25 152 172 146 145 67 68 173 860 150 185 54 238 244 264 142 245 359 64 188 236 440 467 176 100 111 397 181 34 127 285 168 80 217 39 170 96 409 264 27 293 60 89 290 93 158 44 360 209 393 470 276 215 186 17 107 387 87 199 486 200 364 92 208 98 45 112 198 200 51 75 73 84 123 256 497 140 236 327 93 108 130 82 201 557 166 152 149 89 177 110 106 78 340 468 258 114 99 72 272 150 361 278 295 290 28 136 365 263 133 172 27 246 193 368 212 124 30 441 21 75 76 130 112 227 87 275 45 268 77 220 120 69 290 57 21 181 188 104 237 443 402 322 50 177 99 344 374 167 607 329 146 320 241 58 191 64 178 543 113 28 155 219 163 210 154 367 176 195 1 841 138 164 67 156 162 210 126 393 144 100 416 188 132 120 202 89 151 241 532 67 245 136 522 206 63 792 272 178 150 161 82 451 1062 108 97 108 446 279 270 318 355 93 76 348 328 313 159 58 10 183 270 82 159 405 105 236 115 193 135 71 309 426 85 360 345 394 142 119 508 112 227 186 277 383 158 183 201 169 192 413 353 368 174 165 139 218 129 84 185 106 167 149 106 202 192 52 277 184 277 122 323 139 144 176 143 106 132 12 86 245 122 119 225 70 210 189 188 218 55 279 22 282 149 868 210 123 114 155 211 47 194 250 216 147 82 225 171 95 173 158 351 748 556 143 154 463 220 173 181 206 19 145 407 56 190 265 144 349 166 273 170 313 198 110 79 89 397 68 641 130 53 377 240 109 206 175 235 18 281 141 48 200 153 232 127 223 530 843 47 63 163 102 29 37 47 20 181 ]
@@@ Loss per-class: [ 0.827368 2.72763 2.63765 3.63182 7.15334 7.3994 3.41018 2.24756 2.63591 4.54609 1.56127 5.73333 1.47457 3.02976 5.05063 2.96566 2.41093 2.83334 3.26062 4.74835 3.23465 2.81164 3.38878 3.41369 2.79118 3.88492 4.08401 1.98554 3.76696 2.74476 3.03441 2.55184 3.5047 4.51459 1.68546 2.47797 1.33332 1.97826 2.25946 4.88058 6.14761 4.85931 2.56313 3.21246 1.51804 3.16971 2.62517 3.49057 0.558834 1.66324 1.29065 2.05193 1.48303 4.59571 2.45713 3.87611 2.00369 3.3567 2.45466 0 3.59847 2.20982 6.20352 4.43691 4.05673 3.73575 2.42679 4.31731 3.17769 2.6467 3.27179 2.32372 1.81713 2.17405 1.18402 4.26362 0.935171 3.38414 4.99876 8.93727 2.02412 2.39904 1.72013 2.74479 2.33235 6.03996 2.17741 2.861 3.26818 4.31786 4.28295 2.57942 1.97042 3.80608 2.29078 5.16179 4.04387 2.1791 2.26485 3.90347 2.82997 4.82645 6.66688 1.98716 2.68611 3.33058 2.78171 2.65278 7.16456 3.78797 3.0532 1.75033 3.61711 1.52998 4.50524 4.44186 3.60018 3.44724 7.23085 2.66593 3.8548 2.14505 3.26521 3.30672 2.8401 2.6398 3.21691 3.82456 8.63522 3.30258 4.57521 3.42362 2.78995 4.15847 3.36989 5.2978 1.68348 4.10015 3.68644 5.18272 3.01003 4.69674 1.50737 4.70523 3.11269 3.3943 3.32373 7.03259 3.38365 2.33195 3.58774 3.61611 3.20626 4.0138 2.5505 3.02843 2.24459 5.21445 2.24526 8.92803 2.67891 1.9043 2.45299 3.92629 4.62656 3.52512 3.28252 4.70859 3.18975 5.23722 3.44999 2.36677 3.61844 3.484 3.39358 0 3.94053 2.47051 2.22005 0 2.95972 4.38532 3.7583 5.27457 2.80592 4.56237 2.52361 2.79979 3.11402 1.39042 3.46321 4.53364 5.32402 2.4644 3.17968 2.49909 3.30758 3.33638 3.31839 2.43423 3.28679 4.70999 2.19327 3.44436 3.88606 3.67703 6.3194 4.9288 2.68183 3.09746 2.6605 2.99023 2.39616 3.07029 4.0041 2.94226 5.11898 4.33975 3.43957 3.95102 4.39453 2.53569 4.225 3.01659 2.51697 2.33117 3.10112 2.92088 1.8311 1.87156 3.33484 3.55553 4.40783 4.01504 3.60412 5.52666 3.21998 3.03082 3.73829 3.54038 3.14586 3.51941 2.6762 1.31681 2.24544 12.9601 4.39653 3.51801 4.39046 4.97177 3.37103 4.47567 4.44432 3.73751 4.59253 5.23707 2.4266 2.20651 7.16225 1.89911 1.75585 2.78084 3.70375 1.62464 1.76968 3.18583 4.01056 3.38627 4.12753 2.73914 2.91532 6.11906 3.10774 2.65664 4.43113 4.03603 7.80846 4.35957 7.06338 4.08714 2.2803 3.41548 4.04252 2.0809 9.3274 3.35701 2.86056 3.96668 3.98302 3.21824 4.77169 3.14708 2.0392 2.77629 5.93339 4.63971 3.76197 5.03998 2.78983 2.0959 3.4397 1.68903 4.12024 3.65569 5.3252 3.54353 6.83267 5.51782 3.48719 3.73125 2.92679 3.95166 3.38537 2.69012 4.00061 3.51461 3.25121 3.43951 5.34569 2.51549 2.17768 1.68112 2.78856 3.3629 2.965 3.58043 3.60867 3.11837 4.25355 5.22436 3.99015 2.39878 4.52823 3.20367 2.88845 3.15559 2.16768 6.48749 4.52666 4.1355 2.46209 3.53439 2.88898 6.82697 2.54368 3.95196 3.82632 2.70519 4.34777 4.04934 2.74346 2.20573 2.40623 2.23641 2.56975 3.11742 3.16144 3.15233 5.46911 3.29016 7.69225 2.95425 1.78092 3.17218 5.11859 3.59965 4.35926 7.55176 2.89051 2.66916 1.84486 2.99981 4.54933 5.46062 3.44605 1.64129 3.42094 1.67628 5.55127 3.18037 4.72798 5.14508 5.58507 3.25477 4.42194 10.4653 4.07669 5.40117 3.06124 4.87307 7.63526 9.84497 2.99537 3.58478 2.41161 2.25517 2.9382 3.86435 3.84401 2.63817 2.29438 3.85835 2.20106 2.29238 3.19628 5.32127 4.05875 4.04786 3.71427 4.01883 3.66615 2.82228 1.71352 7.66021 4.56195 3.78455 3.40619 3.63799 4.64688 2.90341 3.63162 5.33192 9.83809 4.92888 2.98043 2.48747 2.8186 2.78388 3.62813 6.45401 3.63011 3.19766 4.8169 2.93693 5.6282 7.23412 3.61279 2.67304 2.53142 4.98686 6.9847 3.61538 3.96203 3.37544 3.01038 3.75551 4.21252 2.68672 4.05238 2.64336 1.91675 7.62658 4.29513 3.49371 1.89305 4.48495 1.9775 1.52997 0 3.07658 3.8345 3.71851 2.60298 5.12173 2.45357 3.20255 5.83633 3.31466 3.35648 3.51744 6.43172 4.25069 4.93081 2.41774 4.70767 3.83902 2.90088 4.23546 4.88821 3.12913 3.4231 2.7115 4.58602 6.94112 2.23063 3.99102 6.2916 4.89291 5.39535 3.61088 2.95127 1.59641 1.80556 9.10259 2.67093 3.72458 3.37293 5.61535 3.9052 3.1616 2.50525 6.892 3.89664 3.6434 3.02057 3.28989 2.68156 5.14941 3.94649 2.99686 3.75227 2.91733 3.67235 4.39554 2.62014 0 2.41651 4.73196 3.51312 5.90535 5.06921 2.83647 4.77401 2.96034 2.19365 6.19303 6.11934 5.93922 3.85154 3.33419 3.23478 4.19207 4.78512 4.70051 3.10306 3.67441 4.23997 6.41658 4.95759 3.25708 3.0858 4.62991 6.82489 3.42997 4.63664 3.33641 2.92741 1.77846 3.13926 4.5641 8.2884 3.72439 3.12122 2.74516 2.34881 1.45722 3.43399 3.92829 5.96359 2.82698 2.56979 7.63336 4.28644 3.3039 3.52007 5.75912 3.87014 2.51212 2.91133 3.72496 3.52154 3.75003 3.28914 1.82602 2.72414 2.89584 3.75349 2.92526 4.03575 9.43644 5.77954 3.64009 2.23753 2.43467 5.95707 1.93942 4.93524 9.27215 2.74753 2.80785 9.1963 2.35821 2.70008 3.18606 2.7196 5.32074 2.91446 2.6095 2.25713 2.98614 3.79052 3.69426 4.30692 4.02111 3.05953 2.46175 3.21149 5.56157 6.32951 3.56152 9.87136 2.53203 3.26993 4.2689 4.60472 6.39164 3.40475 2.13552 2.593 4.39668 4.34845 4.22214 4.21999 3.34613 1.99981 4.1221 4.22554 3.69498 2.94469 3.22955 3.45431 3.6468 5.58535 4.04853 3.47367 1.85715 3.79305 2.87862 3.89766 4.76956 2.65317 3.28525 5.46943 4.02277 4.88166 5.49979 2.54874 4.55276 4.87744 2.51283 7.73717 2.67427 3.0051 3.74516 3.98536 4.44637 7.18363 5.85753 2.9555 3.31426 4.03854 4.3549 3.56192 2.40136 4.54256 4.35236 6.30552 4.91399 2.92814 2.92557 3.09866 4.31354 3.03413 3.19908 5.26083 4.22073 5.52259 4.17102 4.23857 4.7195 2.38423 4.28873 4.21177 2.98124 7.16654 4.95588 10.1645 3.87005 1.68735 1.75603 2.48748 2.93858 7.29701 8.44413 2.54418 2.79212 2.0656 5.03298 1.6924 5.22478 4.58692 8.27639 4.71059 4.55398 7.64949 5.89269 3.38325 3.40858 3.09142 2.57975 2.11401 4.01633 6.64024 4.60608 6.63083 7.78103 3.03503 4.42736 4.74671 5.9828 5.06131 2.36243 6.27833 5.88938 3.01141 4.86685 4.04796 2.69036 5.27455 3.50731 2.64066 4.44146 3.94371 3.44693 3.58491 4.83718 3.98858 3.09288 2.86816 3.31131 3.27043 3.95531 3.18382 5.21511 2.14301 3.94901 2.34621 3.02379 9.95559 4.00584 2.1425 2.75978 2.76654 4.13452 3.06425 3.54479 2.78407 3.69199 4.28148 2.5756 2.80454 4.89002 2.26744 2.93618 3.26345 4.03369 2.79538 2.82588 4.91966 2.99094 3.5585 3.48947 9.16391 2.63566 3.33328 3.62479 2.93033 3.45215 3.84681 2.56711 3.43727 7.23458 2.85138 3.0041 2.99853 8.5702 4.8827 5.4675 5.72855 2.17615 4.44119 4.07031 3.10517 4.54301 4.89094 2.64874 2.3712 2.38597 3.02814 3.7481 4.09877 2.64511 2.40019 4.08584 5.74248 2.17116 2.69219 5.25745 3.28 4.41551 3.40671 4.05089 4.03575 6.12617 3.32186 4.8585 3.0114 3.22581 2.14467 1.84817 3.82712 4.35402 3.0808 5.09922 4.78019 2.65805 3.27791 2.65555 2.08921 1.87814 3.87687 3.95031 2.99122 7.08258 4.43825 3.09604 2.98363 4.7015 2.61352 7.45103 2.7527 4.38682 8.28932 3.01564 2.51654 2.42584 4.50103 2.02155 4.14717 4.71668 1.74605 2.65455 2.17895 3.56556 2.82318 2.86844 8.27527 3.82145 3.90128 3.08951 7.9723 3.05261 3.28923 4.85432 4.67439 2.36507 5.52642 2.39532 1.92339 1.74526 4.34503 2.49709 4.14421 3.33429 3.29008 5.67629 5.21044 3.51266 3.53698 4.97932 2.12385 4.70668 3.97701 3.12468 4.2355 2.56815 2.83095 3.71204 4.089 2.79906 1.95001 2.45495 2.87304 2.84371 2.35102 3.05079 2.96998 3.29402 9.52519 1.89485 3.0508 4.39208 1.99432 3.65792 3.96895 2.59739 3.80903 3.43 2.75224 2.08127 2.36061 4.63919 2.46249 4.19462 3.11798 2.78824 4.31476 6.43661 4.13763 4.39225 2.33835 3.30027 4.41276 2.33338 3.52466 2.41972 4.28266 3.97016 3.13539 3.46569 4.26079 6.63654 5.15572 0 3.8876 2.31459 2.69773 5.92778 4.20753 2.94264 4.22749 2.40655 4.07715 5.9826 5.78525 1.96058 4.44877 2.80308 4.53256 2.95858 3.38704 9.83481 4.67835 3.02054 3.52142 5.61888 5.05998 2.8498 4.02593 3.03213 5.10175 4.90598 2.73884 3.56405 3.17878 4.08579 3.72355 3.68945 9.64542 5.79015 5.61402 4.34553 3.41494 2.53276 2.59909 3.92113 4.36967 4.74232 3.45371 2.31068 3.77859 1.90529 3.751 3.59657 4.09675 2.9016 4.95135 3.43459 4.81466 3.20162 6.44327 3.03432 2.52581 2.43356 6.5116 5.92369 4.14576 3.49171 3.82806 3.16855 4.12113 3.28883 3.52322 4.26373 3.38235 5.56941 5.02504 6.91632 2.78283 2.29062 4.08122 3.25989 5.98178 7.12529 3.47619 6.38791 4.13373 3.34419 5.03233 9.22814 4.64978 2.65372 4.15968 3.74305 3.40139 2.65916 3.22433 2.50579 5.69467 6.10012 4.27006 5.83645 2.17174 2.37001 3.896 3.93782 3.13368 3.37438 2.67661 3.18494 3.12139 3.88758 1.74588 2.84328 2.80196 2.99386 2.80926 3.53599 2.95502 8.94218 2.52457 1.79949 7.02318 4.06423 3.02504 2.17698 2.84674 1.37594 9.79792 4.30046 2.71669 2.56088 1.73662 2.77608 2.78606 8.03783 2.95979 4.56684 9.35407 1.92223 1.9964 3.98983 2.33526 5.42201 3.08552 3.12726 4.66161 3.39852 5.03916 3.87082 8.73747 5.2549 7.4339 1.9158 2.80423 1.84945 3.71545 2.04822 3.7274 2.3326 2.75642 2.56845 2.82138 3.20831 4.52912 2.4632 2.26362 1.41611 2.24468 5.79987 5.84368 3.35379 3.07167 2.9625 4.66214 4.10137 2.82691 2.99993 3.12379 3.02484 1.29343 2.66711 2.73363 5.64093 3.49701 2.60581 5.66206 3.87381 4.8662 2.92956 5.34094 3.6081 4.95194 3.39701 2.95029 3.85848 3.93507 5.33147 3.87272 4.1916 4.10617 1.98687 3.19985 4.56548 4.47101 3.78884 2.46589 2.90807 3.40756 4.31059 2.58069 3.21518 4.58347 2.32895 4.47779 2.72513 2.65709 6.11077 6.63853 6.0015 3.36973 5.18927 4.64177 3.4237 2.31493 3.92905 3.73988 4.33607 4.00689 4.42908 4.84751 4.28737 4.05744 2.38162 2.60893 2.88682 4.47887 2.32704 1.80046 3.37574 4.10209 2.14144 3.90048 3.54213 2.71585 4.59081 3.79523 3.28719 5.92743 5.39633 3.25003 4.29496 4.96339 4.18281 3.83788 3.77321 4.03265 3.07012 3.48187 4.00427 2.09435 5.08715 3.43844 3.36623 3.7305 3.22023 7.22726 6.3388 3.49958 2.71169 3.10995 4.29997 4.01631 5.24406 2.83211 5.0361 2.65736 1.98201 4.4863 1.64249 4.04557 3.90475 4.27742 2.38946 3.57173 5.37693 5.07098 4.79026 3.4678 2.39551 3.00004 2.90929 4.35036 1.57221 3.98669 1.92354 3.78109 3.32615 5.41034 6.49142 4.19043 5.61538 6.57892 3.69173 4.05504 3.29276 2.57996 4.57151 4.90563 3.1029 1.55862 3.713 3.40844 4.65242 2.84256 2.01969 1.97331 4.68592 2.73402 3.82345 6.86146 2.74991 3.07883 2.99426 2.8179 3.76328 3.9735 3.50524 2.66573 3.08663 6.11237 4.52397 2.46221 3.12442 4.75469 2.49073 6.31444 3.1951 5.21752 2.1333 3.4091 6.85172 1.95257 4.08257 4.27346 2.46737 5.05174 2.94323 5.6756 1.99236 2.59115 2.70689 2.27476 2.53113 3.25744 3.8715 6.45451 4.44515 2.63198 4.06633 3.82012 2.05716 3.42655 2.63775 4.47685 2.9763 3.81867 4.44214 4.17546 2.9707 3.05835 6.02989 4.73689 5.33528 5.16621 4.07635 2.67712 1.90034 3.34331 3.33444 2.85957 4.9605 4.10434 3.59529 5.11603 2.91777 2.12499 2.8648 3.8515 3.74403 4.55724 4.26262 3.73813 4.65161 3.53302 2.6783 2.6806 3.13268 4.78566 4.16584 4.27505 2.02161 4.44421 2.2517 2.78393 2.28493 1.73999 6.95784 4.05244 1.94859 4.56413 4.30171 4.95388 6.7909 3.81593 3.87311 3.24976 3.39061 4.11219 6.54486 2.44825 7.11986 3.92088 4.20446 4.84227 4.58062 2.86709 4.45202 3.04 7.58961 2.31375 5.45183 3.37817 5.58457 5.22243 3.34409 4.30306 6.71595 2.81784 2.73262 4.1059 3.01544 3.17654 2.66595 3.03896 5.36307 3.45732 3.7131 1.98212 3.15141 3.19899 1.34012 3.32328 3.62001 4.22136 2.99636 4.90695 2.97227 5.23955 3.41953 1.5496 5.59311 7.11302 3.81931 3.4181 2.56211 3.39717 2.84146 2.87334 3.08839 3.08975 5.40145 1.92192 3.36285 2.68541 3.54088 4.11679 4.03886 2.70952 3.03442 2.93163 3.71021 4.87697 1.76438 3.06687 3.68224 3.89555 3.52176 4.81486 3.48103 3.21408 3.21572 4.28144 2.79294 4.22468 2.43676 3.60657 4.84238 2.07327 3.15 4.29799 3.61074 2.97044 5.15227 1.93396 1.35837 4.0217 4.44767 4.76733 2.93547 2.85511 3.2696 2.5083 3.24657 5.63444 4.55533 2.17337 2.89608 2.65386 2.99516 4.89926 9.40058 2.65497 2.25143 4.68915 3.32752 2.52527 5.17906 2.36334 3.70823 4.89291 3.63018 5.42862 2.43668 1.65613 4.74106 3.10123 2.73759 2.90462 4.27687 3.05567 2.42723 4.16267 3.76232 3.57322 3.44331 3.19362 4.36109 3.41716 3.41037 3.12006 3.43805 2.73614 2.11558 3.12372 3.02044 3.41812 3.58238 3.24008 2.57444 4.62843 3.68243 4.08336 4.90841 3.25087 3.96512 2.73139 3.52245 4.85034 4.40149 4.05786 2.14855 3.72611 2.48232 4.15336 3.78238 2.85586 2.75579 4.25727 3.70289 7.91368 4.05819 2.85563 3.52485 3.99102 3.68412 5.57851 2.87547 3.50569 3.41015 2.2772 4.95431 2.85008 5.6467 3.09049 3.80058 1.41582 3.44965 4.63886 4.3067 3.80721 4.02485 6.48031 3.52504 2.95072 3.28757 3.24556 3.94228 3.12811 2.90225 4.57066 3.77533 4.42012 2.49013 2.24936 2.26925 4.83476 3.72186 2.34035 2.6273 3.53152 3.44171 3.69451 6.56102 3.58732 3.17554 5.65819 2.99436 3.28223 3.69721 2.60339 3.0412 2.50318 3.47869 4.27856 2.8927 5.13217 5.21186 4.86824 1.90041 4.56761 2.29635 3.9744 5.22772 3.32248 2.6367 3.51602 3.02209 3.02191 3.74439 6.46685 3.22322 2.8585 5.49167 3.35183 2.89706 2.93297 4.5318 3.14661 1.57556 2.17738 6.39805 5.76869 3.47583 5.06262 8.79783 7.32593 5.81636 8.25374 4.31624 ]
@@@ Frame-accuracy per-class: [ 73.654 41.7582 41.6438 25.6158 0 0 25.0847 47.9087 41.2607 5.36913 65.1952 0 60.0804 33.9223 1.48148 40.1544 44.6184 42.7948 33.8369 6.71141 27.4247 35.5972 35.2617 31.6076 31.8237 17.3913 8.46561 40.7332 17.6991 32.0802 29.9786 43.2802 25.7757 14.0127 59.1143 44.7761 71.1864 56.5966 41.8451 7.72201 0 23.1884 44.4444 25.0765 61.9981 36.4261 31.9328 24.9221 84.6516 43.7928 56.341 65.3696 68.7855 0.97561 38.6037 23.6735 53.7068 22.7848 47.3062 0 25.1163 48.6188 8.54701 10.2564 9.62567 23.1214 45.9959 13.913 40.0679 29.2428 26.2976 50.0602 62.6549 48.7805 74.3282 8.1448 75.7663 20.9836 12.1212 0 50.7463 43.3657 63.7681 43.5955 53.7226 0 45.7424 34.7267 24.735 20.438 3.92157 40.1434 49.3151 14.4144 44.8833 9.92908 12.6888 41.9417 45.4795 18.3784 38.191 8.26446 0 51.6031 38.8991 26.9006 29.0727 41.7303 6.45161 20.6278 28.1124 63.7571 19.6491 64.0608 5.44747 15.4839 24.0964 19.2893 0 33.9688 10.1695 52.2523 21.0826 25.4296 36.2573 38.6412 30.9582 20.6573 0 23.7548 4.83092 8.29016 34.4168 12.3223 19.9557 6.31579 62.3377 15.6098 16.6008 6.29921 32.538 2.91971 70.1195 3.1746 30.0469 25.2492 29.3144 0 21.6867 47.6684 21.4533 24.8756 32.8173 14.2132 42.9285 34.4322 55.3377 4.49438 44.0678 0 35.1421 56.5702 40.2312 12.2271 3.36134 17.2308 19.9396 16.9697 26.3027 6.61157 25.0871 43.9306 18.4874 35.461 17.7778 0 9.39597 44.2396 50.3979 0 31.1183 17.8771 24.7934 0 36.2617 11.5702 45.4148 28.103 32.6437 75.803 17.2662 9.46746 5.7971 57.7259 25.5319 39.3375 30.7692 18.5464 27.6094 49.8886 22.542 2.98507 48.0377 26.9341 12.938 31.8408 2.53165 4.67836 45.0805 31.9328 34.1556 27.9452 40.6332 34.891 21.3333 42.9022 7.97342 17.6166 25.3061 16.7665 5.59441 43.3942 14.4796 30.8008 33.8658 48.062 25.5875 40.3042 56.2889 57.293 25.641 24.8889 8.28402 19.2 38.8489 2.1164 29.7376 31.6049 19.4529 27.7895 22.5657 31.2796 35.7447 68.555 52.1739 0 4.83092 16.1383 6.83761 4.09556 20.0364 5.7554 9.38967 13.9373 15.5689 10.084 47.1409 50.546 0 62.4 63.7119 30.2738 16.8421 59.9362 60.0546 27.1062 17.5439 25.8675 11.1801 30.9572 34.7578 11.3208 40.8946 35.1438 8.06045 17.3585 0 8.28402 0 19.8582 46.2197 18.9055 16.9492 54.1538 0 19.1847 36.3636 7.18563 19.7309 22.5941 3.2 26.7819 62.6539 34.8112 0 7.01754 16.3522 10.4575 36.2018 49.5413 26.506 59.3905 24.2744 13.1455 6.18557 20.5761 0 3.23887 30.6383 19.6364 34.8083 10.6007 16.6038 26.0679 6.1674 17.5115 31.1864 21.1765 15.6863 43.3367 44.7257 63.9281 30.8605 27.5862 32.0802 17.1429 34.7305 34.1969 17.0543 0 25.9887 37.9747 12.844 31.3901 27.2966 37.9562 43.6562 0 16.4875 6.27803 48.048 28.3262 42.807 0 50.7099 6.17761 11.3208 40.293 8.43882 19.7044 43.0335 47.3498 46.6354 40.6048 34.0187 23.5294 32.5879 38.2775 0 32.9238 0 37.5405 66.5381 22.9565 26.2295 18.1818 3.46021 0 38.3489 46.62 57.5127 29.8507 18.5792 3.9604 23.1023 57.9926 11.5254 72.5212 5.64972 33.2494 12.7168 9.41176 24.3902 42.5121 8.35267 0 12.9278 2.66667 34.7418 3.68098 0 0 21.6704 18.0685 46.6403 49.2027 33.67 24.6914 12.1905 48.7047 49.5726 22.4719 55.914 49.5597 31.4115 4.79042 20.438 20.4255 22.4066 10.4265 5.7041 34.957 60.881 4.87805 11.3821 15.748 29.9559 15.1351 12.766 30.4069 33.1492 7.63359 0 2.33918 37.4065 40.1623 30.0341 29.3963 20.1905 6.15385 16.5192 30.605 11.236 26.0442 0 0 15.1436 42.2907 50.8604 2.83688 0 23.7037 18.0602 32.6996 31.202 23.4875 17.8674 36.8071 10.9705 31.5789 54.2231 4.87805 7.63359 21.3992 67.6493 8.69565 60.1835 66.6079 0 25.6324 10.8597 19.1235 47.9751 2.75862 47.5452 25.7757 8.51064 27.1845 23.4432 25.9109 0 4.92611 12.5874 43.9372 0 27.1698 27.4945 12.6582 13.3333 29.9625 25.8555 40.264 2.36686 0 51.2091 15.3846 5.33333 4.87805 9.93789 23.1511 31.2036 64.8649 60.6061 0 44.7284 18.3133 23.2346 2.1978 8.78553 23.477 40.201 3.77358 17.6101 24.8588 30.6418 29.4931 33.0404 1.48148 18.5022 31.2644 15.6682 36.1266 10.6024 16.9014 38.9956 0 38.6252 6.639 22.7425 8.16327 7.94702 35.9338 18.705 38.3057 54.7945 2.53165 4.08163 7.79221 19.1304 27.8746 26.4264 15.311 12.0805 6.38978 30.8725 23.2044 13.7931 0 6.06061 29.8643 31.4199 2.71493 0 37.6471 7.07965 22.695 36.2851 67.5972 26.9871 5.36913 0 10.9091 29.9595 34.6582 52.043 74.842 19.656 12.2667 0 45.4829 41.6327 0 11.0429 28.2282 24.9027 17.284 6.32411 31.6418 36.0958 20.4322 17.8674 22.4719 36.1775 58.4934 35.0951 34.3381 25.5507 30.7692 9.77199 0 3.1746 23.6735 54.1033 41.9672 8.88889 61.4622 4.2328 0 36.773 35.8927 0 53.4747 38.2749 30.9392 47.2131 1.94175 35.3982 34.7826 44.8625 29.3059 23.0366 18.251 7.47664 15.1057 31.7241 46.9734 28.8931 1.43885 8 27.933 0 37.9085 29.1105 15.3005 6.28272 0 29.0076 44.9809 32.2851 11.3821 14.2132 10.687 13.2597 22.7129 55.1724 20.8589 3.90244 19.0813 29.8413 33.1237 22.4439 28.6645 0 16.0428 17.4672 59.2731 18.3908 25.9629 10.0418 8.29493 37.2671 18.0064 1.6 10.9589 5.46448 2.7972 54.5455 11.1675 1.45985 43.8261 0 29.5421 29.6073 13.468 22.9008 8.63787 0 5.12821 32.8076 32.6214 43.6364 8 23.6453 36.9384 11.0429 5.29801 0 11.2 30.6306 43.6214 32.9004 8.29016 17.3633 32.107 5.24017 12.844 3.30579 27.0968 4.65116 23.8806 40.4092 8.90688 16 36.3636 0 7.25389 0 16.4199 61.3592 61.4743 42.0485 24.3021 0 0 40.9006 46.9636 42.8833 4.72441 68.3847 6.45161 19.2593 0 19.6721 7.36196 0 0 24.8139 29.5688 30.6748 42.9708 47.2441 5.2459 0 17.6101 0 0 33.5079 15.9509 9.90099 2.96296 3.8835 48.6692 1.34228 1.68067 35.206 1.87793 17.9104 35.7333 0 21.9409 43.8228 2.13904 17.9272 24.3767 24.295 1.5748 14.3251 28.169 25.9352 14.3177 33.1797 9.33852 32.9049 1.52091 44.0843 21.8009 57.6071 37.8007 0 37.8947 58.3902 33.414 33.5097 31.3253 23.6599 20.6186 31.6891 12.3839 13.2231 43.8903 27.6169 7.5188 54.4367 39.1389 24.0271 6.97051 38.9776 35.2 7.61905 33.4076 12.4688 23.301 0 43.8178 31.7125 25.5708 31.1579 17.6623 29.0657 44.7312 20.7254 0 41.5534 41.6873 37.2671 0 7.38007 1.83486 4.93827 42.9958 19.8473 6.639 17.7778 8.9172 5.40541 43.2612 49.3573 38.7722 22.9765 14.245 16.2162 39.5245 52.7415 14.3791 6.55738 61.461 30.2251 11.5942 22.268 17.3913 25.0825 11.9122 22.0859 2.89855 36.3257 7.54717 27.907 36.2667 42.7054 58.5366 16.1172 8.29493 32.8982 4.97512 8.64865 42.3562 28.5714 36.2353 56.6866 60.4091 25.8065 14.7783 30.2135 0 8.29016 35.493 31.3076 2.8436 39.0158 0 34.5865 8.49421 0 24.9322 42.1818 47.7987 10.4265 55.35 20.0692 6.85714 64.0172 47.5348 51.8974 20.6573 38.5233 31.6027 0 8.29876 8.18713 29.8182 0 24.1042 21.1454 6.48649 13.6752 53.2495 3.2634 52.552 49.9478 66.1078 4.32432 42.1365 18.1132 36.3636 31.6759 4.30108 5.09554 14.0673 25.7534 1.24224 51.0932 11.7647 17.6656 31.5113 14.8148 35.1946 35.6135 23.8806 13.5747 37.1608 57.4899 42.6667 33.2613 22.409 39.8357 27.5132 35.3222 22.0859 0 66.5502 26.936 16.4794 53.4271 19.7044 20.6406 44.2211 8.72727 19.9095 38.9549 60.5449 45.523 10.2857 47.0348 17.2589 21.4634 37.8378 5.57769 0 11.4286 7.66284 46.1916 29.1845 6.99301 49.4488 17.4067 52.5597 9.3617 13.0719 32.4211 29.2835 17.4497 0 4.37956 0 15.5989 47.0922 51.1828 0 18.7266 42.7245 10.4712 48.4342 13.1661 0 3.6036 59.9369 11.3712 28.2974 7.8853 33.1237 20.9059 0 0.930233 30.6306 16.0458 1.29032 9.44882 42.1846 11.1524 35.249 10.9091 5.29801 36.7265 18.5075 20.2299 22.335 11.3744 13.0233 0 16.9014 13.3333 14.9254 30.3406 42.1268 46.0888 13.5593 9.32642 8.05369 23.6311 53.0707 22.63 60.5405 20.5962 30.4207 8.63309 30.1824 0 28.9389 10.8108 27.1762 0 27.972 41.0557 37.9863 2.29885 0 13.8667 22.2222 28.5714 29.3333 21.5569 17.357 29.682 7.65027 20.3046 7.92079 16.4875 0 37.5199 48.8802 9.18164 29.9338 0 0 21.1221 0 10.6952 24.228 8.78049 0 6.62252 43.1239 8.62745 13.7931 23.8267 44.1913 29.4194 42.1308 3.4188 2.35294 11.7647 1.24611 52.3477 53.4454 15.5102 11.8721 22.8571 23.2558 39.8637 18.7755 27.1003 18.4502 69.747 36.6472 17.5137 23.6934 42.5656 28.2098 37.2361 0 45.2555 59.9834 0 14.2615 36.4544 51.1924 35.2087 67.6159 0 7.38007 41.8338 24.0147 57.097 37.0149 36.2353 0 22.7571 14.248 0 58.6271 59.181 12.7321 52.1031 3.66972 25.2761 30.7692 0 37.377 0 10.411 0 1.41844 6.45161 57.6132 38.9333 65.9013 21.2766 50.7784 22.2738 49.6141 40.2154 42.348 30.5691 29.6296 8.21114 43.5955 55.9767 65.803 51.1025 0 0 42.0664 25.8228 38.0665 14.6597 17.4757 37.6648 33.2594 33.2623 35.7473 69.8652 38.4365 41.2646 6.28931 27.685 51.9793 0 24.6445 8.61244 27.8607 17.3913 34.4828 9.79021 29.1161 34.955 21.3592 19.8473 3.66972 16.9492 24.7492 13.1661 49.6994 35.2542 6.74157 13.3652 22.4719 53.1943 35.6506 30.4069 4.90798 43.3204 20.438 10.5263 52.8217 17.6796 40 41.1622 0 0 0.97561 20.3175 0.888889 5.40541 25.8586 52.5043 10.5263 20.1058 15.7791 19.5489 17.6611 9.91736 22.3776 26.2295 40.6467 36.5019 34.9259 7.7821 49.4118 49.683 27.1605 11.6505 50.2697 20.438 21.3333 36.3636 12.8079 24.5315 23.5616 4.30108 17.9775 26.4264 15.3846 6.71141 9.60699 13.8182 26.5683 18.6851 28.6807 14.4201 7.58808 44.7031 9.16031 19.0981 29.6296 16.3424 40.3909 0 0 24.7335 40.868 30.1616 15.261 14.218 4.93827 31.1015 36.3636 37.9374 59.9369 7.94224 63.2804 17.0213 17.5115 31.0078 51.1013 15.4294 0 8.69565 9.58084 19.7628 41.1848 26.56 33.5632 16.1383 36.3317 21.6749 54.6737 13.9918 31.5789 2.40964 0 17.2285 3.97351 0 22.9508 12.1739 30.0341 38.488 7.40741 0 29.9712 67.2865 19.9336 30.1887 12.844 28.9308 44.5808 60.8696 3.50877 46.0285 22.2531 0 46.1538 33.8266 28.6039 33.5829 15.2975 11.9403 33.1839 29.6855 33.6088 0 13.3333 53.5902 25.5193 18.6335 59.7701 0 26.393 1.03627 54.2125 24.9527 0 62.6917 23.1405 7.82123 43.3735 3.20856 32.8076 4.49438 52.982 38.1862 39.8983 51.2221 43.038 23.6659 14.4772 0 12.093 47.4839 18.2857 13.5338 38.4378 30.4239 49.9314 7.56757 28.2974 33.5025 4.3956 16.8889 33.2494 26.9327 0 5.29801 1.36054 3.5503 9.7166 40.9357 60.9045 31.3167 29.1755 41.2214 3.20856 13.8249 24.5211 4.84848 42.1836 34.6188 34.2342 20.3279 12.709 8.93855 6.19718 12.6697 5.6338 25.4777 41.4097 30.9498 24.3714 13.9738 10.0503 8.27586 55.0459 14.6179 55.0484 43.088 47.0389 64.0275 0 12.4542 57.1819 10.2467 10.4869 5.21739 0 24.7465 13.4367 24.4233 27.7647 16.8675 0 53.0011 0 22.5166 13.0719 4.5977 8.88889 29.8901 16 40.6534 0 46.1825 2.58065 24.9433 0 2.8777 24.7849 13.913 0 33.6088 34.4828 7.6555 34.1053 30.6652 40 33.7984 11.8812 14.0845 25.1256 52.5399 25.3672 29.8507 70.6173 28.2246 15.0171 11.5445 21.5321 5.12821 35.5091 7.75194 28.5714 62.9255 0 0 18.0064 35.9909 43.4251 26.6033 22.6537 34.8299 21.5297 26.087 0 44.0879 10.8303 32.8267 19.2593 14.6965 20.3077 35.6295 36.3636 35.5781 22.8374 1.99005 47.7791 42.4403 24.1509 19.917 16.2963 4.46927 36.9637 21.5321 15.7746 11.8519 38.2892 13.9194 42.6794 21.3075 12.5984 50.4732 27.5229 9.52381 13.9535 24.1486 1.21212 57.3643 69.4588 39.6313 8.20513 10.1382 36.5062 32.5581 32.1627 52.1193 17.7215 4.27807 2.61438 53.3716 31.9635 40.8293 30.721 5.12821 0 40.8719 50.2773 7.27273 22.5705 50.0617 4.73934 51.1628 19.9134 6.71835 11.8081 2.7972 47.8191 71.0434 15.2047 28.8488 31.259 31.1787 14.7368 42.6778 47.7876 16.8889 20.6593 19.3029 22.3423 34.1591 4.4164 25.6131 14.8883 35.9882 15.0649 34.0992 50.3536 24.9661 34.384 22.3565 31.5412 28.833 47.1042 11.8343 26.9542 12.2066 12.5373 33.4448 23.4742 43.4568 20.7792 1.90476 10.0901 17.3442 52.973 23.6735 47.6043 12.9032 13.8408 25.4958 37.6307 9.38967 21.8868 0 21.9653 31.3646 23.6735 16.7364 10.1996 0 37.5297 17.4142 19.6286 39.3593 7.20721 39.7138 0 27.9646 22.0736 65.6304 28.0285 15.3846 5.24017 27.6527 20.8038 0 25.1928 31.1377 12.4711 28.4746 18.1818 29.2683 44.3149 9.42408 23.6311 6.30915 51.7781 42.7522 58.221 3.48432 16.1812 48.9752 36.2812 17.8674 30.303 23.2446 0 22.6804 36.319 3.53982 38.8451 28.2486 12.4567 41.2017 36.036 46.8007 22.8739 9.56938 37.7834 3.61991 0 1.11732 56.3522 10.219 37.1005 13.7931 18.6916 29.9338 37.8378 31.9635 39.7094 35.3276 25.0531 0 19.5382 36.0424 2.06186 32.9177 37.785 39.5699 7.84314 29.9776 59.5664 34.6177 0 0 26.9113 0 0 0 10.5263 0 5.50964 ]

LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:406) AvgLoss: 2.76364 (Xent), [AvgXent: 2.76364, AvgTargetEnt: 0]
progress: []
FRAME_ACCURACY >> 39.4041% <<

