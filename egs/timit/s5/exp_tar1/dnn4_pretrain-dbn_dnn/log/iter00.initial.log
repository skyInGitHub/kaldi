nsclab-gpu
nnet-train-frmshuff --cross-validate=true --randomize=false --verbose=0 --minibatch-size=256 --randomizer-size=32768 --randomizer-seed=777 --feature-transform=exp/dnn4_pretrain-dbn_dnn/final.feature_transform 'ark:copy-feats scp:exp/dnn4_pretrain-dbn_dnn/cv.scp ark:- |' 'ark:ali-to-pdf exp/tri3_ali/final.mdl "ark:gunzip -c exp/tri3_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- |' exp/dnn4_pretrain-dbn_dnn/nnet_dbn_dnn.init 
WARNING (nnet-train-frmshuff[5.5.294-06484]:SelectGpuId():cu-device.cc:221) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:349) Selecting from 2 GPUs
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:364) cudaSetDevice(0): TITAN Xp	free:12023M, used:173M, total:12196M, free/total:0.985815
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:364) cudaSetDevice(1): TITAN Xp	free:11887M, used:305M, total:12192M, free/total:0.974986
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:411) Trying to select device: 0 (automatically), mem_ratio: 0.985815
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:430) Success selecting device 0 free mem ratio: 0.985815
LOG (nnet-train-frmshuff[5.5.294-06484]:FinalizeActiveGpu():cu-device.cc:284) The active GPU is [0]: TITAN Xp	free:11957M, used:239M, total:12196M, free/total:0.980403 version 6.1
copy-feats scp:exp/dnn4_pretrain-dbn_dnn/cv.scp ark:- 
LOG (nnet-train-frmshuff[5.5.294-06484]:Init():nnet-randomizer.cc:32) Seeding by srand with : 777
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:163) CROSS-VALIDATION STARTED
ali-to-post ark:- ark:- 
ali-to-pdf exp/tri3_ali/final.mdl 'ark:gunzip -c exp/tri3_ali/ali.*.gz |' ark:- 
LOG (ali-to-pdf[5.5.294-06484]:main():ali-to-pdf.cc:68) Converted 1232 alignments to pdf sequences.
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:354) ### After 0 frames,
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:355) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -5.97286, max 6.40282, mean -0.0150842, stddev 1.00619, skewness 0.15903, kurtosis 2.04008 ) 
[1] output of <AffineTransform> ( min -21.6687, max 12.4401, mean -3.19116, stddev 2.38235, skewness 0.0687523, kurtosis 2.22495 ) 
[2] output of <Sigmoid> ( min 3.88494e-10, max 0.999996, mean 0.130973, stddev 0.221145, skewness 2.39122, kurtosis 5.06078 ) 
[3] output of <AffineTransform> ( min -27.6391, max 14.7177, mean -3.58118, stddev 2.39949, skewness 0.0252364, kurtosis 3.63272 ) 
[4] output of <Sigmoid> ( min 9.91913e-13, max 1, mean 0.0994363, stddev 0.188502, skewness 3.07386, kurtosis 9.49703 ) 
[5] output of <AffineTransform> ( min -14.7761, max 10.3319, mean -3.43661, stddev 2.01411, skewness 0.829555, kurtosis 3.01444 ) 
[6] output of <Sigmoid> ( min 3.8265e-07, max 0.999967, mean 0.0951758, stddev 0.187358, skewness 3.21346, kurtosis 10.2993 ) 
[7] output of <AffineTransform> ( min -23.3454, max 14.9613, mean -3.65634, stddev 2.04569, skewness 0.817333, kurtosis 5.23752 ) 
[8] output of <Sigmoid> ( min 7.26503e-11, max 1, mean 0.0820351, stddev 0.177832, skewness 3.57653, kurtosis 12.9068 ) 
[9] output of <AffineTransform> ( min -13.0648, max 11.2535, mean -3.76436, stddev 1.87404, skewness 1.78474, kurtosis 6.16507 ) 
[10] output of <Sigmoid> ( min 2.11848e-06, max 0.999987, mean 0.0748277, stddev 0.183228, skewness 3.70966, kurtosis 13.4276 ) 
[11] output of <AffineTransform> ( min -23.1801, max 13.666, mean -3.88479, stddev 1.92959, skewness 1.45702, kurtosis 7.70113 ) 
[12] output of <Sigmoid> ( min 8.57079e-11, max 0.999999, mean 0.0681999, stddev 0.174369, skewness 3.98033, kurtosis 15.6726 ) 
[13] output of <AffineTransform> ( min -3.05538, max 3.06947, mean -0.00626111, stddev 0.578538, skewness 0.0218954, kurtosis 0.194388 ) 
[14] output of <Softmax> ( min 2.23119e-05, max 0.0109977, mean 0.000625, stddev 0.000399826, skewness 2.52108, kurtosis 15.1664 ) 
### END FORWARD

LOG (copy-feats[5.5.294-06484]:main():copy-feats.cc:143) Copied 120 feature matrices.
LOG (ali-to-post[5.5.294-06484]:main():ali-to-post.cc:73) Converted 1232 alignments.
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:384) ### After 37632 frames,
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:385) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -6.44529, max 6.47925, mean -0.00130633, stddev 1.02329, skewness 0.0497203, kurtosis 2.08974 ) 
[1] output of <AffineTransform> ( min -21.5319, max 10.6335, mean -3.19142, stddev 2.41107, skewness 0.0527206, kurtosis 2.08649 ) 
[2] output of <Sigmoid> ( min 4.4547e-10, max 0.999976, mean 0.133692, stddev 0.224379, skewness 2.33108, kurtosis 4.72693 ) 
[3] output of <AffineTransform> ( min -26.4777, max 14.8643, mean -3.59551, stddev 2.43579, skewness -0.00893111, kurtosis 3.43936 ) 
[4] output of <Sigmoid> ( min 3.16864e-12, max 1, mean 0.101287, stddev 0.19114, skewness 2.99633, kurtosis 8.93473 ) 
[5] output of <AffineTransform> ( min -14.5742, max 10.2991, mean -3.4381, stddev 2.03695, skewness 0.781445, kurtosis 2.69569 ) 
[6] output of <Sigmoid> ( min 4.68296e-07, max 0.999966, mean 0.0972745, stddev 0.189858, skewness 3.11862, kurtosis 9.63474 ) 
[7] output of <AffineTransform> ( min -21.847, max 13.4435, mean -3.65796, stddev 2.06249, skewness 0.741685, kurtosis 4.80145 ) 
[8] output of <Sigmoid> ( min 3.25072e-10, max 0.999999, mean 0.0836203, stddev 0.179376, skewness 3.48458, kurtosis 12.2067 ) 
[9] output of <AffineTransform> ( min -13.0403, max 10.3998, mean -3.76317, stddev 1.88256, skewness 1.71726, kurtosis 5.72823 ) 
[10] output of <Sigmoid> ( min 2.171e-06, max 0.99997, mean 0.0760323, stddev 0.183975, skewness 3.63894, kurtosis 12.9293 ) 
[11] output of <AffineTransform> ( min -20.3867, max 12.7342, mean -3.883, stddev 1.93384, skewness 1.35557, kurtosis 7.17499 ) 
[12] output of <Sigmoid> ( min 1.40017e-09, max 0.999997, mean 0.0689729, stddev 0.174691, skewness 3.93193, kurtosis 15.2957 ) 
[13] output of <AffineTransform> ( min -3.13088, max 3.14975, mean -0.00600628, stddev 0.581346, skewness -0.0011224, kurtosis 0.28011 ) 
[14] output of <Softmax> ( min 2.08143e-05, max 0.0112862, mean 0.000625001, stddev 0.000399521, skewness 2.5006, kurtosis 14.5558 ) 
### END FORWARD

LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:395) Done 120 files, 0 with no tgt_mats, 0 with other errors. [CROSS-VALIDATION, NOT-RANDOMIZED, 0.00758622 min, processing 82676.3 frames per sec; i/o time 23.9331%]
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:405) PER-CLASS PERFORMANCE:
@@@ Frames per-class: [ 1317 36 35 8 0 0 36 50 18 23 32 7 131 19 21 14 13 10 5 9 18 22 11 28 41 10 5 28 8 23 39 15 18 6 68 27 19 54 73 16 0 0 16 25 58 23 9 23 1838 593 174 49 78 3 12 19 42 21 41 22 17 23 0 4 14 8 22 15 47 25 27 41 30 15 129 14 106 51 0 0 82 29 33 25 43 0 70 18 15 25 4 14 30 13 14 1 15 26 16 13 15 26 22 37 25 8 22 23 18 41 18 33 28 57 14 9 18 8 0 38 14 36 28 22 6 34 12 10 0 13 18 8 21 8 50 0 63 8 25 10 31 13 138 6 6 16 29 0 11 33 29 14 19 13 27 13 23 5 36 0 27 13 37 8 0 14 12 4 6 8 14 50 3 5 10 22 2 48 16 16 28 6 4 0 23 10 15 21 22 16 9 23 0 18 9 32 28 19 23 29 11 6 18 11 11 10 14 12 13 21 30 12 16 31 6 20 11 14 4 2 11 51 14 16 19 16 23 22 29 4 42 8 9 0 0 2 15 15 19 22 51 22 16 101 27 0 18 41 51 9 28 1 0 12 17 7 32 31 7 16 22 36 21 69 44 11 26 25 7 45 2 0 7 19 25 11 0 0 0 3 23 35 21 27 14 25 8 9 1 8 7 71 70 41 10 0 20 0 27 13 10 93 11 10 0 12 3 23 23 21 3 16 16 82 12 13 27 12 0 46 38 33 14 6 27 20 4 8 0 0 24 22 18 7 15 16 46 8 21 22 21 15 11 0 22 9 19 17 22 6 8 13 36 27 23 6 10 8 3 13 0 6 64 19 0 8 15 5 41 26 87 23 6 0 8 123 6 64 11 31 3 0 0 0 6 16 4 0 12 0 10 0 35 9 12 22 20 5 26 6 30 16 22 47 19 22 5 3 12 19 55 33 67 0 13 22 10 26 8 38 5 3 0 24 32 27 18 20 10 4 25 33 15 29 4 0 42 27 35 5 13 5 9 6 23 1 12 20 14 19 46 0 18 0 57 16 35 68 13 27 13 11 29 12 17 25 0 14 12 8 9 29 16 30 4 22 26 13 3 20 3 14 11 0 79 16 0 16 7 5 35 46 103 0 17 23 25 0 14 29 30 0 0 9 26 16 30 13 17 26 0 44 10 8 28 8 44 15 31 0 13 23 0 25 12 6 2 18 7 14 18 16 10 6 23 5 12 0 2 11 16 20 0 23 5 8 31 56 30 15 0 20 4 20 21 47 9 29 0 18 9 0 21 35 9 0 15 25 30 40 30 23 26 32 36 47 6 29 15 13 3 11 7 14 0 29 13 0 24 87 9 32 20 30 23 13 17 39 32 30 10 9 11 10 22 28 26 0 31 9 0 24 12 7 9 9 24 42 41 0 4 5 2 14 55 14 2 14 28 25 23 20 0 25 12 42 10 51 13 4 8 3 9 11 7 13 42 11 9 19 0 36 14 15 8 15 8 0 43 33 0 20 23 40 0 3 12 10 12 5 3 4 33 35 4 16 4 10 20 23 25 14 18 42 0 21 31 18 63 27 25 25 12 0 24 20 90 9 98 5 13 0 1 22 25 18 20 14 11 29 23 28 0 18 0 0 21 6 0 0 0 41 15 0 1 13 2 33 0 21 18 3 28 40 27 0 16 9 20 7 8 21 14 6 34 9 27 20 0 12 66 23 31 0 18 14 81 48 6 19 28 12 50 12 25 18 12 29 6 27 26 4 0 22 4 15 23 52 14 22 12 0 25 41 46 0 15 0 18 44 18 0 19 12 25 29 15 44 19 15 10 37 14 11 0 19 11 0 30 15 15 14 7 0 5 15 18 15 106 61 17 37 21 7 0 67 5 27 49 49 20 3 50 0 18 25 41 5 33 0 10 11 16 20 59 23 7 74 12 6 49 32 55 10 31 17 0 23 8 19 4 20 8 0 2 47 19 20 49 38 11 31 0 3 26 0 0 15 22 5 48 2 17 6 26 20 50 9 20 33 39 28 29 17 28 23 30 3 28 69 12 13 89 0 7 27 11 0 40 35 83 6 36 5 29 56 5 6 8 19 31 7 19 40 38 50 6 23 27 8 0 0 0 12 25 36 25 7 19 0 13 33 16 14 16 51 15 8 18 21 13 0 4 26 26 7 12 31 19 15 0 37 16 8 58 10 7 9 16 0 0 12 3 22 31 8 7 15 15 77 21 34 15 9 6 24 4 11 4 20 0 32 23 29 3 1 26 16 16 11 14 42 4 0 8 5 29 20 41 46 9 39 0 0 9 73 8 30 17 0 8 8 0 28 37 42 30 36 0 6 4 0 52 42 11 17 34 15 21 11 22 32 62 16 62 24 46 20 17 0 30 111 11 12 28 52 43 236 14 27 25 70 67 23 19 9 44 29 0 6 62 22 33 15 33 8 6 10 9 4 0 19 0 52 30 43 14 77 28 72 23 19 35 33 22 24 42 163 45 0 0 30 40 10 0 31 40 25 26 30 100 50 57 1 23 30 0 16 2 38 0 11 12 28 33 9 23 13 0 17 28 25 25 4 22 43 18 27 15 4 35 9 0 40 21 13 27 0 7 5 7 0 0 16 39 18 18 10 6 30 4 0 12 27 60 29 5 35 69 5 11 33 4 13 37 23 18 25 8 0 34 12 7 4 31 19 15 25 31 6 34 10 7 24 17 8 0 11 11 41 42 0 9 13 16 8 11 59 19 38 6 22 0 50 40 18 1 10 9 39 42 8 15 88 5 39 33 10 0 4 20 0 17 20 0 16 12 11 8 23 110 4 2 32 14 9 8 12 34 43 0 12 51 81 39 21 4 25 47 0 10 12 28 7 3 21 0 24 2 30 19 9 58 21 8 26 8 14 0 31 27 40 41 34 23 19 0 16 41 8 17 59 17 29 12 25 0 12 22 18 35 11 22 0 18 20 35 49 9 45 24 12 12 19 12 24 47 20 19 17 11 14 7 17 7 34 77 30 18 16 8 29 18 44 33 50 17 15 10 50 53 6 35 16 16 9 35 26 5 19 57 0 5 9 23 14 28 14 32 18 16 8 11 15 8 24 23 0 23 12 6 19 56 28 76 18 35 16 36 46 35 47 20 23 30 36 11 27 0 10 118 16 0 24 1 29 23 7 43 20 31 0 124 0 16 9 15 1 7 23 60 14 8 57 17 7 2 56 10 46 28 79 0 24 15 61 25 13 101 26 30 59 21 0 28 116 0 0 0 34 29 56 61 42 8 7 45 33 22 5 7 20 29 36 0 26 27 8 31 1 5 17 5 43 37 2 14 38 51 11 12 69 15 40 23 35 22 18 5 17 19 28 43 22 41 19 8 15 26 11 16 5 14 28 23 15 28 2 15 52 22 38 7 52 2 17 15 6 0 16 0 5 20 13 24 11 0 5 20 22 28 0 24 0 13 0 123 10 0 16 12 23 5 22 43 28 16 12 4 12 12 15 26 14 111 50 19 32 48 24 0 22 33 0 20 46 2 20 21 31 33 13 22 31 13 13 22 0 5 34 11 67 16 19 29 39 7 20 14 36 4 13 6 2 45 10 54 22 38 30 63 8 5 52 14 10 8 7 7 25 ]
@@@ Loss per-class: [ 6.79658 7.39707 7.423 6.4309 0 0 7.41739 6.45346 6.94204 6.97473 7.49664 6.68555 7.62068 7.12353 6.4052 6.92192 7.04298 7.39169 6.98784 6.79186 7.3863 7.20424 7.3838 8.47061 7.984 7.57242 6.69729 6.31177 6.61801 7.68543 7.31189 7.00931 7.44904 6.46915 7.11842 8.14243 7.25455 7.48726 7.19744 6.92299 0 0 7.71291 7.90865 7.37318 7.82449 7.71973 7.93982 7.36864 7.2023 7.07152 7.61488 7.58126 6.61741 7.40183 7.53111 7.4335 7.26052 8.04513 6.773 6.76691 8.00641 0 6.62678 7.38332 7.16126 8.04392 6.68167 7.06156 7.2788 7.78894 8.17116 7.60916 6.44117 7.22289 7.70093 6.48927 7.08674 0 0 7.54562 7.60902 7.28755 8.13534 7.51381 0 7.13915 7.3903 7.06347 7.10103 7.25212 7.64927 7.672 6.95857 7.39283 5.4965 7.25457 7.73782 7.20107 7.3999 6.88611 7.19884 7.89088 8.3261 7.74787 7.9016 7.34265 7.41095 7.7411 7.33676 6.86966 7.3473 6.6405 7.82067 7.3445 7.24909 7.38556 6.25604 0 7.63248 6.93645 7.08482 7.90231 6.97118 6.74197 7.24259 7.85444 7.49214 0 7.27236 7.2706 6.42268 7.10463 6.84611 7.02346 0 8.1096 7.15537 7.6545 7.71794 8.08532 6.73947 7.27586 7.22943 6.63576 7.32074 7.42181 0 7.39638 7.66835 7.62821 7.9118 7.55477 7.14999 7.39292 7.71161 6.46964 7.43459 7.21398 0 7.48583 7.2297 7.7961 7.0656 0 7.88019 7.71993 6.19997 6.80411 7.34329 7.68005 6.98981 5.52128 7.05849 8.20064 7.19536 5.93681 7.47493 7.10519 7.87644 6.71503 6.90648 6.0549 0 7.92533 7.75747 7.46783 7.23376 7.70483 6.30884 6.67598 7.92901 0 7.02375 6.99116 7.11154 7.05502 7.07626 7.51128 7.54559 6.93275 6.86135 6.88805 6.49895 7.73482 7.18893 7.25845 7.97962 7.37067 7.66562 7.5685 8.07355 6.06093 7.8135 6.78099 8.05121 7.24256 8.63491 6.9482 5.95717 7.14472 7.76986 6.76923 7.54222 7.69238 7.07775 7.2524 7.56973 8.13162 6.7676 6.93441 7.22291 6.93328 0 0 6.10573 7.11638 8.22547 7.32472 7.35112 7.54905 6.74641 8.02466 7.63832 6.79172 0 6.71922 8.25065 7.83342 7.29872 7.62973 4.9115 0 6.83994 7.19702 7.06757 8.0549 8.09365 7.00203 7.19308 7.06201 7.35746 6.86379 7.21058 8.07411 7.32271 7.85816 7.23602 7.14249 7.02222 5.4942 0 7.13825 7.31958 7.32099 7.5138 0 0 0 6.7988 7.83692 8.33483 7.50428 6.95081 7.51938 7.4006 6.64695 7.08189 4.66162 7.11642 6.9689 7.55027 7.35458 7.91857 6.57028 0 7.38123 0 6.95551 6.85203 7.87025 7.48321 7.31253 8.30883 0 7.43029 5.34064 7.58551 6.73885 6.48946 6.4686 6.68291 6.876 7.20948 6.17457 6.94893 7.29444 7.55379 0 7.97136 7.38072 6.9711 7.85585 7.16976 7.40248 6.35872 6.36952 7.25828 0 0 7.72836 8.44035 7.47079 6.73317 7.12167 7.32626 7.08062 7.3478 7.86306 7.15319 7.25617 7.05904 6.91927 0 7.16564 7.01505 7.52802 7.14657 7.16914 7.05556 6.78475 7.27553 6.88947 7.70895 7.85484 6.98097 6.68901 7.24809 6.11232 7.0224 0 8.09111 7.6418 6.42829 0 6.77528 7.28721 6.41663 7.17833 8.03866 7.73477 7.57677 7.22661 0 6.45024 6.98129 7.02375 7.48426 6.92861 6.78059 6.26325 0 0 0 7.06841 8.32886 7.18238 0 7.02451 0 6.65377 0 7.68233 7.49436 7.53797 7.6107 7.76186 6.67028 6.83267 7.0718 7.22973 7.35468 7.53471 6.95276 7.42378 7.0943 7.44975 6.05574 6.87953 8.62877 8.06111 7.43149 7.23928 0 6.37827 7.58364 6.72511 7.8342 7.89716 7.63376 6.4827 6.21555 0 8.0496 7.13574 7.08939 7.21266 6.99641 6.41248 6.93505 7.66893 7.13744 7.40914 7.09556 6.00917 0 7.4332 8.44449 7.51533 7.28857 7.07536 7.84597 6.99424 7.06419 7.44114 4.03448 8.09829 7.84772 6.98815 7.10057 8.04009 0 6.94822 0 7.15616 7.34181 7.66836 8.27101 7.94449 8.02531 7.34838 7.03322 7.30022 7.26252 7.0398 8.07308 0 7.67104 6.86359 7.14212 6.41769 7.39105 7.45121 6.55361 6.03741 7.7498 7.04698 7.84779 6.74176 7.60277 6.21039 7.23106 7.2953 0 7.45158 8.06448 0 7.022 7.44094 6.01702 7.65408 8.6098 7.10195 0 7.46206 7.14876 7.66661 0 7.40348 7.29922 7.37574 0 0 7.58522 8.02249 7.42046 7.33967 7.98697 7.97038 7.70541 0 6.53627 7.19945 7.63743 7.49625 6.9897 7.23581 6.58679 8.24268 0 6.82113 7.46108 0 7.48655 7.0874 7.52468 5.53199 6.78939 8.25858 6.98893 6.87363 7.77255 6.95444 7.58759 7.25857 6.51939 8.26078 0 5.94536 7.11516 7.71496 6.76594 0 7.88546 6.33459 7.71177 7.31126 6.9719 7.44505 7.45386 0 7.13054 6.19091 6.85256 7.38855 6.82322 7.52312 6.84709 0 7.42478 6.56832 0 6.649 7.30391 7.28946 0 7.51104 7.06999 7.31542 7.64168 7.37326 7.42316 6.70348 8.00045 7.01272 7.92736 7.64822 7.69232 6.77002 6.92671 6.02395 7.5308 7.36859 6.48028 0 7.33995 7.92826 0 7.31387 7.5402 7.30928 7.98988 7.35942 7.24282 8.08687 7.14908 8.14959 7.55026 7.3009 6.98848 6.88654 6.56947 6.6301 7.6599 7.35944 7.44543 7.48409 0 8.16853 6.92068 0 7.71081 6.87836 7.36264 6.86138 7.56772 8.10171 7.196 7.68488 0 6.97894 7.39593 6.17195 7.51839 6.80568 7.09795 5.85678 7.32039 7.96323 7.271 6.60417 7.308 0 7.51009 7.75839 7.3261 7.17601 7.0685 6.71013 6.38032 6.84986 6.20987 6.68281 8.29111 7.41294 7.67566 7.0534 7.69458 6.86623 7.72787 0 7.37768 7.8599 7.73233 7.30477 6.86702 7.13646 0 7.56444 7.75855 0 7.27883 6.93016 7.14305 0 5.64498 7.7489 7.45377 7.9642 6.22364 6.10985 6.39392 7.05934 7.7049 6.88657 7.25045 7.72141 7.02554 8.08814 6.92323 8.1471 7.50933 7.17834 8.72741 0 7.01521 7.14268 7.66727 7.51242 7.97271 7.50398 8.11779 7.45563 0 7.04411 7.44565 7.50985 7.42867 7.52212 6.7537 7.32603 0 4.30621 6.79078 7.322 8.0906 7.23596 8.05764 6.7585 7.28567 6.6304 6.78732 0 6.81647 0 0 6.93946 7.42099 0 0 0 7.83634 6.65674 0 4.955 7.21611 6.1188 7.26272 0 7.39578 7.39829 6.26856 7.61156 7.87567 7.30157 0 7.29159 8.43641 7.2043 6.20152 6.72782 6.90895 7.43374 8.03765 7.71104 8.07974 7.66232 7.16561 0 7.58933 6.35004 7.76644 7.43588 0 6.83162 7.48827 7.4941 7.96711 6.82455 7.07988 6.70917 7.45654 7.65627 7.29636 7.40026 6.37099 6.75146 7.39187 6.64617 7.60487 7.08578 6.08756 0 6.70448 7.17375 6.15047 8.22142 7.55552 7.16409 7.02724 7.27347 0 7.38101 7.17888 7.79833 0 7.174 0 6.32469 6.7215 7.00172 0 6.95896 7.69479 8.25074 7.92279 7.10349 6.47105 7.0413 7.49767 7.32155 7.56126 7.0093 7.05297 0 7.73856 6.8923 0 8.47298 7.35525 7.2155 7.01986 7.04118 0 6.75777 7.61668 6.96534 7.31802 7.48986 6.92436 7.62474 7.29586 7.04452 7.2491 0 7.99249 7.57796 7.72163 7.76757 7.8701 7.08521 5.9589 7.85447 0 6.73677 7.88264 7.02259 6.94682 7.30096 0 6.35056 7.59056 7.31214 8.03884 7.36371 7.8295 7.61908 7.05008 7.6336 7.30084 7.12488 7.36895 8.28966 7.95322 7.37223 7.10297 0 6.47051 7.0251 6.83815 6.11677 7.52191 7.15375 0 6.17851 7.78016 7.78434 8.19044 7.48506 8.28282 7.35207 8.00192 0 6.36568 7.3984 0 0 7.72751 7.46613 7.14994 6.73152 5.27743 7.8009 6.84388 7.20555 7.26524 7.58814 6.7385 8.14199 7.2673 7.73395 7.80226 7.43974 7.24481 6.3351 6.92856 7.5726 5.98219 6.81235 8.4812 7.88773 7.34534 8.39741 0 6.841 6.98871 6.85017 0 7.63168 7.2986 7.28201 5.96056 7.53239 7.00306 7.30416 7.40311 7.29551 7.46005 7.0706 6.89389 7.30826 6.40776 6.95147 7.2197 7.33923 6.80585 7.15217 7.32137 6.80913 6.92185 0 0 0 7.15073 7.15152 6.8722 7.09103 7.11685 7.56398 0 7.12447 7.15392 6.59389 7.26478 7.33896 7.45663 6.46953 6.72139 6.9303 7.50967 6.53329 0 8.23596 7.62535 7.32868 6.57197 7.65683 6.99424 6.737 7.56278 0 7.27197 7.95078 6.74649 7.47306 8.20838 6.3081 7.31112 7.24354 0 0 6.86891 6.28967 7.46242 6.84233 7.30996 7.17862 6.07966 7.09017 7.36607 7.77258 6.94737 7.89053 6.51221 6.57969 7.84064 6.75441 7.37297 6.91617 7.22906 0 7.01834 7.13059 7.43213 6.80717 4.70069 7.30792 6.50775 7.29641 7.1055 7.3663 7.46205 6.2686 0 6.596 6.97607 6.83413 7.50159 8.03339 7.60796 6.9393 7.44476 0 0 6.40038 7.15129 7.16588 7.25314 7.24758 0 7.86896 7.67697 0 7.38403 7.31725 7.11201 7.03524 7.81996 0 6.91735 6.9888 0 7.24395 8.00733 7.16271 7.79728 7.08736 7.9649 7.82708 7.02124 7.29232 7.60183 7.2801 7.1766 7.52144 7.6968 7.44735 7.4466 7.6616 0 7.75818 7.80678 7.4062 7.8939 7.0035 6.67441 7.80612 7.40857 7.55978 7.60248 7.62226 6.92523 6.89147 7.13757 6.81301 6.92606 7.81151 7.45722 0 6.42241 6.82696 7.53193 7.24388 6.40724 7.28244 6.98959 7.112 6.97412 6.9396 6.81498 0 7.13863 0 6.9705 6.69199 7.54931 6.46523 7.07043 7.28158 7.95526 7.48397 7.28098 7.68433 7.63018 6.95456 7.14783 7.96189 7.27156 7.50141 0 0 7.77705 7.00313 6.57391 0 7.76436 7.64698 7.68583 7.14048 7.01287 7.31441 7.45599 7.86975 4.68958 7.89421 7.33462 0 7.64062 6.34628 7.61847 0 6.89677 7.28897 6.88976 7.31397 7.10893 7.40322 7.09388 0 7.4991 6.91144 7.69365 6.86509 6.02208 7.80204 6.89474 7.2463 6.84242 7.15274 6.74364 7.25958 7.57118 0 6.57104 7.53872 7.45326 6.46558 0 6.92931 7.62515 7.10191 0 0 7.08086 8.08725 7.24254 7.25254 6.91625 8.32201 7.53607 7.47559 0 7.25098 6.86131 7.77745 7.2712 7.21748 7.43678 6.61139 6.54756 7.70661 7.80298 5.47886 8.09878 7.71018 7.57176 7.83038 7.25347 7.25052 0 7.40377 7.18112 6.81539 5.87122 7.06007 8.03845 7.02645 7.7014 7.04139 6.55715 7.76934 7.95738 6.83112 7.09016 7.8152 7.01803 0 6.95552 6.86202 7.2006 7.16299 0 7.00521 7.2185 7.30428 6.95677 7.26205 6.76193 7.42963 7.21743 5.84812 7.34898 0 7.76616 7.05597 7.66626 5.38968 8.17127 7.35194 7.3172 7.89643 6.64494 7.42834 7.49044 7.29256 7.81994 6.86182 6.69072 0 6.50916 7.46943 0 7.44549 6.92517 0 7.02818 6.42311 7.41161 7.46856 7.40409 8.12201 7.63864 6.16013 7.83542 7.97821 6.60757 6.67668 6.53172 7.18639 7.75302 0 7.63896 8.13393 7.58539 7.32448 7.39285 6.1207 7.20105 7.24293 0 7.0226 6.9591 7.84309 6.62054 6.58845 7.43917 0 6.8186 5.69675 7.70678 7.0488 6.94503 7.33894 7.77932 7.53082 7.08056 7.39026 6.86216 0 6.40363 7.19648 7.64075 7.39945 7.14264 7.42277 6.68448 0 7.69926 7.29803 7.35486 7.17191 7.46785 7.62604 7.49148 7.2429 7.94346 0 7.07364 7.17767 7.81637 7.4221 7.32962 7.55833 0 8.20377 7.38591 8.09896 8.46919 6.63271 7.0585 7.70258 6.76103 6.93931 7.99154 7.41667 7.17679 6.70756 7.23658 7.33584 7.28104 6.1121 7.36544 6.32163 7.25111 6.41571 6.82503 7.16347 7.25852 7.7938 7.46462 7.14182 6.96698 6.92369 7.20678 7.68892 6.99959 7.39494 6.7125 7.16218 7.27259 7.33042 6.70381 7.98576 7.19865 7.51754 7.30838 7.46708 7.46519 7.70075 7.50645 7.92969 0 7.31497 7.31979 7.75191 7.49527 7.47607 6.86825 7.77289 6.91925 7.21022 7.14519 7.13102 6.67737 7.58022 7.17394 7.07139 0 8.05241 6.35593 6.97311 7.26939 7.68575 7.23117 6.89217 6.95176 7.12579 7.28802 7.09382 7.31891 7.84314 6.24843 7.41774 7.50974 7.78251 6.85239 7.09321 7.61574 0 6.98915 7.42543 6.84481 0 7.18523 4.80379 8.12904 7.41537 7.49341 7.20117 7.50851 8.25533 0 8.1732 0 7.4548 6.38147 8.3241 5.06485 6.85934 7.57123 7.18545 7.80315 6.64659 7.24178 7.17986 6.56989 6.12544 7.36133 6.87585 7.24835 7.71307 7.83771 0 7.9224 6.95833 7.9597 8.03154 7.86251 7.59484 6.73836 7.23855 7.78624 7.23674 0 7.4609 7.1607 0 0 0 7.15601 7.24085 7.97969 7.00418 7.31062 6.79145 7.42692 7.33839 7.71399 7.39744 6.68447 6.60498 7.32492 7.45096 7.7345 0 7.01111 7.87181 6.60194 7.59059 5.09321 6.42173 7.43895 7.33864 7.64937 6.89727 5.97228 7.54428 8.08193 6.72465 6.59981 6.72431 7.3506 6.89385 7.99103 8.18049 7.59474 7.4803 6.91122 7.68098 6.33483 6.8387 7.0907 7.2384 8.24981 7.71876 6.96223 6.92549 7.56084 7.69174 7.31707 7.30314 6.18106 6.58972 8.21307 7.59296 7.43491 7.02411 5.63407 6.21605 7.51095 7.59859 6.99585 7.0576 8.02593 5.75031 7.21846 7.22306 6.8408 0 7.75162 0 6.3264 7.16759 6.69787 6.85748 7.68202 0 6.51547 7.77038 7.8839 7.22227 0 8.0026 0 7.2264 0 7.79999 7.13066 0 6.89489 7.73597 7.52419 5.83929 7.70595 7.45939 8.0302 7.69794 7.03549 6.47173 6.77162 6.38376 7.2588 6.80683 7.8093 8.10315 7.57891 7.48004 7.40408 7.24151 7.37517 0 7.53782 7.24334 0 6.70124 7.88915 6.63701 7.15815 7.58308 7.94901 7.50028 6.66086 6.78502 7.43704 7.60126 7.48085 7.49925 0 6.44177 7.82859 7.09812 7.41716 7.46523 7.25101 7.43401 7.91867 7.1113 7.20303 7.02414 6.95405 6.77867 7.74022 7.66103 5.71246 7.64976 6.47205 8.0513 6.96352 7.49495 7.02369 7.30545 7.74642 6.68701 7.15683 7.11837 7.1833 6.8865 7.10502 7.63624 7.83554 ]
@@@ Frame-accuracy per-class: [ 0.0759013 0 0 0 0 0 0 3.9604 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7.01754 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 28.5714 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6.06061 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2.24719 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1.50376 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 11.236 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.80952 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2.46914 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 22.2222 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.8835 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ]

LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:406) AvgLoss: 7.48893 (Xent), [AvgXent: 7.48893, AvgTargetEnt: 0]
progress: []
FRAME_ACCURACY >> 0.0531463% <<

