nsclab-gpu
nnet-train-frmshuff --cross-validate=true --randomize=false --verbose=0 --minibatch-size=256 --randomizer-size=32768 --randomizer-seed=777 --feature-transform=exp/dnn4_pretrain-dbn_dnn/final.feature_transform 'ark:copy-feats scp:exp/dnn4_pretrain-dbn_dnn/cv.scp ark:- |' 'ark:ali-to-pdf exp/tri3_ali/final.mdl "ark:gunzip -c exp/tri3_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- |' exp/dnn4_pretrain-dbn_dnn/nnet_dbn_dnn.init 
WARNING (nnet-train-frmshuff[5.5.294-06484]:SelectGpuId():cu-device.cc:221) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:349) Selecting from 2 GPUs
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:364) cudaSetDevice(0): TITAN Xp	free:12023M, used:173M, total:12196M, free/total:0.985815
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:364) cudaSetDevice(1): TITAN Xp	free:11888M, used:304M, total:12192M, free/total:0.975032
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:411) Trying to select device: 0 (automatically), mem_ratio: 0.985815
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:430) Success selecting device 0 free mem ratio: 0.985815
LOG (nnet-train-frmshuff[5.5.294-06484]:FinalizeActiveGpu():cu-device.cc:284) The active GPU is [0]: TITAN Xp	free:11957M, used:239M, total:12196M, free/total:0.980403 version 6.1
copy-feats scp:exp/dnn4_pretrain-dbn_dnn/cv.scp ark:- 
LOG (nnet-train-frmshuff[5.5.294-06484]:Init():nnet-randomizer.cc:32) Seeding by srand with : 777
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:163) CROSS-VALIDATION STARTED
ali-to-post ark:- ark:- 
ali-to-pdf exp/tri3_ali/final.mdl 'ark:gunzip -c exp/tri3_ali/ali.*.gz |' ark:- 
LOG (ali-to-pdf[5.5.294-06484]:main():ali-to-pdf.cc:68) Converted 1232 alignments to pdf sequences.
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:354) ### After 0 frames,
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:355) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -6.64564, max 6.95883, mean -0.0166641, stddev 1.06164, skewness 0.0499633, kurtosis 2.23964 ) 
[1] output of <AffineTransform> ( min -23.9905, max 12.9606, mean -3.15476, stddev 2.47303, skewness 0.0444229, kurtosis 2.47487 ) 
[2] output of <Sigmoid> ( min 3.81135e-11, max 0.999998, mean 0.138052, stddev 0.227655, skewness 2.2595, kurtosis 4.37482 ) 
[3] output of <AffineTransform> ( min -23.6332, max 15.389, mean -3.62449, stddev 2.45393, skewness 0.0438858, kurtosis 3.32388 ) 
[4] output of <Sigmoid> ( min 5.44803e-11, max 1, mean 0.100607, stddev 0.191003, skewness 3.00081, kurtosis 8.97589 ) 
[5] output of <AffineTransform> ( min -15.7598, max 12.4166, mean -3.44599, stddev 2.04707, skewness 0.798397, kurtosis 2.91506 ) 
[6] output of <Sigmoid> ( min 1.43093e-07, max 0.999996, mean 0.0970311, stddev 0.188058, skewness 3.11168, kurtosis 9.68366 ) 
[7] output of <AffineTransform> ( min -22.3118, max 13.7416, mean -3.63709, stddev 2.04158, skewness 0.75674, kurtosis 4.66992 ) 
[8] output of <Sigmoid> ( min 2.04215e-10, max 0.999999, mean 0.0835375, stddev 0.17754, skewness 3.50062, kurtosis 12.4129 ) 
[9] output of <AffineTransform> ( min -13.6439, max 13.8565, mean -3.77198, stddev 1.86938, skewness 1.669, kurtosis 5.9053 ) 
[10] output of <Sigmoid> ( min 1.18723e-06, max 0.999999, mean 0.0745559, stddev 0.17918, skewness 3.69759, kurtosis 13.5364 ) 
[11] output of <AffineTransform> ( min -20.9199, max 12.7039, mean -3.86522, stddev 1.89072, skewness 1.41181, kurtosis 7.46102 ) 
[12] output of <Sigmoid> ( min 8.21494e-10, max 0.999997, mean 0.0676479, stddev 0.17088, skewness 4.00866, kurtosis 16.0351 ) 
[13] output of <AffineTransform> ( min -2.98856, max 2.79105, mean 0.00215562, stddev 0.580054, skewness 0.0120086, kurtosis 0.224381 ) 
[14] output of <Softmax> ( min 2.61406e-05, max 0.00829713, mean 0.000647669, stddev 0.000412258, skewness 2.3878, kurtosis 12.1347 ) 
### END FORWARD

LOG (ali-to-post[5.5.294-06484]:main():ali-to-post.cc:73) Converted 1232 alignments.
LOG (copy-feats[5.5.294-06484]:main():copy-feats.cc:143) Copied 120 feature matrices.
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:384) ### After 36096 frames,
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:385) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -5.3553, max 5.23382, mean 0.00714401, stddev 1.01911, skewness -0.0427393, kurtosis 1.35459 ) 
[1] output of <AffineTransform> ( min -19.6654, max 10.3516, mean -3.14179, stddev 2.37741, skewness -0.00150419, kurtosis 1.93902 ) 
[2] output of <Sigmoid> ( min 2.88021e-09, max 0.999968, mean 0.135664, stddev 0.221139, skewness 2.28459, kurtosis 4.56563 ) 
[3] output of <AffineTransform> ( min -23.4361, max 14.962, mean -3.60227, stddev 2.38676, skewness -0.0637996, kurtosis 2.86287 ) 
[4] output of <Sigmoid> ( min 6.63503e-11, max 1, mean 0.099882, stddev 0.187118, skewness 3.01231, kurtosis 9.14257 ) 
[5] output of <AffineTransform> ( min -15.6139, max 10.0976, mean -3.42244, stddev 1.99434, skewness 0.684274, kurtosis 2.60167 ) 
[6] output of <Sigmoid> ( min 1.65571e-07, max 0.999959, mean 0.0959063, stddev 0.183707, skewness 3.15608, kurtosis 10.0662 ) 
[7] output of <AffineTransform> ( min -20.7285, max 13.6233, mean -3.61045, stddev 1.99794, skewness 0.63985, kurtosis 4.4126 ) 
[8] output of <Sigmoid> ( min 9.94777e-10, max 0.999999, mean 0.0828368, stddev 0.174257, skewness 3.54564, kurtosis 12.8235 ) 
[9] output of <AffineTransform> ( min -14.1634, max 10.9682, mean -3.75173, stddev 1.83103, skewness 1.59652, kurtosis 5.74338 ) 
[10] output of <Sigmoid> ( min 7.06175e-07, max 0.999983, mean 0.0733859, stddev 0.175895, skewness 3.7799, kurtosis 14.2394 ) 
[11] output of <AffineTransform> ( min -20.0334, max 12.4989, mean -3.84656, stddev 1.85723, skewness 1.30744, kurtosis 7.23483 ) 
[12] output of <Sigmoid> ( min 1.99354e-09, max 0.999996, mean 0.0667433, stddev 0.167883, skewness 4.08487, kurtosis 16.7463 ) 
[13] output of <AffineTransform> ( min -2.97582, max 2.88571, mean -0.00101109, stddev 0.568692, skewness 0.00887512, kurtosis 0.273146 ) 
[14] output of <Softmax> ( min 2.58537e-05, max 0.00919363, mean 0.000647666, stddev 0.000403518, skewness 2.38619, kurtosis 12.6396 ) 
### END FORWARD

LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:395) Done 120 files, 0 with no tgt_mats, 0 with other errors. [CROSS-VALIDATION, NOT-RANDOMIZED, 0.0070759 min, processing 85021 frames per sec; i/o time 24.7515%]
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:405) PER-CLASS PERFORMANCE:
@@@ Frames per-class: [ 1188 0 10 0 53 16 14 0 31 79 26 21 66 10 41 28 26 9 26 60 24 23 7 7 24 11 56 41 56 29 8 4 9 10 77 6 69 32 34 15 3 41 24 18 65 17 6 6 668 160 4 1629 0 42 4 41 41 25 0 9 7 105 5 8 33 7 0 51 156 56 16 9 38 12 3 20 9 114 52 8 14 0 24 37 31 13 60 40 13 20 29 29 33 85 5 32 17 38 9 24 7 28 27 2 13 15 7 19 21 91 26 29 13 23 11 33 20 10 11 8 131 8 37 8 23 8 39 1 3 26 14 19 10 0 11 0 0 7 13 32 25 5 17 12 22 22 19 0 32 14 6 16 30 44 13 18 17 13 17 44 15 15 4 2 16 50 72 16 27 16 22 13 12 11 13 6 37 13 8 30 20 31 21 34 3 28 11 8 19 6 12 17 5 7 17 36 0 31 28 22 11 23 56 14 8 12 69 30 31 17 13 4 18 15 14 11 6 32 10 1 28 41 23 11 0 11 13 28 0 11 22 6 10 14 14 10 22 17 74 13 46 18 13 29 12 0 0 27 8 14 20 7 19 30 4 7 49 0 25 13 13 15 8 0 0 15 41 5 6 8 0 7 23 39 16 64 0 23 8 13 0 0 49 37 10 10 7 63 14 29 23 22 35 18 26 30 6 18 42 30 28 17 0 17 27 13 19 17 9 3 62 11 22 8 0 4 18 14 13 16 20 7 36 33 6 48 17 19 8 11 0 27 22 24 22 27 3 15 58 44 42 3 29 3 26 34 17 0 16 41 30 8 0 5 5 53 0 12 7 43 17 9 13 14 22 57 4 16 0 62 35 24 18 20 29 8 8 14 3 8 33 17 47 36 0 11 32 12 6 8 0 5 13 23 13 12 30 39 23 52 13 0 7 16 11 5 24 0 102 5 20 13 32 22 15 20 51 0 50 48 22 24 31 16 4 20 6 12 26 1 19 13 17 17 12 15 21 36 16 4 10 30 14 7 14 14 14 5 1 29 48 0 2 20 11 0 20 23 0 0 4 10 16 12 8 23 2 46 18 1 0 23 46 6 53 52 55 15 10 0 3 13 5 46 21 4 41 13 2 0 3 27 0 19 21 23 49 12 16 71 10 0 17 7 18 21 35 13 13 22 21 10 9 13 6 0 0 25 36 14 50 10 6 10 37 0 2 0 29 0 34 9 12 48 9 10 45 5 12 3 7 19 7 0 18 8 13 73 14 35 28 24 2 40 13 31 21 23 5 19 47 36 12 29 0 19 9 1 11 22 21 6 13 7 0 41 6 74 51 0 12 4 15 36 11 113 36 3 0 13 16 7 56 9 31 9 35 13 5 6 15 7 23 0 24 16 42 11 28 40 11 20 0 12 26 0 38 54 10 6 10 8 59 20 0 11 13 16 0 16 10 23 34 16 16 19 25 18 61 40 6 0 3 26 11 37 3 4 22 0 14 8 72 0 20 5 14 5 40 15 15 19 13 0 19 27 4 31 28 15 3 17 30 20 9 11 14 35 12 12 0 5 0 20 64 32 3 0 4 22 17 9 1 9 19 19 3 2 19 74 0 11 9 17 6 11 0 14 0 67 38 13 24 6 0 28 13 2 16 41 14 30 16 21 8 16 42 95 0 22 0 0 20 35 24 9 5 49 10 13 52 4 65 0 7 31 30 6 0 8 3 33 23 0 0 7 62 15 107 23 34 6 98 13 19 30 15 30 9 2 21 24 4 32 37 10 7 0 157 9 0 53 27 26 21 3 0 18 17 8 0 5 20 57 10 25 13 8 0 25 33 4 11 9 21 0 0 30 66 36 5 15 55 9 0 8 27 19 26 4 0 16 9 22 51 0 0 27 33 9 48 3 15 7 0 18 12 27 27 30 2 7 60 0 12 20 14 2 43 7 22 20 32 12 25 4 0 38 43 22 28 8 2 22 0 34 21 32 64 28 29 0 31 0 64 61 28 9 9 5 0 26 0 13 21 20 27 5 0 15 27 0 23 0 29 27 17 11 26 0 14 44 28 20 0 28 15 35 41 59 14 2 25 31 0 49 91 17 7 23 31 42 51 7 44 24 31 10 6 0 13 14 36 2 35 36 79 6 31 21 0 12 21 37 9 0 31 0 10 22 12 9 16 23 17 25 8 0 16 34 7 12 21 5 15 20 19 1 18 0 16 6 17 14 53 18 53 24 39 49 11 10 10 9 3 37 46 36 20 8 0 9 14 40 13 38 8 30 21 9 26 3 11 19 14 11 21 22 31 7 51 13 43 17 0 29 20 12 25 38 8 12 0 14 3 19 22 12 48 8 55 34 32 43 36 0 13 24 0 0 29 37 11 24 12 12 12 35 40 8 45 76 12 16 4 0 5 10 31 21 18 8 49 94 19 4 24 26 3 11 22 5 9 58 16 17 6 16 22 19 19 15 12 20 3 10 48 43 6 10 0 0 15 3 6 18 18 17 30 12 0 28 23 31 8 18 4 0 41 0 74 10 9 13 51 8 16 11 28 14 44 65 36 25 64 12 0 32 14 47 16 58 11 49 24 22 34 21 13 25 39 8 41 73 68 22 24 5 38 56 5 3 9 45 19 19 22 21 25 31 21 10 0 24 4 31 33 52 10 0 73 31 20 24 12 0 45 7 14 51 6 7 14 0 8 27 26 0 15 7 0 46 19 45 28 32 16 11 66 48 17 51 11 55 29 31 67 21 12 65 21 37 21 43 21 25 7 42 12 11 40 194 9 9 16 37 4 8 15 38 18 34 0 43 0 15 1 58 16 38 20 14 19 8 7 16 16 6 17 26 56 9 15 64 8 8 10 0 13 12 37 55 15 7 13 21 0 12 13 30 39 14 27 12 28 24 22 12 0 23 86 62 25 16 37 15 48 19 18 8 26 37 9 48 4 15 2 10 0 64 35 13 40 0 13 39 14 8 39 4 54 0 63 8 57 28 18 29 27 14 0 27 12 32 30 33 55 21 25 10 12 11 0 7 22 20 47 20 17 36 93 0 3 33 11 6 52 43 6 15 21 19 22 35 20 52 21 55 24 32 9 17 14 3 0 3 31 40 66 21 28 14 20 1 15 0 15 11 0 0 11 7 31 62 12 12 28 11 11 12 17 10 102 25 0 8 10 9 0 38 7 0 38 38 0 8 0 13 22 124 0 25 9 27 13 18 34 25 25 49 0 4 34 45 21 15 4 45 32 25 23 58 12 11 17 21 19 21 51 145 24 21 14 11 19 18 5 129 11 2 0 9 15 25 5 2 23 3 12 11 0 25 12 0 27 22 39 11 15 26 31 22 41 0 50 17 12 72 0 24 14 12 30 7 29 8 3 8 15 53 5 43 40 34 54 41 0 14 18 16 9 20 31 24 19 39 15 15 19 17 9 0 7 17 32 0 31 19 34 4 26 45 9 32 19 17 15 13 11 2 37 19 10 7 14 32 45 10 14 17 4 31 25 144 ]
@@@ Loss per-class: [ 7.27796 0 7.56579 0 7.18456 7.60033 7.28877 0 6.81913 7.8326 7.03596 6.2514 7.20039 7.41618 6.50736 7.55619 7.62769 7.36772 7.70724 8.06341 7.68981 7.13628 6.89301 7.79091 7.82985 7.92102 6.67671 7.45017 7.79311 7.71285 7.19029 6.21836 6.30422 7.33667 7.00592 6.61712 7.43878 6.65248 7.7276 7.52173 6.5607 7.34294 8.38077 6.5946 7.57745 7.47894 7.56619 7.22646 7.47186 6.90633 6.43703 7.19943 0 7.80521 6.39707 7.47007 7.68264 7.80694 0 7.52979 6.69924 7.62416 7.17046 6.56882 7.1398 7.5532 0 7.14855 7.18095 7.8456 6.9658 7.89664 7.53169 6.91992 6.26349 7.29714 5.98571 7.24501 7.27774 6.91381 7.82244 0 6.98392 7.49391 7.33476 6.98484 7.43219 6.77219 7.90258 7.80151 8.15506 7.25981 8.20371 7.41224 7.08218 8.14972 7.73055 7.46127 6.59716 7.24568 6.37174 6.43306 7.3336 7.17449 8.84946 6.81524 6.76646 7.14023 7.57839 6.88234 7.28692 7.08703 7.50543 8.53893 6.74408 7.3032 7.24606 6.82125 6.99807 6.99602 7.47529 6.18711 7.21974 6.34704 7.47548 6.73642 7.44606 5.28646 6.68682 7.28091 7.22124 7.20222 6.8272 0 6.7865 0 0 7.19938 8.04878 6.63308 8.1868 6.99127 6.90404 6.779 7.87506 6.4905 7.25548 0 8.28622 6.98402 7.10495 7.50225 7.1421 7.04685 7.94154 6.49466 6.38643 6.63317 6.7988 7.26303 7.25205 6.38885 6.71305 5.93565 7.36079 7.21432 7.04341 5.90557 7.12985 6.97035 7.31385 7.56524 7.01765 7.3367 7.51928 6.52544 8.04033 7.75564 7.49045 8.34269 7.28108 7.12089 6.33792 8.17641 6.32828 7.4309 8.33874 7.12645 7.8765 7.24389 7.15153 7.33231 7.11251 7.35846 8.10343 7.80545 0 7.51759 6.51723 7.54474 7.14457 7.27746 7.966 8.22183 7.00857 7.12446 8.13236 6.93854 7.71293 7.63991 7.06465 7.11103 7.67474 6.88614 7.40673 7.28925 7.38008 8.22182 7.03706 4.61383 6.55438 7.45223 7.07254 7.56039 0 6.68924 6.3839 7.49411 0 7.24465 7.17887 6.87503 7.49883 6.95249 7.54795 6.61998 6.78528 8.44421 7.85318 6.83475 8.11111 7.21115 7.18291 7.54917 6.12265 0 0 7.3793 6.09748 6.87511 7.07317 6.60502 7.80384 7.24341 5.8959 7.34314 7.56528 0 7.7578 6.6393 7.2631 7.13165 7.84883 0 0 7.85844 7.19372 7.27141 7.07382 7.83408 0 7.64276 7.36339 7.49473 7.58283 6.89007 0 7.18276 7.71405 7.0797 0 0 7.48687 7.56468 7.03129 6.45242 6.48008 7.56473 7.75025 7.38636 7.3425 7.68252 7.81596 7.46975 7.03118 8.00266 6.95714 6.83438 8.15388 6.60478 7.73598 7.24936 0 7.33081 7.51365 7.56529 8.17527 7.47338 6.45833 6.73306 7.60186 6.9872 7.86589 7.15747 0 6.57617 7.30274 7.54436 7.2369 7.83045 7.87218 6.6973 7.41322 7.34191 6.9582 7.48842 7.65202 6.87871 6.39544 6.71758 0 7.77066 7.36512 7.46832 7.00493 7.65673 6.68653 7.04492 7.17312 7.10299 8.17638 6.43306 8.30292 7.50257 6.87923 7.3587 7.10172 0 7.31038 7.93993 7.50244 6.73682 0 7.12944 6.57683 7.36823 0 7.05836 6.61316 7.63946 6.95662 7.48316 6.97953 6.2841 6.91295 7.30809 6.60799 7.38226 0 7.11013 7.51717 7.64705 7.0776 7.50615 6.9935 6.60754 7.04626 7.49034 6.43714 8.09663 6.675 6.61728 7.05388 7.40223 0 6.90878 7.43483 7.42671 6.3915 7.2365 0 6.40845 8.08923 7.41644 7.67491 7.33473 7.319 7.47729 7.74687 7.95554 6.88064 0 7.57471 6.93988 5.54989 6.92084 7.18029 0 7.77522 6.77986 7.73495 7.30793 7.7626 6.88214 7.04118 7.68581 8.49326 0 8.19709 7.32452 7.20552 7.43826 7.12455 7.2203 6.82385 6.995 7.04781 7.29229 7.15417 4.29045 7.06894 6.87836 6.92275 6.87249 6.61303 7.52161 7.56976 8.07465 7.42461 7.07334 7.43523 7.31016 7.09967 7.48672 6.63258 6.54347 7.70477 6.97538 4.82984 7.19238 7.66413 0 5.95903 6.36931 7.21379 0 7.17173 8.08263 0 0 6.53591 7.19768 7.91446 6.94563 6.95648 7.27581 6.0899 6.94125 6.82405 4.79841 0 7.1441 7.73536 6.21006 7.39584 7.47947 7.68156 6.96111 7.67399 0 6.32268 7.37321 7.10046 7.32356 7.47668 6.26481 7.1806 6.95166 6.49908 0 6.41126 7.86479 0 6.97152 7.15592 7.46207 8.16441 7.2403 7.4452 7.25547 7.06917 0 7.67586 6.93551 7.34696 7.66624 7.0931 7.25547 7.32273 7.40812 7.45255 6.63419 7.30463 6.878 7.30626 0 0 7.14023 7.32248 7.34872 7.49215 7.10596 6.84198 8.07713 7.38771 0 6.32502 0 7.97206 0 7.6202 7.59648 7.56306 7.57838 6.96366 6.97733 7.88061 6.36177 7.42878 6.23237 6.98876 7.38987 6.7384 0 7.57826 6.91396 6.49408 7.67012 7.94056 7.44647 7.66038 7.94781 5.97961 6.78019 6.56795 7.15841 7.6307 7.03868 7.55865 7.76203 7.18895 7.72364 6.5386 6.90309 0 8.08147 7.57318 4.7035 6.65017 7.66129 6.88246 6.59121 7.3749 7.52432 0 7.48516 7.34608 7.19027 7.17595 0 7.19233 7.40479 6.88559 7.26095 7.1436 8.02903 7.75263 6.68613 0 7.57711 6.93752 7.48742 8.18243 6.94153 6.6683 6.47585 7.98487 6.88084 6.64128 6.72803 6.71649 6.99216 7.50199 0 6.67784 6.85503 7.29837 7.69044 7.44342 7.34849 7.46057 8.05489 0 7.38807 6.93033 0 8.29694 8.07967 6.97124 7.60975 7.19065 7.52887 8.09048 7.66091 0 6.87244 7.29575 7.19773 0 7.84126 7.18304 7.1561 7.4603 7.45396 7.3213 7.02511 8.0112 6.85581 7.36032 7.24671 7.21085 0 6.18554 8.027 6.82381 7.68655 7.03926 6.90903 7.09562 0 6.94326 7.92938 8.31317 0 7.19429 6.37168 7.84541 6.29851 7.03336 7.27628 7.86221 7.31233 7.25049 0 7.65568 7.444 7.17323 6.95937 8.3955 6.73651 6.04121 7.7057 7.74064 7.9456 6.83123 6.69226 7.69514 6.25437 7.79503 7.22701 0 6.808 0 7.63374 7.27729 6.57927 6.10052 0 6.95906 7.83168 7.57345 7.17361 5.53995 7.01278 7.44517 7.43342 6.56929 5.62177 6.81036 7.03885 0 7.01735 5.93541 6.88857 6.87408 7.97088 0 8.72488 0 7.16632 7.13689 6.48821 7.44865 7.19913 0 7.33166 7.47241 5.89048 7.47221 7.4936 6.65558 8.06896 6.19237 7.51247 7.10765 7.28512 7.55177 7.62957 0 7.74985 0 0 7.24048 7.88445 7.06152 7.7273 7.05254 7.77474 7.79268 6.8828 6.96301 6.56252 7.33643 0 8.27161 7.60425 7.57096 6.28238 0 7.35216 6.15557 7.85823 7.35504 0 0 7.95412 7.67132 7.46778 7.32206 6.40212 7.39526 7.53237 7.80492 6.68056 7.57624 7.09769 7.16462 6.76121 7.76309 6.38394 6.57413 6.68571 6.91513 7.10496 7.03531 6.71688 7.0076 0 7.88862 7.36043 0 7.52235 8.38125 7.91685 7.71862 6.69501 0 6.55394 6.79563 6.28991 0 7.98606 7.84071 7.66765 6.41537 6.7703 6.7343 6.13706 0 7.49255 7.26389 6.73793 6.68789 6.68946 7.93891 0 0 6.72199 7.96981 7.79104 6.71582 7.42071 7.64592 7.14521 0 6.5442 7.13056 7.47187 7.05035 6.25619 0 7.74018 7.78472 7.97334 7.25784 0 0 7.85285 7.66861 7.32253 6.56033 5.89167 7.24314 6.56976 0 8.00853 6.92908 6.77308 6.63501 7.00352 5.39116 6.92502 7.32126 0 7.59477 7.41338 7.6552 5.95565 7.3228 7.3028 6.70467 6.39643 8.12564 7.26966 7.33069 6.46834 0 7.01907 7.71657 7.20451 6.59918 7.07989 6.5697 7.56954 0 8.05278 8.79702 7.28929 8.26973 7.73359 7.38757 0 7.89325 0 6.8403 6.77415 6.98476 7.44507 7.40523 6.38156 0 6.72607 0 6.3328 7.99041 7.76558 7.75659 7.24801 0 6.81379 6.78213 0 7.33129 0 7.2915 7.16773 7.51685 7.03593 7.84855 0 7.78464 7.27698 7.63274 8.49265 0 7.81641 6.62981 7.6342 7.68332 7.34643 6.71471 5.82096 7.55383 7.83242 0 6.45314 7.58778 8.06748 7.12662 7.56099 7.74926 8.10572 6.89946 6.75742 7.08333 7.08446 8.04002 6.92016 6.13061 0 7.18235 7.14526 7.47597 6.06911 7.64923 7.5512 6.82239 7.61096 7.92148 7.82306 0 6.60795 7.65028 6.99431 6.73204 0 7.57627 0 7.00707 7.33808 6.47645 6.69815 7.75883 7.29664 7.19809 7.3649 7.08536 0 7.10786 7.30145 7.71279 7.51092 7.23048 7.06393 6.49351 7.89196 6.97829 5.40923 8.1187 0 6.36347 6.84502 7.223 7.11856 6.83278 7.41403 7.56762 7.17585 7.31011 7.41778 6.9794 7.40494 7.14538 6.38666 6.23914 7.91389 7.05174 7.37224 7.59901 6.89828 0 7.60842 6.7293 6.97739 7.75465 7.46471 6.66045 6.46978 7.8616 6.9152 7.34526 5.98999 7.17959 7.16941 7.64084 7.18968 6.91405 6.54151 7.72703 7.26246 7.30234 7.90889 7.08978 6.44006 0 7.40453 7.37496 7.22502 7.63755 7.14188 7.61546 7.31931 0 6.97228 6.95819 8.55416 7.56252 7.70689 7.59504 7.32237 7.59876 7.52473 7.60009 7.5152 8.19847 0 7.72653 7.18677 0 0 7.29074 7.23465 6.79781 6.99053 7.84125 7.03469 6.78538 7.55468 7.39832 7.26891 7.12807 7.85826 7.96482 8.00967 7.35815 0 6.88873 7.48179 7.05689 7.40952 7.58103 6.66509 7.39898 7.97872 6.74339 6.83459 7.79481 7.24278 6.84334 7.75968 7.62871 6.39782 6.91963 7.01375 7.21794 6.76265 6.77659 6.55071 7.03243 6.78236 8.22186 6.47977 7.33441 6.70444 6.788 6.59289 7.72147 7.24765 7.00431 7.22782 0 0 7.77061 6.47084 6.4623 7.76661 7.95565 7.659 7.93506 7.80891 0 7.65852 6.59684 6.85151 6.96656 8.19894 6.81952 0 7.26172 0 7.6311 7.2829 6.86351 7.29221 7.59845 7.14644 7.00997 7.38608 8.01889 7.09385 8.01118 6.83402 7.08455 7.32194 7.23923 7.43007 0 7.28255 8.37489 6.86239 7.41448 7.34899 7.31495 6.89397 7.71785 6.70153 7.23289 6.785 7.58175 6.73275 7.62202 6.93392 6.94426 7.42017 7.55664 6.93129 7.51093 6.83784 7.7444 7.56289 6.8093 5.73303 7.17081 7.60538 7.66032 7.3585 7.09748 6.79318 6.70137 6.91092 7.62682 7.30695 0 7.42607 6.79068 6.94511 8.00699 7.60364 6.54132 0 7.18066 7.58423 7.55194 7.77911 6.86195 0 7.42008 6.98744 6.93001 7.08791 6.52042 6.47386 7.26768 0 6.84879 7.68499 7.22349 0 7.49483 7.07896 0 7.88898 7.00063 6.96375 7.84653 8.33528 7.2067 7.30237 7.61259 6.81659 7.10385 7.61568 7.585 7.3316 7.94824 7.28423 7.33207 7.20438 7.02758 7.28434 7.74623 7.47194 6.89446 7.20726 6.71251 6.82652 6.89405 7.04399 7.19703 7.35281 7.98069 7.38608 7.12737 6.33503 8.56378 7.31952 6.17563 6.12948 7.34623 7.65905 7.51763 7.01194 0 7.07464 0 6.50682 5.37463 7.09201 6.62476 7.44076 7.79723 7.34511 6.3823 7.29939 7.36161 6.636 6.62968 7.03245 7.89629 7.49558 7.20056 6.84902 7.92498 7.05307 5.97121 7.46658 6.92041 0 7.49114 6.36123 7.16386 7.43904 7.5362 7.70949 6.94238 7.47168 0 6.90595 7.12696 7.03208 7.49434 7.21294 8.01364 7.77704 7.07452 7.10141 7.26236 6.75428 0 6.86579 7.47053 7.02133 6.43815 6.89438 7.67037 6.45262 6.47488 6.8729 7.39465 7.31398 7.40607 7.13318 7.47198 8.14508 7.82171 7.76804 6.82532 6.56279 0 7.78892 7.95967 6.99394 7.95388 0 8.21138 6.85399 7.39027 6.59146 7.64622 6.1055 7.90673 0 7.97764 7.48941 7.70693 7.79622 7.13809 6.85975 7.47475 6.53416 0 7.94559 7.17558 7.32756 7.95428 7.25997 7.65087 8.16231 8.51823 7.40947 7.85257 7.27105 0 7.08536 8.20385 6.56999 7.40473 6.7399 7.11635 7.65581 7.70396 0 6.07624 7.25712 7.56055 6.97591 7.65951 7.69403 7.07381 7.68667 8.04344 7.14966 7.83913 7.41472 6.67873 6.99536 6.82367 8.01695 7.31362 7.36844 7.28155 7.35424 7.63487 6.8187 0 6.41745 7.78205 7.45648 7.06595 7.0791 7.05278 7.64529 7.68643 4.55127 6.94132 0 7.13699 6.65463 0 0 7.31198 7.80845 6.97128 7.12417 7.12148 7.74311 7.11028 7.32476 7.17313 7.33461 6.32195 7.14924 7.124 7.54112 0 6.6449 6.57222 7.37754 0 7.54241 7.33824 0 7.22432 7.14151 0 6.78094 0 7.46316 7.37307 7.05647 0 7.55046 7.3104 8.24591 7.45392 7.12692 7.02071 7.26777 7.01688 7.77282 0 5.97942 7.47823 8.42967 7.87455 7.5806 6.72391 7.86096 7.48738 7.57666 7.62577 7.74088 6.50936 6.91611 7.90689 6.94071 7.59666 7.62959 7.44168 6.75887 7.2103 7.81771 7.14861 6.76101 7.95814 7.28967 6.32363 7.98579 7.31412 6.17421 0 6.63246 7.43179 7.21677 6.20918 5.89304 6.96094 6.02369 6.95505 7.6217 0 6.23982 7.92201 0 6.90378 7.14972 7.04592 7.62248 7.50908 7.41487 7.44873 7.56693 7.44916 0 7.48459 8.19177 7.61508 7.16937 0 6.67218 6.88544 7.11101 7.50264 6.90344 8.08518 6.64845 5.99518 7.48881 7.19549 6.81183 7.51637 7.11441 7.34527 7.94943 8.21113 6.95165 0 6.96792 6.54178 8.10997 7.58222 7.1796 7.35027 7.81797 8.10385 7.61566 7.77082 6.27082 7.39293 7.89465 6.94003 0 7.16406 7.5536 6.65744 0 7.55017 7.03129 7.28873 6.92517 8.15401 7.56797 7.14398 7.56325 7.40252 7.82613 7.87934 7.21344 6.93581 5.47138 7.39369 6.96646 6.56527 7.51758 7.37394 7.1027 8.13718 6.91527 7.90265 6.62158 6.51818 7.82598 8.09005 6.78381 ]
@@@ Frame-accuracy per-class: [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2.40964 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 18.1818 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 52.1739 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6.06061 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.27869 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5.40541 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ]

LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:406) AvgLoss: 7.49463 (Xent), [AvgXent: 7.49463, AvgTargetEnt: 0]
progress: []
FRAME_ACCURACY >> 0.0387855% <<

