nsclab-gpu
nnet-train-frmshuff --cross-validate=false --randomize=true --verbose=0 --minibatch-size=256 --randomizer-size=32768 --randomizer-seed=777 --learn-rate=0.008 --momentum=0 --l1-penalty=0 --l2-penalty=0 --feature-transform=exp/dnn4_pretrain-dbn_dnn/final.feature_transform 'ark:copy-feats scp:exp/dnn4_pretrain-dbn_dnn/train.scp ark:- |' 'ark:ali-to-pdf exp/tri3_ali/final.mdl "ark:gunzip -c exp/tri3_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- |' exp/dnn4_pretrain-dbn_dnn/nnet_dbn_dnn.init exp/dnn4_pretrain-dbn_dnn/nnet/nnet_dbn_dnn_iter01 
WARNING (nnet-train-frmshuff[5.5.294-06484]:SelectGpuId():cu-device.cc:221) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:349) Selecting from 2 GPUs
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:364) cudaSetDevice(0): TITAN Xp	free:12023M, used:173M, total:12196M, free/total:0.985815
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:364) cudaSetDevice(1): TITAN Xp	free:11888M, used:304M, total:12192M, free/total:0.975032
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:411) Trying to select device: 0 (automatically), mem_ratio: 0.985815
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:430) Success selecting device 0 free mem ratio: 0.985815
LOG (nnet-train-frmshuff[5.5.294-06484]:FinalizeActiveGpu():cu-device.cc:284) The active GPU is [0]: TITAN Xp	free:11957M, used:239M, total:12196M, free/total:0.980403 version 6.1
copy-feats scp:exp/dnn4_pretrain-dbn_dnn/train.scp ark:- 
LOG (nnet-train-frmshuff[5.5.294-06484]:Init():nnet-randomizer.cc:32) Seeding by srand with : 777
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:163) TRAINING STARTED
ali-to-post ark:- ark:- 
ali-to-pdf exp/tri3_ali/final.mdl 'ark:gunzip -c exp/tri3_ali/ali.*.gz |' ark:- 
LOG (ali-to-pdf[5.5.294-06484]:main():ali-to-pdf.cc:68) Converted 1232 alignments to pdf sequences.
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:354) ### After 0 frames,
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:355) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -7.86424, max 7.06273, mean -0.000964877, stddev 0.993005, skewness 0.0133901, kurtosis 2.03207 ) 
[1] output of <AffineTransform> ( min -22.2666, max 11.8526, mean -3.15209, stddev 2.30752, skewness 0.114641, kurtosis 2.03574 ) 
[2] output of <Sigmoid> ( min 2.13669e-10, max 0.999993, mean 0.131238, stddev 0.217707, skewness 2.35544, kurtosis 4.93727 ) 
[3] output of <AffineTransform> ( min -22.9254, max 14.5658, mean -3.60647, stddev 2.34214, skewness -0.0620643, kurtosis 3.12487 ) 
[4] output of <Sigmoid> ( min 1.10565e-10, max 1, mean 0.0961985, stddev 0.182369, skewness 3.12627, kurtosis 9.95438 ) 
[5] output of <AffineTransform> ( min -16.0426, max 10.2357, mean -3.43275, stddev 1.95899, skewness 0.747751, kurtosis 2.77675 ) 
[6] output of <Sigmoid> ( min 1.07839e-07, max 0.999964, mean 0.0938556, stddev 0.182068, skewness 3.20161, kurtosis 10.3589 ) 
[7] output of <AffineTransform> ( min -21.1374, max 13.151, mean -3.62807, stddev 1.96999, skewness 0.666461, kurtosis 4.5095 ) 
[8] output of <Sigmoid> ( min 6.60934e-10, max 0.999998, mean 0.0805652, stddev 0.171292, skewness 3.62056, kurtosis 13.4284 ) 
[9] output of <AffineTransform> ( min -15.4644, max 10.7868, mean -3.76578, stddev 1.80573, skewness 1.63163, kurtosis 5.81906 ) 
[10] output of <Sigmoid> ( min 1.92263e-07, max 0.999979, mean 0.0723096, stddev 0.174722, skewness 3.79416, kurtosis 14.3357 ) 
[11] output of <AffineTransform> ( min -19.2018, max 12.4411, mean -3.86168, stddev 1.83552, skewness 1.3399, kurtosis 7.35502 ) 
[12] output of <Sigmoid> ( min 4.5788e-09, max 0.999996, mean 0.0654952, stddev 0.165854, skewness 4.12519, kurtosis 17.1253 ) 
[13] output of <AffineTransform> ( min -2.96576, max 2.76381, mean -0.00190093, stddev 0.562548, skewness 0.0131743, kurtosis 0.35283 ) 
[14] output of <Softmax> ( min 2.51624e-05, max 0.00823855, mean 0.000647669, stddev 0.000400345, skewness 2.44327, kurtosis 13.1079 ) 
### END FORWARD

LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:357) 
### BACKWARD PROPAGATION BUFFER CONTENT :
[0] diff of <Input>  ( min -0.119184, max 0.0738026, mean 6.55225e-05, stddev 0.00662102, skewness -0.13205, kurtosis 10.7945 ) 
[1] diff-output of <AffineTransform> ( min -0.0476955, max 0.0411406, mean 5.5753e-05, stddev 0.00181308, skewness 0.464724, kurtosis 39.0878 ) 
[2] diff-output of <Sigmoid> ( min -0.192735, max 0.239972, mean -4.08152e-05, stddev 0.0145443, skewness 0.0528023, kurtosis 6.24419 ) 
[3] diff-output of <AffineTransform> ( min -0.0430387, max 0.0405, mean 4.811e-05, stddev 0.0017718, skewness 0.538, kurtosis 42.7204 ) 
[4] diff-output of <Sigmoid> ( min -0.176259, max 0.190825, mean -0.000282185, stddev 0.0163101, skewness 0.00863335, kurtosis 4.04561 ) 
[5] diff-output of <AffineTransform> ( min -0.037024, max 0.034931, mean 5.11733e-05, stddev 0.00179975, skewness 0.733803, kurtosis 38.2917 ) 
[6] diff-output of <Sigmoid> ( min -0.167448, max 0.162833, mean -0.000192387, stddev 0.0158438, skewness 0.0727537, kurtosis 4.05069 ) 
[7] diff-output of <AffineTransform> ( min -0.0348903, max 0.0381182, mean 4.77418e-05, stddev 0.00189983, skewness 0.565118, kurtosis 39.0364 ) 
[8] diff-output of <Sigmoid> ( min -0.170133, max 0.153318, mean -0.000230606, stddev 0.0177971, skewness 0.0419297, kurtosis 3.25208 ) 
[9] diff-output of <AffineTransform> ( min -0.0371823, max 0.0369317, mean 4.19038e-05, stddev 0.00221104, skewness 0.679066, kurtosis 34.4464 ) 
[10] diff-output of <Sigmoid> ( min -0.163749, max 0.208957, mean -0.000385101, stddev 0.0239812, skewness 0.0434628, kurtosis 3.06224 ) 
[11] diff-output of <AffineTransform> ( min -0.07896, max 0.0898112, mean 6.66286e-05, stddev 0.00593413, skewness 0.216495, kurtosis 25.8828 ) 
[12] diff-output of <Sigmoid> ( min -0.421184, max 0.437919, mean 0.000518732, stddev 0.0979758, skewness -0.00232153, kurtosis 0.0245655 ) 
[13] diff-output of <AffineTransform> ( min -0.999918, max 0.00823855, mean -9.40974e-10, stddev 0.0254391, skewness -39.2629, kurtosis 1539.38 ) 
[14] diff-output of <Softmax> ( min -0.999918, max 0.00823855, mean -9.40974e-10, stddev 0.0254391, skewness -39.2629, kurtosis 1539.38 ) 
### END BACKWARD


LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:358) 
### GRADIENT STATS :
Component 1 : <AffineTransform>, 
  linearity_grad ( min -0.267369, max 0.321047, mean -0.000255382, stddev 0.0307695, skewness 0.0208106, kurtosis 1.44952 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.105994, max 0.136157, mean 0.0142728, stddev 0.0341091, skewness 0.0356064, kurtosis 0.466624 ) , lr-coef 1
Component 2 : <Sigmoid>, 
Component 3 : <AffineTransform>, 
  linearity_grad ( min -0.0821857, max 0.0930879, mean 0.00159824, stddev 0.0080792, skewness 0.316754, kurtosis 4.34733 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.115935, max 0.129358, mean 0.0123162, stddev 0.0336082, skewness -0.147384, kurtosis 0.641881 ) , lr-coef 1
Component 4 : <Sigmoid>, 
Component 5 : <AffineTransform>, 
  linearity_grad ( min -0.07144, max 0.0999283, mean 0.00128302, stddev 0.00667821, skewness 0.610727, kurtosis 6.24273 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.10995, max 0.157088, mean 0.0131003, stddev 0.0348588, skewness 0.285169, kurtosis 0.831469 ) , lr-coef 1
Component 6 : <Sigmoid>, 
Component 7 : <AffineTransform>, 
  linearity_grad ( min -0.115478, max 0.141988, mean 0.00116539, stddev 0.00716664, skewness 0.581499, kurtosis 9.73519 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.127192, max 0.170708, mean 0.0122219, stddev 0.0383913, skewness 0.0508081, kurtosis 1.1915 ) , lr-coef 1
Component 8 : <Sigmoid>, 
Component 9 : <AffineTransform>, 
  linearity_grad ( min -0.114991, max 0.142133, mean 0.000898598, stddev 0.00764499, skewness 0.966535, kurtosis 11.576 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.137097, max 0.22034, mean 0.0107273, stddev 0.0446063, skewness 0.615955, kurtosis 1.58137 ) , lr-coef 1
Component 10 : <Sigmoid>, 
Component 11 : <AffineTransform>, 
  linearity_grad ( min -0.685544, max 0.717771, mean 0.00130882, stddev 0.0212565, skewness 0.131356, kurtosis 37.9583 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.980133, max 0.863613, mean 0.0170569, stddev 0.130405, skewness -0.192982, kurtosis 7.34015 ) , lr-coef 1
Component 12 : <Sigmoid>, 
Component 13 : <AffineTransform>, 
  linearity_grad ( min -10.8429, max 0.166582, mean -4.63695e-10, stddev 0.086721, skewness -26.1278, kurtosis 1770.13 ) , lr-coef 1, max-norm 0
  bias_grad ( min -10.8529, max 0.406921, mean 3.70599e-09, stddev 0.566562, skewness -9.32269, kurtosis 148.236 ) , lr-coef 1
Component 14 : <Softmax>, 
### END GRADIENT

LOG (ali-to-post[5.5.294-06484]:main():ali-to-post.cc:73) Converted 1232 alignments.
LOG (copy-feats[5.5.294-06484]:main():copy-feats.cc:143) Copied 1112 feature matrices.
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:384) ### After 331520 frames,
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:385) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -7.8171, max 6.44093, mean -0.00429144, stddev 1.00295, skewness -0.0631885, kurtosis 1.97814 ) 
[1] output of <AffineTransform> ( min -26.4662, max 16.2129, mean -3.1673, stddev 2.92065, skewness 0.128705, kurtosis 1.89486 ) 
[2] output of <Sigmoid> ( min 3.20536e-12, max 1, mean 0.16084, stddev 0.258058, skewness 1.9625, kurtosis 2.79149 ) 
[3] output of <AffineTransform> ( min -27.8164, max 17.7332, mean -3.78172, stddev 2.58666, skewness 0.122116, kurtosis 3.62651 ) 
[4] output of <Sigmoid> ( min 8.30802e-13, max 1, mean 0.0990472, stddev 0.19336, skewness 2.9798, kurtosis 8.78727 ) 
[5] output of <AffineTransform> ( min -14.8217, max 14.3866, mean -3.23607, stddev 2.05737, skewness 0.806794, kurtosis 3.2854 ) 
[6] output of <Sigmoid> ( min 3.65611e-07, max 0.999999, mean 0.10805, stddev 0.194181, skewness 2.90355, kurtosis 8.34348 ) 
[7] output of <AffineTransform> ( min -23.8368, max 15.7805, mean -3.21851, stddev 2.27531, skewness 0.673766, kurtosis 3.80012 ) 
[8] output of <Sigmoid> ( min 4.44426e-11, max 1, mean 0.119496, stddev 0.214564, skewness 2.63132, kurtosis 6.38895 ) 
[9] output of <AffineTransform> ( min -15.8642, max 15.738, mean -3.14252, stddev 2.5647, skewness 1.43506, kurtosis 3.50207 ) 
[10] output of <Sigmoid> ( min 1.28908e-07, max 1, mean 0.141016, stddev 0.258934, skewness 2.21941, kurtosis 3.75771 ) 
[11] output of <AffineTransform> ( min -27.9339, max 19.1801, mean -3.45884, stddev 3.07856, skewness 1.13169, kurtosis 3.9322 ) 
[12] output of <Sigmoid> ( min 7.38673e-13, max 1, mean 0.13924, stddev 0.274543, skewness 2.21105, kurtosis 3.51236 ) 
[13] output of <AffineTransform> ( min -10.042, max 17.025, mean -0.00211329, stddev 2.44385, skewness 0.728916, kurtosis 1.58502 ) 
[14] output of <Softmax> ( min 9.08606e-12, max 0.979994, mean 0.000647541, stddev 0.0154447, skewness 43.2042, kurtosis 2152.17 ) 
### END FORWARD

LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:387) 
### BACKWARD PROPAGATION BUFFER CONTENT :
[0] diff of <Input>  ( min -1.01221, max 0.696469, mean -0.000113453, stddev 0.0346671, skewness -0.939161, kurtosis 43.5997 ) 
[1] diff-output of <AffineTransform> ( min -0.287877, max 0.266398, mean 2.44737e-05, stddev 0.00744213, skewness -0.291083, kurtosis 91.4028 ) 
[2] diff-output of <Sigmoid> ( min -1.78243, max 1.34887, mean 0.000262687, stddev 0.0622588, skewness -0.0693307, kurtosis 24.6177 ) 
[3] diff-output of <AffineTransform> ( min -0.414015, max 0.267048, mean -3.0036e-07, stddev 0.00744473, skewness -0.930896, kurtosis 130.829 ) 
[4] diff-output of <Sigmoid> ( min -2.21548, max 1.33752, mean -0.000215043, stddev 0.0727375, skewness -0.225219, kurtosis 17.3597 ) 
[5] diff-output of <AffineTransform> ( min -0.258532, max 0.259402, mean 1.4486e-05, stddev 0.0073467, skewness -0.387922, kurtosis 82.5269 ) 
[6] diff-output of <Sigmoid> ( min -1.35018, max 1.04188, mean 0.00013368, stddev 0.0630645, skewness -0.110905, kurtosis 10.7517 ) 
[7] diff-output of <AffineTransform> ( min -0.193052, max 0.179269, mean -7.29484e-06, stddev 0.00656476, skewness -0.218531, kurtosis 49.3511 ) 
[8] diff-output of <Sigmoid> ( min -1.13395, max 0.811702, mean -0.000128544, stddev 0.0549291, skewness -0.126347, kurtosis 9.7662 ) 
[9] diff-output of <AffineTransform> ( min -0.138054, max 0.137694, mean 1.70709e-06, stddev 0.00572632, skewness -0.308593, kurtosis 39.8546 ) 
[10] diff-output of <Sigmoid> ( min -0.578784, max 0.591794, mean -4.63662e-05, stddev 0.0460266, skewness -0.190749, kurtosis 9.25936 ) 
[11] diff-output of <AffineTransform> ( min -0.149586, max 0.150696, mean 1.12201e-05, stddev 0.00753518, skewness -0.128881, kurtosis 30.6017 ) 
[12] diff-output of <Sigmoid> ( min -0.954256, max 0.818472, mean 0.000415107, stddev 0.0834992, skewness -0.0753138, kurtosis 2.74665 ) 
[13] diff-output of <AffineTransform> ( min -0.999977, max 0.89756, mean -9.18535e-09, stddev 0.0196949, skewness -29.0983, kurtosis 1657.85 ) 
[14] diff-output of <Softmax> ( min -0.999977, max 0.89756, mean -9.18535e-09, stddev 0.0196949, skewness -29.0983, kurtosis 1657.85 ) 
### END BACKWARD


LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:388) 
### GRADIENT STATS :
Component 1 : <AffineTransform>, 
  linearity_grad ( min -1.15218, max 1.26734, mean 0.000187918, stddev 0.119505, skewness -0.0171946, kurtosis 3.3562 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.514649, max 0.602031, mean 0.00626526, stddev 0.119042, skewness -0.0228128, kurtosis 2.10551 ) , lr-coef 1
Component 2 : <Sigmoid>, 
Component 3 : <AffineTransform>, 
  linearity_grad ( min -0.513884, max 0.879385, mean -2.32249e-05, stddev 0.036812, skewness 0.290276, kurtosis 11.3228 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.493301, max 0.776528, mean -7.68884e-05, stddev 0.12035, skewness 0.385741, kurtosis 2.96564 ) , lr-coef 1
Component 4 : <Sigmoid>, 
Component 5 : <AffineTransform>, 
  linearity_grad ( min -0.464044, max 0.850139, mean 0.000292582, stddev 0.0257394, skewness 0.490298, kurtosis 17.1879 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.503323, max 0.827883, mean 0.00370845, stddev 0.116586, skewness 0.459254, kurtosis 3.98946 ) , lr-coef 1
Component 6 : <Sigmoid>, 
Component 7 : <AffineTransform>, 
  linearity_grad ( min -0.348428, max 0.604102, mean -0.000267553, stddev 0.0234076, skewness 0.242602, kurtosis 13.2776 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.452469, max 0.572845, mean -0.00186745, stddev 0.101539, skewness 0.258357, kurtosis 2.31863 ) , lr-coef 1
Component 8 : <Sigmoid>, 
Component 9 : <AffineTransform>, 
  linearity_grad ( min -0.285081, max 0.488342, mean -9.5409e-05, stddev 0.0226041, skewness 0.172799, kurtosis 9.86005 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.279464, max 0.522535, mean 0.000437013, stddev 0.0884589, skewness 0.228772, kurtosis 2.66471 ) , lr-coef 1
Component 10 : <Sigmoid>, 
Component 11 : <AffineTransform>, 
  linearity_grad ( min -0.361447, max 0.424357, mean 0.000329954, stddev 0.0367509, skewness 0.166877, kurtosis 8.74085 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.523712, max 0.515048, mean 0.00287237, stddev 0.121577, skewness 0.250266, kurtosis 2.38948 ) , lr-coef 1
Component 12 : <Sigmoid>, 
Component 13 : <AffineTransform>, 
  linearity_grad ( min -2.96853, max 2.0698, mean -3.3244e-08, stddev 0.103203, skewness -5.60314, kurtosis 100.161 ) , lr-coef 1, max-norm 0
  bias_grad ( min -2.95028, max 2.08386, mean 2.47066e-09, stddev 0.332736, skewness -2.14195, kurtosis 15.3384 ) , lr-coef 1
Component 14 : <Softmax>, 
### END GRADIENT

LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:395) Done 1112 files, 0 with no tgt_mats, 0 with other errors. [TRAINING, RANDOMIZED, 0.0762589 min, processing 72454.9 frames per sec; i/o time 5.00946%]
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:405) PER-CLASS PERFORMANCE:
@@@ Frames per-class: [ 12529 82 168 15 39 90 125 14 160 949 326 163 664 119 473 184 201 211 193 569 325 158 270 55 263 76 470 288 269 203 80 65 33 45 448 78 546 224 339 87 41 297 220 92 457 158 159 130 5856 862 179 17467 15 263 124 345 325 302 31 186 53 1047 72 114 389 21 23 419 1069 180 214 154 271 105 32 194 124 783 354 176 83 31 186 287 234 208 522 218 96 210 382 201 298 644 47 277 116 333 115 258 97 191 173 96 181 409 120 133 163 726 178 139 96 129 226 307 166 16 24 18 1329 98 244 90 176 53 288 200 54 100 123 272 117 15 41 21 21 91 60 126 197 51 96 82 224 194 191 50 223 160 89 169 118 146 105 70 169 132 174 243 95 138 65 27 97 248 159 113 303 171 151 133 105 77 297 99 319 34 122 282 178 379 148 336 54 208 100 220 133 84 0 247 47 103 145 242 114 216 104 255 48 166 433 153 128 171 581 195 185 145 265 92 119 238 74 15 103 342 96 132 301 367 241 102 18 97 54 169 50 92 213 98 90 124 291 101 304 146 431 115 261 157 246 195 51 13 17 244 135 169 227 123 218 412 126 192 391 18 262 155 102 86 146 8 84 265 167 112 230 95 30 117 236 201 158 427 18 339 60 93 79 7 556 136 131 62 108 182 86 236 115 179 279 93 212 288 133 159 416 105 190 73 14 212 282 219 234 242 140 32 460 138 190 86 26 79 90 133 144 126 140 151 182 229 202 329 98 70 41 56 66 175 96 203 78 340 96 213 364 337 325 81 226 93 176 268 122 33 145 322 416 116 17 201 122 526 15 186 115 516 185 57 164 61 172 568 94 119 42 316 216 137 148 261 176 117 134 69 63 77 237 114 511 347 33 67 214 40 90 96 17 96 188 154 191 78 226 224 143 56 66 20 116 235 10 59 112 20 925 112 101 64 330 173 189 119 264 24 289 498 201 200 288 170 100 111 51 116 255 69 154 98 113 61 112 148 150 296 169 111 187 529 47 97 193 170 112 19 19 198 265 44 73 167 148 25 129 100 34 156 24 99 180 54 118 142 115 354 173 25 22 70 541 199 244 461 412 310 188 49 50 53 42 349 163 110 334 142 37 16 58 279 22 113 151 223 463 86 147 708 197 88 176 144 118 78 283 134 124 217 134 136 164 82 86 87 59 265 197 244 476 106 97 76 428 30 80 60 182 25 346 68 102 346 99 294 462 91 115 92 282 167 22 13 218 76 128 496 74 214 223 237 61 292 81 254 250 237 71 176 480 458 121 230 29 67 142 49 178 161 191 106 62 106 85 203 120 293 684 40 382 61 138 263 77 746 294 86 85 192 116 95 405 107 330 128 134 151 98 16 275 229 196 131 254 150 387 109 172 188 121 314 56 46 176 49 272 684 10 53 150 105 527 176 13 56 136 138 15 203 67 121 317 127 199 225 89 151 283 188 68 23 181 366 147 424 193 84 127 27 157 150 384 28 128 25 70 176 306 125 178 127 77 84 206 199 46 321 126 161 65 130 315 216 31 106 136 438 132 119 33 47 84 167 570 193 84 23 80 162 274 207 95 133 408 298 83 33 214 590 5 107 61 368 62 130 8 166 7 331 177 265 261 63 20 208 156 71 281 350 223 399 79 147 167 122 501 519 24 221 65 212 266 290 284 114 166 555 0 197 293 81 799 7 124 220 234 54 12 165 33 91 209 98 17 81 480 178 1029 328 182 84 777 104 282 346 109 171 187 35 132 213 88 226 373 76 50 15 1407 121 29 82 258 203 262 69 149 103 244 58 68 77 122 372 192 157 96 225 24 194 317 140 92 200 122 18 14 131 429 169 92 176 412 72 47 39 185 6 2 100 21 100 153 121 450 92 21 119 261 149 152 112 131 91 28 78 119 300 289 229 134 97 534 32 172 152 261 70 244 148 87 89 345 141 217 58 4 469 407 196 138 141 72 190 55 228 321 238 558 473 261 26 370 14 772 305 191 0 87 240 80 132 38 97 159 476 170 46 29 147 294 121 307 12 196 119 224 139 174 28 112 256 242 131 46 117 110 266 491 265 0 123 173 382 15 363 1014 217 48 39 221 167 289 113 288 267 18 127 145 12 122 108 276 36 147 206 496 88 137 277 16 124 87 332 137 130 140 48 103 153 92 165 219 108 176 304 117 25 176 240 50 79 258 173 192 204 136 37 159 88 122 73 129 201 727 213 578 202 494 508 120 108 98 101 62 177 243 378 312 54 37 94 141 440 91 318 143 302 161 165 118 54 243 130 127 81 137 52 190 117 433 158 270 167 51 261 82 116 130 171 109 146 87 135 147 186 228 139 499 43 521 263 266 332 317 25 207 207 79 20 226 288 259 312 198 426 119 230 582 55 205 344 192 80 133 126 73 83 241 211 147 108 566 589 163 160 199 175 91 80 271 125 142 301 218 115 155 204 205 162 293 140 63 294 129 120 270 522 103 106 28 11 143 131 119 209 153 133 177 102 13 40 170 103 136 233 65 89 174 12 173 124 65 70 493 135 108 232 152 104 134 856 498 405 419 152 23 265 332 423 180 491 63 418 219 46 263 257 105 341 170 75 318 453 595 40 175 71 387 283 82 58 59 500 149 267 232 219 277 19 116 0 11 237 64 116 236 549 160 54 547 261 243 277 165 12 320 111 196 275 87 85 197 83 102 156 321 22 145 132 38 352 178 409 280 217 274 140 353 528 110 278 303 459 391 184 266 329 55 698 165 186 118 381 146 206 29 257 153 84 198 1839 150 182 52 338 190 18 149 310 251 217 30 198 32 118 98 310 397 155 203 200 194 117 185 30 54 142 216 418 307 24 310 404 261 80 86 75 110 107 267 270 210 75 145 72 17 79 146 426 254 173 297 0 266 293 146 127 15 350 792 493 238 123 210 143 211 231 401 70 142 356 154 467 53 190 145 53 249 853 435 197 236 79 218 507 193 37 354 137 162 30 239 118 564 275 121 253 272 250 20 277 14 167 136 256 458 143 149 177 162 35 13 64 201 104 168 192 199 350 685 54 302 286 244 24 337 239 332 277 180 79 131 169 105 488 264 274 214 278 59 136 46 42 16 94 267 496 588 255 297 248 339 100 228 27 119 137 18 130 118 232 213 701 153 185 276 129 123 110 271 84 569 175 70 130 71 69 37 334 114 11 371 284 42 101 82 308 262 561 147 349 94 213 117 213 383 44 497 95 25 15 310 439 202 94 32 317 393 337 159 467 35 238 143 86 85 120 347 1105 163 193 304 151 134 141 88 1025 80 82 30 140 112 165 87 123 228 102 55 87 73 180 169 37 85 193 460 98 137 241 214 338 149 142 511 327 81 345 73 43 70 0 414 85 226 75 96 112 170 534 77 223 400 137 510 324 32 115 174 153 89 196 149 237 225 389 131 139 154 202 113 45 140 205 332 128 299 193 357 66 364 450 118 171 200 126 154 143 124 68 334 83 85 81 68 253 210 53 109 186 113 286 166 1445 ]
@@@ Loss per-class: [ 0.941669 4.00787 3.03212 7.0881 4.01956 4.27622 3.94624 7.70555 3.00175 1.53313 1.49093 4.10011 1.65419 3.02703 2.83849 2.99787 2.71713 2.70635 3.15875 1.87657 2.05154 3.01419 2.01719 4.607 2.77208 4.96648 1.78924 2.17293 2.53908 2.72954 4.93461 4.98105 4.09313 5.61296 1.98576 3.64352 1.78011 1.57407 2.51712 4.94154 5.99695 2.71297 4.26249 3.35237 1.44173 2.67625 3.22067 3.64049 1.54016 1.5473 2.8594 0.538716 8.33926 2.0235 3.78656 1.87232 2.34151 1.94907 6.37541 3.36908 5.20333 1.72799 4.53698 2.65859 1.51931 7.52281 7.2523 2.28032 1.41582 2.91196 2.17026 3.2943 2.40023 5.13651 6.15772 3.0966 2.98632 0.946652 2.0217 2.89645 4.36709 6.8888 2.6834 3.26754 2.39078 3.00479 1.6945 2.11083 3.61072 3.46995 1.77819 2.69911 2.10981 1.89469 5.79004 1.80883 3.5523 2.38939 4.37211 2.4346 4.07765 2.88981 2.29344 3.74832 3.12397 1.41233 4.15843 3.34129 2.89003 1.41718 2.34768 4.44324 3.64106 2.73567 1.82224 2.9902 2.5609 8.66585 7.14385 7.98005 1.42642 3.88982 2.40962 4.44003 2.37802 5.79243 2.70674 2.73953 4.83873 3.55933 3.45923 2.24517 3.35229 7.72467 4.69391 7.1325 6.39475 3.60027 5.15997 3.26223 2.95722 4.83567 3.37141 4.97258 2.36443 3.41212 2.7429 5.27131 2.96707 3.32804 4.11606 3.33027 3.13717 3.6797 4.06655 3.25913 2.87032 3.17689 2.80001 1.8771 4.34626 2.94891 5.58464 7.41748 3.46535 2.48183 3.43497 3.66851 1.91695 2.94375 2.6737 3.47596 4.85063 3.6543 2.90254 3.16302 2.53461 6.03283 3.51547 2.12963 3.22918 2.65767 3.00683 3.15335 4.79097 3.80777 3.22959 2.91056 2.7411 4.36095 0 2.46138 6.19985 3.35025 4.11999 2.83913 4.4506 2.16958 3.5605 2.86546 5.63228 3.67952 1.79877 2.68342 3.14814 2.04587 2.02919 2.67589 2.74935 3.96145 2.95155 3.88764 3.71004 3.88452 4.56496 8.55566 5.69411 2.71023 3.22648 3.31618 2.12125 2.35319 3.69496 4.63856 5.7273 4.51776 3.96532 3.78675 5.3816 4.57526 1.96586 4.98604 3.58577 3.40038 2.52638 3.18107 3.08401 3.43755 1.8947 2.85855 2.01494 3.70759 2.15847 3.39807 5.14064 8.87984 8.53968 3.75483 2.38033 2.3328 2.89032 2.48283 3.34254 2.05882 3.76454 3.3358 2.11495 8.05814 2.4855 3.50314 4.1849 4.12302 2.54343 9.2751 4.38541 2.12404 2.47185 4.21909 2.0328 4.78844 7.1761 4.18887 3.14253 1.88719 4.96855 2.14577 6.68847 3.7282 3.94096 4.4804 4.94115 10.046 1.52521 4.5039 3.74094 4.21704 3.75477 3.1357 4.19844 2.63027 3.77005 3.79543 3.04743 4.58692 4.30122 2.22043 3.185 3.85286 2.79256 3.41466 3.03459 5.60859 6.77815 3.44668 2.62641 3.2138 3.40524 3.07645 2.22124 7.13786 1.63162 3.97868 2.37819 2.98864 5.60517 3.65562 4.91045 5.35171 3.27388 3.21017 2.80168 2.79324 3.40636 3.17357 2.59203 3.6613 4.75165 4.3783 5.12518 5.95228 4.50192 2.56167 3.6282 2.96149 3.41531 1.96674 3.71301 3.93453 2.63257 2.58779 1.95872 4.60372 2.98631 4.30172 3.49427 2.5733 4.27339 5.82774 2.97729 2.99647 2.3737 4.51902 6.99417 3.22256 4.23547 1.6802 7.84012 2.73287 3.65434 1.74678 3.66463 5.69468 3.29023 4.30192 3.36511 1.54818 4.3917 4.14112 5.17892 2.4142 3.62698 3.16206 2.8736 2.79478 3.42239 3.69824 3.55013 4.49943 4.83618 3.83555 2.15622 4.28232 2.17958 2.99876 4.67793 3.29847 2.52216 5.58894 3.50633 3.72442 8.01677 3.22031 3.56512 2.79569 2.85533 4.72217 2.13094 1.86831 3.3191 6.14522 3.93516 7.51683 4.42852 2.62916 6.89767 4.91821 4.84403 8.55424 1.36416 3.61373 4.90687 3.62964 2.27487 2.74802 2.51063 3.11255 2.82168 6.32054 3.37759 2.22209 3.45682 3.46973 1.98246 3.52618 2.78988 4.60879 4.82168 3.51835 3.26495 4.30843 2.27212 3.35787 3.54886 4.12293 3.00041 3.56233 2.90323 3.31154 2.94385 3.88895 3.02624 2.1176 4.80017 4.1011 3.52015 3.93661 3.5263 8.66532 7.82651 2.0457 2.45304 4.64048 5.01323 2.81649 3.57047 7.58772 3.11826 3.71605 7.25825 2.43944 5.89901 3.84724 3.05904 4.35921 3.60427 3.88213 3.55473 2.08728 3.59639 5.30634 5.24873 3.72012 1.65568 3.5289 2.42125 2.56615 2.2802 3.71018 3.53997 6.52192 4.72086 4.13933 5.99986 2.85828 3.49688 3.42577 2.6844 4.08042 5.81896 5.66494 4.14077 3.42235 5.95792 4.06346 3.93553 2.69989 1.7106 4.47541 2.77196 1.35172 3.41539 4.77217 2.64635 2.98872 3.18656 4.97244 2.68719 2.77528 4.02945 2.44853 3.53823 3.78205 3.21027 3.09555 4.78023 3.48606 4.72624 3.06848 3.4124 2.76741 2.37946 3.97423 4.75641 4.03357 2.07748 5.71923 4.40259 5.5334 3.79104 6.15761 2.70167 4.28054 3.6163 2.52771 3.50671 4.26457 1.87603 3.6379 4.78785 4.30241 2.0964 3.26062 7.14174 8.59121 4.23671 4.16736 2.85517 1.99536 4.20908 3.47741 2.93568 3.05539 6.33911 2.82644 4.55368 2.95715 3.60676 2.83923 4.41103 2.77506 2.96687 2.15739 3.55505 2.51157 6.02647 5.16211 3.5347 6.44103 3.38374 2.4711 2.60075 3.49532 4.54266 5.87908 4.21054 3.39311 3.49034 2.7735 2.44753 5.84293 2.55409 6.30409 3.04031 3.03575 5.05338 1.64628 2.43202 4.90967 5.2072 2.95857 3.49879 4.6142 2.32304 4.42647 2.32366 4.12834 4.29451 3.33068 4.57076 7.59668 2.69283 3.57394 3.29524 3.33501 2.33448 3.24796 1.93813 4.05446 3.07587 2.77162 3.09836 3.42614 5.11542 3.85905 3.15601 6.18368 2.33636 1.80302 8.93652 4.22599 3.24388 4.09757 2.19843 3.41685 8.18831 4.82677 4.50176 2.79213 8.33583 3.55474 6.35455 3.10301 2.44095 4.55936 3.46051 4.62702 4.36878 2.73402 3.00336 3.11213 6.45316 6.51797 4.74984 2.35709 3.52423 2.01794 3.39705 4.39896 4.81006 6.78082 3.7962 2.57239 2.2499 7.89313 4.20642 6.93966 4.83462 3.48737 2.12629 3.31304 2.75313 4.49697 5.99381 4.71645 3.85365 2.50776 6.69324 2.60977 3.62644 3.3828 5.10097 3.7324 2.94196 2.19005 6.10045 3.85026 3.7359 1.68792 3.32828 3.40442 5.81918 4.95869 3.44975 3.38264 2.01091 3.42039 3.8137 7.00658 4.64454 3.87449 2.51212 3.28243 5.89567 3.27617 2.8535 3.13391 4.9643 7.47572 3.71398 2.49987 11.9404 3.80628 3.95751 2.12647 4.68579 3.14877 7.16196 3.35823 11.1761 2.12717 3.18475 2.04032 2.83914 4.14768 6.13131 2.6485 3.37593 3.59483 3.32388 2.35722 2.6545 2.89577 4.38659 3.18935 2.77204 3.20801 1.72254 2.21607 7.30787 3.39002 5.03504 2.9256 3.12816 2.90411 2.69743 3.95401 3.00637 2.054 0 2.69906 2.91821 4.04057 1.99853 9.68561 3.26485 2.44698 2.90682 5.0944 9.36181 3.34671 6.51828 4.41221 3.78208 3.6892 7.45312 4.11868 2.27828 3.28388 1.38802 2.49412 4.53741 4.98152 1.81649 3.24588 2.97351 3.00164 4.76975 2.53446 3.49681 6.50853 3.04574 2.80674 4.55947 2.49101 3.02889 4.00393 5.10361 9.19236 1.43607 4.36167 6.13124 4.64474 3.26644 3.15282 2.90289 5.18716 3.74343 3.12646 2.44015 5.6295 4.39559 5.03372 3.74234 2.7737 2.85949 3.20693 4.17325 3.14483 7.27172 2.85934 2.38059 3.55082 4.16683 2.93643 4.54832 7.74029 7.99822 4.23313 2.01233 4.25684 4.20714 3.70039 2.65996 4.15503 5.59109 5.86707 2.42186 10.7952 9.78282 3.75831 7.26248 4.17217 3.7156 3.09697 1.87662 3.84271 8.43027 5.16969 2.36012 3.05662 2.98609 4.18106 4.15508 4.12435 7.28844 5.94422 4.23851 2.55534 2.83088 2.65234 3.67797 3.7021 1.46633 6.17806 3.27607 3.05637 2.31509 4.42888 3.38316 3.06445 4.81874 2.74145 2.67385 2.94666 2.65887 4.19657 7.23147 1.68864 2.67326 2.43513 3.87533 4.0721 4.35422 3.47783 4.44164 3.84195 1.94695 2.77265 1.72633 2.41684 2.90153 6.3316 2.19905 8.26466 2.28865 2.04712 3.39548 0 4.80223 3.07627 4.69054 2.70512 5.2595 3.54171 2.8973 2.01793 4.53554 5.92131 6.58609 4.28714 3.45315 5.03263 3.45945 8.806 3.23172 2.43427 3.04348 4.32479 3.67194 6.17629 4.71966 3.28882 4.55828 3.67878 5.48824 4.55509 5.93683 2.88147 2.6581 3.14979 0 3.71549 5.12871 2.77843 8.00643 1.83101 1.91285 3.61018 5.30832 6.41261 2.81567 3.39392 2.8326 3.58792 2.68667 1.8285 7.35522 3.21618 2.94943 9.10116 4.27919 3.54168 2.98909 6.39179 3.26771 2.59065 3.11288 3.56987 4.45056 2.3178 5.95383 3.53359 4.83568 1.88511 3.30549 3.80467 3.9074 5.69474 4.12733 3.03873 2.86012 3.25155 2.923 3.33921 3.23053 2.92997 4.12777 7.97864 3.81653 2.36655 5.73821 3.87116 2.86721 3.63969 3.15015 2.96767 3.76457 6.97177 3.65749 5.15194 3.85131 4.29077 4.68731 4.86636 2.56876 4.17094 1.75549 2.47398 2.4513 2.3806 3.98384 3.62341 4.19543 4.04446 4.95778 4.14701 2.47204 2.72051 2.79163 5.13182 4.80908 4.54404 3.45767 2.11554 4.84818 3.33375 3.77065 3.2378 2.9151 3.0827 4.27971 4.56546 3.31858 3.58037 4.42223 5.91678 4.64624 6.11194 3.67168 4.62552 1.99454 2.7722 2.47188 3.38626 4.34072 3.26692 5.36175 4.22166 3.09442 3.13595 4.29846 3.64663 5.46862 3.32078 3.38016 3.51092 2.54621 4.57191 2.0467 6.18072 1.71528 2.72113 2.56718 3.53046 3.68996 7.82984 3.62844 3.91339 3.12224 7.00805 2.63138 2.89127 2.59613 2.44018 3.37792 1.7449 3.76534 2.89925 2.55544 5.0387 2.75336 2.5894 4.41893 4.50663 2.97554 4.10972 4.50473 4.39843 2.95855 2.48575 3.00848 3.40176 2.15997 2.68025 3.57548 3.22555 2.97808 2.68402 4.32602 4.77193 3.71601 3.65935 4.23582 2.08246 3.14437 3.73381 4.24458 3.46295 4.73283 2.37093 3.55716 3.52276 4.8979 3.40463 3.90221 4.27955 3.45749 1.55079 4.42888 3.37522 8.43248 6.98835 3.36323 4.01426 3.58821 3.17339 3.50763 4.05922 3.16934 3.30888 8.09573 5.16438 2.8214 3.56193 3.94847 3.43059 4.94669 4.26711 4.60235 8.2675 3.38926 4.16234 4.93902 6.08683 2.5094 3.90426 4.64591 2.29573 3.78248 4.20688 3.57178 1.66566 3.0449 2.62532 1.99929 3.92014 6.63583 2.05501 2.36845 2.7904 3.148 2.17603 4.71909 2.58853 3.86757 6.13819 3.36555 3.39649 3.15084 2.068 3.12377 3.44629 2.83329 2.43446 2.82913 6.00935 2.70107 4.38847 2.59544 2.5265 3.93644 5.13409 5.10205 2.68595 3.55703 3.39925 3.47049 2.46262 2.58029 6.12499 3.41106 0 9.31685 2.76249 5.17669 3.68672 3.01417 2.38325 3.76803 5.31469 2.39752 3.75942 2.47734 2.89995 3.42813 7.34358 2.96265 5.59218 3.39585 2.48262 4.51673 4.2771 4.4805 4.25245 4.36924 4.33492 3.43012 6.29536 3.73863 4.49451 4.98487 3.01622 3.72539 2.33116 2.45188 2.21098 3.38657 3.75841 2.30405 2.39221 5.13908 2.27656 2.80162 2.33403 2.71415 2.5605 3.61083 2.58942 4.77893 2.39403 3.79795 3.22216 3.5012 3.10313 4.29884 2.61175 6.83854 2.81638 3.68056 4.95477 3.50163 1.53753 4.14146 2.67264 5.75804 1.8532 3.51174 6.2603 3.479 2.78376 3.26049 2.79511 6.72366 4.38242 7.2863 3.86693 3.81216 2.83491 3.01606 3.33077 2.79975 3.04477 2.60014 4.62377 3.20684 6.28199 5.1606 3.74004 2.91908 2.66771 3.5078 6.67467 3.06003 2.51617 2.62269 4.22061 5.86868 6.21839 3.74694 3.80575 2.33444 3.07454 3.6076 4.6174 3.94289 4.91889 7.2569 4.12671 3.7643 1.72365 3.18601 3.29259 1.94617 0 3.12576 2.91042 3.53691 5.15003 8.83349 2.42561 2.00667 2.46595 3.0039 4.48581 3.56451 3.62258 2.95394 2.90172 2.44255 4.3637 3.12205 2.0276 4.11081 2.11433 5.26888 4.21852 4.20502 3.37516 2.79103 1.69375 2.97166 5.27706 3.37233 4.69861 2.93286 2.54514 3.50711 6.23784 2.39151 4.23357 3.73801 6.29225 2.66171 5.18297 2.28178 2.39742 4.85716 2.5593 3.2367 2.49325 7.13625 2.49344 8.34517 3.666 5.03021 3.01637 1.86163 3.30963 3.67909 3.79724 3.7266 5.34029 8.78393 5.73199 3.15155 4.77468 3.85838 3.04841 2.4326 2.70448 2.76723 5.2475 2.67497 3.405 4.27167 6.16793 2.80976 2.67209 3.07959 3.3728 3.3025 4.7083 5.49833 2.76631 4.28824 1.83126 2.58903 3.0296 2.76788 2.8557 5.27166 4.69613 5.07815 6.36327 7.43515 4.26209 3.2031 2.13764 1.78263 3.8416 2.86627 3.08881 2.61522 4.09616 3.53049 5.72666 4.25541 3.94957 6.09034 4.30991 3.75343 3.6472 2.24601 2.11169 2.90829 3.32894 3.26641 3.48662 4.40282 4.04709 3.23012 4.02551 1.86605 3.19374 4.84003 4.705 4.17113 4.96444 6.16912 3.79657 3.7817 8.29084 2.59856 3.82335 5.87794 3.93136 3.78317 3.32658 4.441 2.24484 3.51766 2.63535 3.8067 2.29375 5.70713 3.70847 1.68276 6.23474 2.19162 4.99821 6.7733 7.23851 2.28003 2.19631 2.84582 4.92557 7.28395 3.73116 2.776 2.99727 3.22805 2.94892 5.65534 3.42464 4.14899 4.39488 5.85768 3.38089 3.38126 1.5248 3.58669 2.98875 2.96796 3.28259 3.12187 4.00266 3.67731 1.78779 4.62128 5.5567 6.45041 3.04957 3.87369 3.88206 4.68661 3.6924 3.46615 4.85069 5.77241 4.51053 5.41788 2.92283 4.29732 4.63381 4.32851 3.37943 2.20812 4.26061 3.67894 2.91588 3.01086 2.80105 3.24884 3.41592 1.93281 2.68768 4.47021 2.78167 4.88156 5.84458 4.90163 0 2.50099 4.13462 3.034 5.57779 4.45321 4.34675 3.86749 2.07069 4.9746 3.34556 3.13549 4.1058 1.74025 2.20679 5.63699 3.45222 4.0694 2.83936 4.46439 4.05213 3.66932 3.216 3.10794 2.59963 4.4475 3.56515 3.60038 3.73416 3.82033 6.21769 3.63214 3.12284 2.79525 3.7138 2.25405 3.18698 2.04019 5.14818 2.55044 2.32774 3.88129 3.28956 3.69335 3.54568 3.40223 3.2681 4.02962 4.26719 1.97637 5.08285 3.6578 4.52289 4.80144 2.87389 4.0515 6.668 4.79617 4.57359 3.71923 2.45516 3.37435 0.81736 ]
@@@ Frame-accuracy per-class: [ 68.9812 18.1818 35.0148 0 15.1899 8.83978 15.9363 0 27.4143 61.822 63.0934 4.89297 58.2393 34.3096 31.0454 30.8943 40.1985 36.4066 35.6589 62.511 58.0645 29.653 58.0407 32.4324 36.8121 3.92157 54.1977 45.0607 42.3006 40.2948 4.96894 1.52672 14.9254 0 57.748 10.1911 58.3715 65.9243 39.7644 3.42857 0 42.0168 9.97732 11.8919 60.5464 49.8423 33.2288 23.7548 48.1346 63.3043 31.7549 85.4215 0 53.8899 12.8514 67.4385 43.0108 56.1983 0 31.6354 7.47664 36.7542 12.4138 35.8079 68.5494 0 0 49.1061 53.9504 24.3767 48.0186 19.4175 45.6722 2.8436 0 23.6504 23.2932 74.665 50.7757 32.8612 2.39521 0 37.5335 13.2174 47.7612 33.5731 47.8469 50.3432 17.6166 17.5772 55.6863 28.7841 58.6265 51.8231 8.42105 64.5045 19.7425 41.979 10.3896 45.648 12.3077 32.376 48.415 17.6166 34.7107 64.2247 7.46888 21.7228 33.0275 68.2725 44.2577 7.8853 18.6528 38.61 66.6667 23.7398 42.6426 0 4.08163 0 66.0399 13.198 51.9427 4.41989 47.0255 1.86916 25.3033 33.4165 9.17431 25.8706 20.2429 46.2385 28.0851 0 21.6867 0 0 19.6721 8.26446 22.9249 31.8987 9.70874 27.9793 1.21212 52.1158 6.6838 39.1645 9.90099 25.0559 30.5296 7.82123 27.1386 21.097 22.5256 12.3223 31.2057 33.0383 33.2075 28.0802 63.655 10.4712 32.491 0 0 24.6154 45.4728 19.4357 26.4317 57.9901 30.3207 38.9439 32.9588 2.8436 25.8065 39.3277 26.1307 41.9405 8.69565 38.3673 52.0354 29.1317 27.668 22.8956 27.6374 0 13.9089 28.8557 37.6417 41.1985 17.7515 0 48.4848 2.10526 38.6473 11.6838 42.8866 5.24017 54.9654 18.1818 35.225 4.12371 19.8198 62.0531 41.0423 31.1284 57.7259 60.1892 45.5243 43.1267 7.56014 22.9755 10.8108 13.3891 7.96646 10.7383 0 0 37.9562 31.0881 25.6604 53.3997 38.0952 21.118 7.80488 10.8108 9.23077 25.6881 17.6991 7.92079 4.32432 54.8009 5.07614 17.6796 24.0964 43.5678 24.6305 31.5271 23.2082 60.4867 33.7662 61.9503 21.5873 47.4645 27.11 1.94175 0 0 16.7689 40.5904 47.1976 33.8462 47.7733 25.1716 60.8485 20.5534 23.3766 54.1507 0 43.4286 12.8617 9.7561 15.0289 37.5427 0 20.1183 50.4708 38.806 16.8889 56.3991 2.09424 0 11.9149 24.9471 57.072 6.30915 42.3392 0 16.2003 18.1818 14.9733 8.80503 0 64.69 2.9304 14.4487 12.8 15.6682 40.5479 13.8728 43.9746 12.1212 18.9415 30.7692 3.20856 16.4706 57.539 34.4569 15.047 32.6531 20.8531 24.6719 0 0 26.8235 46.0177 25.0569 22.6013 39.5876 50.5338 0 66.2324 17.3285 42.5197 34.6821 15.0943 16.3522 9.94475 3.74532 31.1419 32.4111 38.4342 41.5842 19.726 20.4793 42.963 20.6373 5.07614 8.51064 4.81928 1.76991 16.5414 35.8974 18.6528 39.312 26.7516 49.9266 27.9793 17.7986 43.0727 42.3704 55.6068 15.9509 24.7241 7.48663 23.796 39.1061 6.53061 2.98507 37.1134 27.2868 45.6182 7.72532 0 28.2878 6.53061 60.9687 0 38.0697 12.987 65.2469 17.7898 3.47826 20.0608 17.8862 28.4058 63.3245 4.2328 12.5523 11.7647 55.2923 23.0947 28.3636 36.3636 37.0937 18.1303 21.2766 26.0223 7.19424 12.5984 20.6452 52.2105 10.4803 39.6872 25.6115 32.8358 38.5185 43.3566 2.46914 24.3094 18.6528 0 33.1606 19.6286 37.5405 40.2089 10.1911 52.5386 69.4878 26.4808 5.30973 21.0526 0 17.1674 39.0658 0 5.04202 7.11111 0 67.423 18.6667 5.91133 24.8062 54.4629 33.4294 43.7995 39.3305 34.0265 12.2449 19.3437 46.1384 24.3176 19.4514 52.6863 15.8358 37.8109 4.4843 5.82524 32.618 30.5284 4.31655 40.7767 20.3046 17.6211 8.13008 32 13.468 33.887 20.5734 38.9381 10.7623 32.5333 51.3692 0 13.3333 13.9535 11.1437 20.4444 0 0 59.9496 40.3013 13.4831 5.44218 28.0597 22.8956 0 33.9768 16.9154 0 54.9521 4.08163 12.0603 32.687 16.5138 17.7215 16.1404 23.3766 55.2891 18.4438 11.7647 13.3333 19.8582 67.59 25.5639 50.3067 40.7367 46.303 11.2721 13.7931 0 7.92079 14.9533 0 35.4793 27.5229 28.9593 35.5755 14.7368 5.33333 0 1.7094 19.678 13.3333 1.76211 17.1617 34.8993 58.6839 8.09249 41.3559 73.1122 16.7089 1.12994 49.2918 36.6782 31.2236 3.82166 37.037 38.6617 15.261 38.1609 28.9963 27.8388 31.6109 37.5758 5.78035 32 10.084 33.5217 22.7848 38.0368 48.0588 5.6338 7.17949 18.3007 58.1097 3.27869 11.1801 3.30579 17.5342 0 41.847 5.83942 28.2927 38.961 41.206 19.3548 59.8919 15.3005 7.79221 7.56757 54.1593 14.9254 4.44444 0 9.15332 22.2222 37.3541 55.5891 8.05369 23.31 29.9776 33.6842 0 32.4786 13.4969 29.8625 21.5569 37.0526 18.1818 31.1615 18.5224 56.9248 23.8683 45.1193 3.38983 7.40741 22.4561 4.0404 29.6919 39.6285 35.5091 29.108 19.2 5.6338 16.3743 19.656 28.2158 26.9165 22.7904 2.46914 47.8431 1.62602 34.657 33.3966 1.29032 64.7019 50.9338 3.46821 1.16959 32.7273 17.1674 1.04712 46.8557 10.2326 41.1498 10.1167 11.1524 22.4422 4.06091 0 41.7423 30.0654 26.972 25.0951 50.6876 12.6246 58.0645 9.13242 28.9855 35.5438 28.8066 16.2162 1.76991 19.3548 27.762 4.0404 49.9083 58.5829 0 11.215 23.2558 8.53081 51.3744 22.6629 0 14.1593 11.7216 46.2094 0 22.6044 0 37.8601 46.2992 12.549 19.5489 9.31264 12.2905 38.2838 25.0441 23.8727 2.91971 4.25532 6.06061 49.9318 21.6949 47.5854 21.1886 2.36686 7.84314 0 15.2381 55.814 54.0962 0 10.8949 0 11.3475 7.93201 48.2871 34.2629 44.8179 10.9804 1.29032 5.91716 16.4649 47.1178 0 28.6159 17.3913 25.387 6.10687 25.2874 32.3296 39.7229 9.52381 16.9014 21.2454 63.1699 26.4151 26.7782 2.98507 4.21053 26.0355 25.6716 58.3699 20.6718 20.1183 4.25532 7.45342 13.5385 31.694 28.4337 0 29.2135 46.2668 25.1256 2.39521 0 6.06061 47.5868 0 13.9535 11.3821 48.0326 0 38.3142 0 24.024 0 54.6003 25.9155 50.8475 35.1816 18.8976 14.6341 37.8897 14.6965 18.1818 30.1954 22.2539 35.3468 27.7847 11.3208 21.0169 25.0746 22.8571 62.2134 51.2031 0 28.4424 4.58015 32 25.5159 34.0792 39.0158 10.4803 28.8288 55.2655 0 43.5443 32.7087 17.1779 48.03 0 31.3253 37.6417 37.5267 1.83486 0 17.5227 0 8.74317 10.5012 23.3503 0 22.0859 48.283 22.409 64.3031 43.8356 12.0548 10.6509 55.1768 33.4928 23.0088 22.2222 5.47945 44.3149 21.3333 0 20.3774 34.6604 24.8588 45.4746 36.4123 20.915 7.92079 0 70.8348 9.87654 0 2.42424 21.6634 29.484 28.5714 5.7554 4.68227 25.1208 34.7648 0 14.5985 12.9032 28.5714 38.6577 36.3636 19.6825 7.25389 27.051 0 35.9897 50.0787 24.911 14.0541 27.9302 4.89796 0 0 5.32319 60.0698 8.25959 4.32432 13.5977 35.8788 9.65517 4.21053 2.53165 44.7439 0 0 17.9104 0 11.9403 15.6352 27.9835 57.0477 18.3784 0 2.51046 47.0363 32.7759 36.0656 9.77778 9.88593 20.765 0 0 15.8996 28.9517 31.0881 42.7015 19.3309 13.3333 76.333 0 32.4638 35.4098 54.6845 9.92908 20.4499 43.0976 1.14286 37.9888 32.4168 21.9081 44.1379 11.9658 0 64.3237 33.1288 39.6947 16.6065 18.3746 15.1724 30.4462 25.2252 13.5667 48.5226 29.7694 68.9346 54.2767 29.4455 0 56.1404 0 37.7994 65.4664 17.2324 0 1.14286 31.6008 7.45342 28.6792 15.5844 21.5385 37.6176 53.9349 11.7302 0 3.38983 4.0678 23.7691 4.11523 29.5935 0 24.9364 51.046 25.3898 17.9211 14.8997 7.01754 6.22222 29.2398 7.01031 13.6882 2.15054 9.3617 2.71493 32.6454 39.0641 25.2354 0 21.8623 4.03458 38.4314 0 55.8459 58.9453 18.3908 4.12371 0 43.7923 25.0746 19.3437 14.978 38.4749 54.2056 0 29.0196 32.3024 0 13.0612 19.3548 47.7396 0 27.1186 42.615 18.7311 23.7288 16.7273 36.3964 0 17.6707 11.4286 58.9474 41.4545 17.6245 22.7758 2.06186 15.4589 26.0586 38.9189 37.4622 35.9909 21.1982 27.762 35.1396 13.617 0 16.9972 45.3222 0 23.8994 36.3636 40.9222 33.2468 31.7848 24.1758 0 14.4201 2.25989 14.6939 12.2449 8.49421 6.45161 41.0997 20.1405 54.2783 48.8889 50.5561 46.0177 21.5768 25.8065 15.2284 17.734 1.6 10.1408 46.8172 36.7239 32.32 1.83486 18.6667 14.8148 20.4947 49.0352 3.27869 30.1413 12.5436 22.1488 32.1981 40.4834 5.90717 5.50459 22.5873 22.2222 6.27451 0 6.54545 0 20.9974 7.65957 60.669 30.9148 45.4713 22.0896 11.6505 29.8279 1.21212 11.1588 31.4176 22.1574 15.5251 22.5256 6.85714 15.4982 27.7966 23.0563 48.14 5.01792 56.4565 0 58.1016 43.6433 40.5253 18.0451 18.8976 0 17.8313 5.3012 42.7673 0 44.5916 40.5546 37.3796 49.92 29.2191 65.4162 7.53138 39.0456 22.8326 3.6036 28.2238 39.7678 7.79221 19.8758 37.4532 11.0672 8.16327 16.7665 32.7122 49.6454 37.2881 23.9631 43.7776 41.5606 25.0765 22.4299 30.5764 36.4672 12.0219 3.72671 12.523 25.498 5.61404 52.073 36.6133 19.9134 10.2894 16.1369 9.24574 40.6154 20.1022 15.6584 3.14961 25.4669 27.027 19.0871 22.9205 68.5167 11.5942 27.23 0 0 32.7526 15.9696 14.2259 25.7757 26.7101 12.7341 25.3521 27.3171 0 7.40741 42.2287 23.1884 19.7802 27.8373 6.10687 10.0559 9.74212 0 28.2421 11.245 0 0 45.7953 5.90406 12.9032 49.0323 20.3279 14.3541 12.6394 58.4939 34.5035 40.9371 62.4553 5.90164 4.25532 52.7307 54.7368 41.0862 40.4432 53.5097 9.44882 35.8423 9.5672 0 26.186 18.2524 21.8009 57.9795 30.4985 30.4636 31.3972 50.9372 22.67 2.46914 37.6068 13.986 50.3226 43.3862 13.3333 3.4188 10.084 40.3596 21.4047 19.0654 18.9247 41.9134 36.036 0 28.3262 0 0 37.4737 9.30233 18.8841 32.981 50.0455 11.215 0 49.4977 14.914 42.2998 30.2703 27.7946 0 28.0811 1.79372 29.0076 35.9347 11.4286 11.6959 4.05063 16.7665 9.7561 12.7796 27.6827 0 7.56014 6.03774 12.987 22.9787 24.6499 45.9096 43.1373 49.1954 16.3934 12.0996 52.8996 50.7096 2.71493 48.474 37.2323 53.5365 40.1022 48.2385 27.0169 41.8816 9.00901 34.7888 9.06344 29.4906 21.097 27.2608 12.9693 33.414 0 34.9515 20.8469 3.5503 15.1134 61.1579 14.6179 46.0274 7.61905 63.2201 34.1207 16.2162 20.7358 42.8341 27.4354 38.1609 0 9.57179 0 7.59494 17.2589 33.4944 33.9623 21.2219 45.7002 27.9302 32.3907 0.851064 38.2749 0 7.33945 16.8421 30.0231 48.5066 24.3902 4.08163 29.9517 44.0049 44.3595 31.0559 0 0 22.6244 15.814 42.243 31.793 16.152 6.62252 16.4948 5.51724 0 6.28931 20.4778 67.7608 28.2908 26.513 55.7983 0 34.8968 41.2266 25.9386 6.27451 0 35.9486 49.9685 45.3901 36.478 3.23887 19.4774 23.6934 35.9338 37.149 41.8431 21.2766 36.4912 55.8205 13.5922 46.631 9.34579 20.4724 16.4948 44.8598 44.0882 70.6503 31.9173 4.55696 19.4503 12.5786 34.7826 45.1232 20.6718 0 38.9281 10.1818 16.6154 0 48.8518 10.9705 50.31 50.4537 8.23045 37.8698 22.3853 46.3074 0 38.1982 0 20.2985 14.652 24.5614 61.9411 27.1777 27.4247 23.0986 17.2308 0 0 3.10078 35.732 9.56938 12.4629 31.1688 43.609 35.9486 34.7192 1.83486 51.9008 24.4328 11.8609 0 39.4074 48.0167 19.8496 19.8198 27.1468 11.3208 1.52091 36.5782 22.7488 61.6172 37.8072 32.7869 28.4382 32.316 0 3.663 2.15054 0 0 9.52381 31.4019 57.8046 57.9439 12.9159 35.6303 36.6197 41.8262 15.9204 26.2582 0 15.8996 10.1818 5.40541 14.5594 19.4093 34.4086 58.0796 57.0207 32.5733 14.0162 28.5714 23.166 4.8583 19.0045 29.0976 15.3846 60.4039 25.641 1.41844 3.06513 11.1888 8.63309 0 13.7519 19.214 0 38.7618 24.9561 0 17.734 27.8788 28.8493 9.14286 37.3998 20.339 42.9185 17.9894 50.1171 0 17.3302 68.0574 0 61.9095 0 0 0 41.868 43.4585 40.9877 8.46561 0 20.4724 29.7332 31.7037 27.5862 35.9358 2.8169 20.9644 16.7247 6.93642 0 26.556 28.2014 65.5812 18.3486 32.0413 29.8851 20.462 40.8922 12.7208 13.5593 55.2901 7.45342 6.06061 6.55738 32.7402 13.3333 19.3353 12.5714 20.2429 21.8818 9.7561 0 26.2857 12.2449 31.0249 12.9794 24 8.18713 32.5581 53.4202 15.2284 23.2727 39.3375 34.4988 38.7001 24.7492 23.8596 52.1994 38.4733 4.90798 37.3372 13.6054 0 1.41844 0 46.3209 12.8655 30.0221 0 11.399 6.22222 8.79765 49.392 9.03226 25.9508 30.9613 14.5455 64.6425 51.772 3.07692 28.5714 25.2149 33.2248 10.0559 16.285 18.0602 34.9474 35.0333 43.9024 5.32319 25.0896 25.2427 17.7778 17.6211 0 16.3701 39.4161 43.0075 13.2296 51.7529 18.0879 55.6643 1.50376 52.4005 42.3973 15.1899 22.7405 25.9352 32.4111 23.301 21.6028 9.63855 23.3577 61.8834 2.39521 24.5614 6.13497 4.37956 28.0079 10.9264 0 9.13242 11.2601 10.5727 33.5079 39.6396 88.689 ]

LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:406) AvgLoss: 2.73538 (Xent), [AvgXent: 2.73538, AvgTargetEnt: 0]
progress: []
FRAME_ACCURACY >> 39.7261% <<

