nsclab-gpu
nnet-train-frmshuff --cross-validate=false --randomize=true --verbose=0 --minibatch-size=256 --randomizer-size=32768 --randomizer-seed=777 --learn-rate=0.008 --momentum=0 --l1-penalty=0 --l2-penalty=0 --feature-transform=exp/dnn4_pretrain-dbn_dnn/final.feature_transform 'ark:copy-feats scp:exp/dnn4_pretrain-dbn_dnn/train.scp ark:- |' 'ark:ali-to-pdf exp/tri3_ali/final.mdl "ark:gunzip -c exp/tri3_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- |' exp/dnn4_pretrain-dbn_dnn/nnet_dbn_dnn.init exp/dnn4_pretrain-dbn_dnn/nnet/nnet_dbn_dnn_iter01 
WARNING (nnet-train-frmshuff[5.5.294-06484]:SelectGpuId():cu-device.cc:221) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:349) Selecting from 2 GPUs
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:364) cudaSetDevice(0): TITAN Xp	free:12023M, used:173M, total:12196M, free/total:0.985815
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:364) cudaSetDevice(1): TITAN Xp	free:11888M, used:304M, total:12192M, free/total:0.975032
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:411) Trying to select device: 0 (automatically), mem_ratio: 0.985815
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:430) Success selecting device 0 free mem ratio: 0.985815
LOG (nnet-train-frmshuff[5.5.294-06484]:FinalizeActiveGpu():cu-device.cc:284) The active GPU is [0]: TITAN Xp	free:11957M, used:239M, total:12196M, free/total:0.980403 version 6.1
copy-feats scp:exp/dnn4_pretrain-dbn_dnn/train.scp ark:- 
LOG (nnet-train-frmshuff[5.5.294-06484]:Init():nnet-randomizer.cc:32) Seeding by srand with : 777
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:163) TRAINING STARTED
ali-to-post ark:- ark:- 
ali-to-pdf exp/tri3_ali/final.mdl 'ark:gunzip -c exp/tri3_ali/ali.*.gz |' ark:- 
LOG (ali-to-pdf[5.5.294-06484]:main():ali-to-pdf.cc:68) Converted 1232 alignments to pdf sequences.
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:354) ### After 0 frames,
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:355) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -7.81539, max 8.96107, mean 0.00568746, stddev 0.99447, skewness 0.129835, kurtosis 2.1535 ) 
[1] output of <AffineTransform> ( min -22.366, max 12.5098, mean -3.19097, stddev 2.3303, skewness 0.137713, kurtosis 1.79363 ) 
[2] output of <Sigmoid> ( min 1.93445e-10, max 0.999996, mean 0.130675, stddev 0.218784, skewness 2.35434, kurtosis 4.90631 ) 
[3] output of <AffineTransform> ( min -24.9581, max 17.9471, mean -3.61249, stddev 2.32665, skewness 0.0196102, kurtosis 2.6633 ) 
[4] output of <Sigmoid> ( min 1.44821e-11, max 1, mean 0.0968869, stddev 0.183521, skewness 3.08434, kurtosis 9.64153 ) 
[5] output of <AffineTransform> ( min -17.4344, max 10.1048, mean -3.40955, stddev 1.95605, skewness 0.767349, kurtosis 2.60762 ) 
[6] output of <Sigmoid> ( min 2.68137e-08, max 0.999959, mean 0.095311, stddev 0.183614, skewness 3.16789, kurtosis 10.1007 ) 
[7] output of <AffineTransform> ( min -21.012, max 16.3956, mean -3.64489, stddev 1.97959, skewness 0.721934, kurtosis 4.36127 ) 
[8] output of <Sigmoid> ( min 7.49219e-10, max 1, mean 0.08077, stddev 0.172247, skewness 3.59505, kurtosis 13.2203 ) 
[9] output of <AffineTransform> ( min -16.8804, max 10.4634, mean -3.7578, stddev 1.80733, skewness 1.65175, kurtosis 5.57742 ) 
[10] output of <Sigmoid> ( min 4.66609e-08, max 0.999971, mean 0.0733069, stddev 0.177165, skewness 3.74872, kurtosis 13.904 ) 
[11] output of <AffineTransform> ( min -20.9698, max 15.4826, mean -3.87523, stddev 1.85528, skewness 1.31381, kurtosis 7.16411 ) 
[12] output of <Sigmoid> ( min 7.8152e-10, max 1, mean 0.0659067, stddev 0.166675, skewness 4.09661, kurtosis 16.8776 ) 
[13] output of <AffineTransform> ( min -2.97657, max 3.04436, mean -0.00660015, stddev 0.566018, skewness 0.0166488, kurtosis 0.349229 ) 
[14] output of <Softmax> ( min 2.68857e-05, max 0.0109829, mean 0.000657897, stddev 0.000411086, skewness 2.55594, kurtosis 15.7475 ) 
### END FORWARD

LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:357) 
### BACKWARD PROPAGATION BUFFER CONTENT :
[0] diff of <Input>  ( min -0.0914052, max 0.217583, mean 8.65188e-05, stddev 0.00673705, skewness 0.389588, kurtosis 18.8803 ) 
[1] diff-output of <AffineTransform> ( min -0.0834027, max 0.0448162, mean 4.654e-05, stddev 0.001841, skewness -0.105529, kurtosis 53.8462 ) 
[2] diff-output of <Sigmoid> ( min -0.334259, max 0.289281, mean -0.000136647, stddev 0.0147751, skewness -0.0469398, kurtosis 8.36337 ) 
[3] diff-output of <AffineTransform> ( min -0.0626587, max 0.051236, mean 5.23837e-05, stddev 0.00179151, skewness 0.347326, kurtosis 48.1588 ) 
[4] diff-output of <Sigmoid> ( min -0.281356, max 0.205413, mean -0.000281178, stddev 0.0164844, skewness 0.00494016, kurtosis 4.76873 ) 
[5] diff-output of <AffineTransform> ( min -0.0509775, max 0.0450782, mean 4.76365e-05, stddev 0.0018086, skewness 0.521747, kurtosis 38.8962 ) 
[6] diff-output of <Sigmoid> ( min -0.204781, max 0.184212, mean -0.000417486, stddev 0.0159762, skewness 0.0348571, kurtosis 3.99059 ) 
[7] diff-output of <AffineTransform> ( min -0.0370092, max 0.0396431, mean 5.7717e-05, stddev 0.00190464, skewness 0.791148, kurtosis 37.1557 ) 
[8] diff-output of <Sigmoid> ( min -0.162952, max 0.17491, mean -0.000316343, stddev 0.0177714, skewness 0.077381, kurtosis 3.42785 ) 
[9] diff-output of <AffineTransform> ( min -0.0374863, max 0.0391578, mean 5.06046e-05, stddev 0.00220771, skewness 0.699644, kurtosis 32.2035 ) 
[10] diff-output of <Sigmoid> ( min -0.183877, max 0.195594, mean -0.000685062, stddev 0.0239559, skewness 0.0269843, kurtosis 2.95364 ) 
[11] diff-output of <AffineTransform> ( min -0.0815404, max 0.0889261, mean 7.57568e-05, stddev 0.00595854, skewness 0.402002, kurtosis 25.7675 ) 
[12] diff-output of <Sigmoid> ( min -0.450455, max 0.451411, mean -1.4357e-05, stddev 0.0985168, skewness -0.0165809, kurtosis 0.0160282 ) 
[13] diff-output of <AffineTransform> ( min -0.999933, max 0.0109829, mean -3.52922e-10, stddev 0.0256406, skewness -38.9555, kurtosis 1515.36 ) 
[14] diff-output of <Softmax> ( min -0.999933, max 0.0109829, mean -3.52922e-10, stddev 0.0256406, skewness -38.9555, kurtosis 1515.36 ) 
### END BACKWARD


LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:358) 
### GRADIENT STATS :
Component 1 : <AffineTransform>, 
  linearity_grad ( min -0.302117, max 0.303516, mean 3.64906e-05, stddev 0.0312156, skewness 0.017035, kurtosis 1.4741 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.116636, max 0.129989, mean 0.0119142, stddev 0.0336287, skewness 0.161144, kurtosis 0.529553 ) , lr-coef 1
Component 2 : <Sigmoid>, 
Component 3 : <AffineTransform>, 
  linearity_grad ( min -0.0921213, max 0.0948579, mean 0.00180051, stddev 0.00818778, skewness 0.447468, kurtosis 5.19195 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.126299, max 0.143359, mean 0.0134102, stddev 0.0339398, skewness 0.0887995, kurtosis 1.17696 ) , lr-coef 1
Component 4 : <Sigmoid>, 
Component 5 : <AffineTransform>, 
  linearity_grad ( min -0.0883276, max 0.158063, mean 0.00123185, stddev 0.00669329, skewness 0.601516, kurtosis 6.9055 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.111958, max 0.173697, mean 0.0121949, stddev 0.0347724, skewness 0.556003, kurtosis 1.56536 ) , lr-coef 1
Component 6 : <Sigmoid>, 
Component 7 : <AffineTransform>, 
  linearity_grad ( min -0.12088, max 0.114982, mean 0.00147528, stddev 0.00714728, skewness 0.869842, kurtosis 9.35019 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.14173, max 0.176289, mean 0.0147756, stddev 0.0370968, skewness 0.390896, kurtosis 1.29529 ) , lr-coef 1
Component 8 : <Sigmoid>, 
Component 9 : <AffineTransform>, 
  linearity_grad ( min -0.128246, max 0.181852, mean 0.00110924, stddev 0.00757655, skewness 1.21052, kurtosis 13.8643 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.143565, max 0.216447, mean 0.0129548, stddev 0.043086, skewness 1.07094, kurtosis 2.83816 ) , lr-coef 1
Component 10 : <Sigmoid>, 
Component 11 : <AffineTransform>, 
  linearity_grad ( min -0.33113, max 0.465021, mean 0.00154789, stddev 0.021328, skewness 0.897801, kurtosis 23.1113 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.428034, max 0.540719, mean 0.0193938, stddev 0.126738, skewness 0.412083, kurtosis 1.71341 ) , lr-coef 1
Component 12 : <Sigmoid>, 
Component 13 : <AffineTransform>, 
  linearity_grad ( min -15.682, max 0.188415, mean 1.3182e-09, stddev 0.0924465, skewness -35.7234, kurtosis 3390.58 ) , lr-coef 1, max-norm 0
  bias_grad ( min -15.846, max 0.365144, mean -6.27417e-10, stddev 0.651178, skewness -13.7346, kurtosis 285.831 ) , lr-coef 1
Component 14 : <Softmax>, 
### END GRADIENT

LOG (ali-to-post[5.5.294-06484]:main():ali-to-post.cc:73) Converted 1232 alignments.
LOG (copy-feats[5.5.294-06484]:main():copy-feats.cc:143) Copied 1112 feature matrices.
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:384) ### After 342784 frames,
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:385) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -6.78548, max 7.66488, mean 0.00618661, stddev 0.998336, skewness 0.126755, kurtosis 2.01683 ) 
[1] output of <AffineTransform> ( min -23.6545, max 17.3681, mean -3.20913, stddev 2.91207, skewness 0.139432, kurtosis 1.73591 ) 
[2] output of <Sigmoid> ( min 5.33304e-11, max 1, mean 0.159083, stddev 0.257529, skewness 1.97083, kurtosis 2.81609 ) 
[3] output of <AffineTransform> ( min -30.5226, max 17.2395, mean -3.79167, stddev 2.51334, skewness 0.0675268, kurtosis 2.99987 ) 
[4] output of <Sigmoid> ( min 5.5491e-14, max 1, mean 0.0970417, stddev 0.188841, skewness 3.01254, kurtosis 9.09963 ) 
[5] output of <AffineTransform> ( min -15.291, max 10.9789, mean -3.20935, stddev 2.0034, skewness 0.7957, kurtosis 3.01623 ) 
[6] output of <Sigmoid> ( min 2.28667e-07, max 0.999983, mean 0.107391, stddev 0.191133, skewness 2.92841, kurtosis 8.58288 ) 
[7] output of <AffineTransform> ( min -23.2433, max 17.5889, mean -3.21178, stddev 2.23502, skewness 0.666112, kurtosis 3.56285 ) 
[8] output of <Sigmoid> ( min 8.04583e-11, max 1, mean 0.119016, stddev 0.211903, skewness 2.63047, kurtosis 6.44389 ) 
[9] output of <AffineTransform> ( min -16.3046, max 16.0514, mean -3.13992, stddev 2.52835, skewness 1.45412, kurtosis 3.42127 ) 
[10] output of <Sigmoid> ( min 8.29852e-08, max 1, mean 0.140111, stddev 0.257923, skewness 2.22876, kurtosis 3.80363 ) 
[11] output of <AffineTransform> ( min -27.9064, max 20.9028, mean -3.44901, stddev 3.04494, skewness 1.12103, kurtosis 3.71217 ) 
[12] output of <Sigmoid> ( min 7.59262e-13, max 1, mean 0.140009, stddev 0.274236, skewness 2.1988, kurtosis 3.466 ) 
[13] output of <AffineTransform> ( min -9.25131, max 16.8623, mean -0.0186978, stddev 2.45662, skewness 0.751134, kurtosis 1.49655 ) 
[14] output of <Softmax> ( min 3.54986e-11, max 0.983395, mean 0.000657762, stddev 0.0153571, skewness 42.4813, kurtosis 2073.3 ) 
### END FORWARD

LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:387) 
### BACKWARD PROPAGATION BUFFER CONTENT :
[0] diff of <Input>  ( min -1.20911, max 0.565921, mean -0.000817616, stddev 0.0351107, skewness -0.974913, kurtosis 39.7046 ) 
[1] diff-output of <AffineTransform> ( min -0.247785, max 0.366522, mean 4.80559e-05, stddev 0.0076424, skewness 0.868356, kurtosis 91.9877 ) 
[2] diff-output of <Sigmoid> ( min -1.15505, max 1.473, mean 0.000293292, stddev 0.0635145, skewness 0.0587513, kurtosis 16.9724 ) 
[3] diff-output of <AffineTransform> ( min -0.276814, max 0.288075, mean 2.58792e-05, stddev 0.00760517, skewness -0.315122, kurtosis 91.8241 ) 
[4] diff-output of <Sigmoid> ( min -1.10977, max 1.33696, mean -9.53661e-05, stddev 0.0739707, skewness -0.0743307, kurtosis 9.57103 ) 
[5] diff-output of <AffineTransform> ( min -0.199668, max 0.240011, mean 3.89086e-05, stddev 0.00747358, skewness 0.198567, kurtosis 60.9086 ) 
[6] diff-output of <Sigmoid> ( min -0.866071, max 0.990487, mean 0.000342176, stddev 0.0637856, skewness -0.0262514, kurtosis 8.99498 ) 
[7] diff-output of <AffineTransform> ( min -0.184206, max 0.175674, mean 1.11835e-05, stddev 0.00667842, skewness -0.298631, kurtosis 48.464 ) 
[8] diff-output of <Sigmoid> ( min -0.737818, max 0.824618, mean 6.5024e-05, stddev 0.055581, skewness -0.0968284, kurtosis 8.40614 ) 
[9] diff-output of <AffineTransform> ( min -0.116331, max 0.150151, mean 3.46934e-06, stddev 0.00583731, skewness -0.0429159, kurtosis 37.8289 ) 
[10] diff-output of <Sigmoid> ( min -0.662587, max 0.749892, mean 0.000172996, stddev 0.047128, skewness -0.139492, kurtosis 10.2892 ) 
[11] diff-output of <AffineTransform> ( min -0.225727, max 0.128302, mean 1.33869e-05, stddev 0.00780339, skewness -0.286483, kurtosis 33.8249 ) 
[12] diff-output of <Sigmoid> ( min -1.11108, max 0.83475, mean 0.000554854, stddev 0.0855192, skewness -0.0866432, kurtosis 3.07074 ) 
[13] diff-output of <AffineTransform> ( min -0.999997, max 0.949841, mean -3.45202e-09, stddev 0.0201068, skewness -28.4709, kurtosis 1603.63 ) 
[14] diff-output of <Softmax> ( min -0.999997, max 0.949841, mean -3.45202e-09, stddev 0.0201068, skewness -28.4709, kurtosis 1603.63 ) 
### END BACKWARD


LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:388) 
### GRADIENT STATS :
Component 1 : <AffineTransform>, 
  linearity_grad ( min -1.54707, max 1.52815, mean -0.000851076, stddev 0.125467, skewness -0.0523848, kurtosis 3.89922 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.487811, max 0.496531, mean 0.0123023, stddev 0.137839, skewness 0.0815295, kurtosis 1.22858 ) , lr-coef 1
Component 2 : <Sigmoid>, 
Component 3 : <AffineTransform>, 
  linearity_grad ( min -0.447528, max 0.802785, mean 0.00117996, stddev 0.0399274, skewness 0.339658, kurtosis 8.73324 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.516457, max 0.672158, mean 0.00662506, stddev 0.137506, skewness 0.359256, kurtosis 1.94058 ) , lr-coef 1
Component 4 : <Sigmoid>, 
Component 5 : <AffineTransform>, 
  linearity_grad ( min -0.466178, max 0.456114, mean 0.00106788, stddev 0.0278828, skewness 0.488464, kurtosis 13.0813 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.525307, max 0.77628, mean 0.00996058, stddev 0.144446, skewness 0.360639, kurtosis 2.64556 ) , lr-coef 1
Component 6 : <Sigmoid>, 
Component 7 : <AffineTransform>, 
  linearity_grad ( min -0.372333, max 0.430126, mean 0.000418232, stddev 0.0258201, skewness 0.206273, kurtosis 11.7695 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.695153, max 0.58585, mean 0.00286298, stddev 0.127555, skewness 0.122632, kurtosis 2.29688 ) , lr-coef 1
Component 8 : <Sigmoid>, 
Component 9 : <AffineTransform>, 
  linearity_grad ( min -0.336161, max 0.313774, mean 0.000188891, stddev 0.0247717, skewness -0.0424008, kurtosis 9.02795 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.534126, max 0.476896, mean 0.000888148, stddev 0.107698, skewness -0.285569, kurtosis 2.60823 ) , lr-coef 1
Component 10 : <Sigmoid>, 
Component 11 : <AffineTransform>, 
  linearity_grad ( min -0.45368, max 0.459865, mean 0.000827079, stddev 0.0391214, skewness 0.0495738, kurtosis 8.64658 ) , lr-coef 1, max-norm 0
  bias_grad ( min -0.598982, max 0.599805, mean 0.00342708, stddev 0.134776, skewness -0.0752236, kurtosis 2.11007 ) , lr-coef 1
Component 12 : <Sigmoid>, 
Component 13 : <AffineTransform>, 
  linearity_grad ( min -2.21064, max 2.5472, mean -1.16758e-08, stddev 0.102487, skewness -3.51808, kurtosis 75.6343 ) , lr-coef 1, max-norm 0
  bias_grad ( min -2.06593, max 2.55285, mean -6.27417e-10, stddev 0.322472, skewness -1.23414, kurtosis 9.81822 ) , lr-coef 1
Component 14 : <Softmax>, 
### END GRADIENT

LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:395) Done 1112 files, 0 with no tgt_mats, 0 with other errors. [TRAINING, RANDOMIZED, 0.0783305 min, processing 72935.4 frames per sec; i/o time 5.06276%]
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:405) PER-CLASS PERFORMANCE:
@@@ Frames per-class: [ 14111 120 114 116 102 29 147 431 139 624 262 548 95 133 232 62 162 188 145 264 293 157 210 252 206 227 177 121 194 213 59 160 138 41 524 149 489 359 331 244 27 18 144 252 121 124 944 114 5399 17569 1561 284 471 20 632 126 26 94 273 190 263 30 128 442 158 105 525 11 77 78 552 418 679 528 1016 172 727 226 492 148 41 38 347 273 210 329 186 75 361 141 50 229 14 179 16 260 399 380 208 348 168 179 81 181 88 1263 98 58 160 98 115 54 66 62 11 119 350 151 110 448 126 190 86 63 70 151 159 154 147 99 51 457 174 395 129 114 299 480 34 300 110 192 230 423 223 68 294 243 288 142 120 14 444 408 166 105 144 162 529 125 74 223 6 210 64 142 100 249 210 274 130 35 152 111 628 206 146 284 105 10 20 172 279 270 107 661 122 308 130 173 423 220 622 85 221 292 20 66 211 128 191 614 140 15 160 183 392 155 147 70 20 125 365 508 342 43 242 266 181 354 229 517 160 211 158 92 236 201 194 33 15 451 392 162 89 139 99 401 98 88 71 358 242 87 117 16 197 149 150 216 426 191 185 158 110 313 96 94 154 130 118 114 166 121 21 289 100 381 83 90 18 148 268 118 186 92 525 93 176 196 123 174 146 156 68 60 152 179 13 146 212 95 173 135 298 16 240 157 78 116 133 150 196 69 70 269 161 182 203 142 109 580 111 212 100 113 81 22 106 188 435 309 183 96 159 180 813 140 117 84 160 60 136 105 93 279 49 323 115 81 187 130 220 198 248 229 121 138 321 339 134 107 289 145 381 16 122 414 251 218 14 704 214 173 295 205 31 188 148 232 127 78 197 214 102 228 131 158 286 12 325 95 342 165 147 125 128 38 240 479 145 74 148 97 75 288 83 71 221 75 341 213 744 497 119 182 123 166 206 71 174 21 369 305 186 525 28 62 137 211 161 86 153 69 98 289 226 97 32 248 67 432 171 470 200 262 113 68 112 237 175 493 28 74 192 110 147 76 178 359 200 35 65 15 148 172 163 12 106 46 106 215 128 158 115 37 61 128 34 180 113 80 227 21 80 215 103 113 91 332 175 93 103 339 102 197 194 276 33 250 439 144 479 88 246 22 155 155 186 44 355 235 212 135 123 240 272 212 71 252 101 106 188 76 61 17 82 47 139 27 164 88 260 223 132 257 98 177 282 25 59 247 184 238 139 298 77 80 278 302 61 410 162 317 174 79 22 160 279 126 205 160 228 522 176 162 39 25 152 251 223 86 286 122 166 15 119 81 176 132 85 109 289 78 185 137 361 29 347 417 364 180 85 89 269 23 149 133 121 83 160 410 95 466 17 101 145 14 66 44 19 216 251 140 66 52 161 311 239 133 142 268 107 151 0 355 94 68 210 42 10 178 124 47 82 201 71 323 122 177 221 226 105 181 815 94 322 180 82 190 119 1408 32 372 114 94 24 267 55 79 63 291 21 150 99 752 230 170 140 110 8 98 137 290 267 238 179 97 239 110 166 159 62 123 14 30 87 117 100 356 341 52 95 143 51 151 264 109 363 8 275 14 204 202 154 229 140 80 115 52 45 276 235 129 256 254 94 238 182 131 86 130 130 96 196 120 148 225 34 202 54 553 173 208 160 275 133 618 6 45 306 59 45 178 28 112 171 289 96 111 63 455 146 103 77 247 134 346 420 197 51 400 17 181 362 122 83 799 204 92 131 1688 75 185 61 630 124 236 80 58 118 320 115 14 516 106 54 103 280 177 169 213 101 56 536 291 116 201 195 336 18 111 70 109 79 37 111 187 128 201 1008 95 457 71 152 219 305 96 191 130 235 14 71 155 225 240 189 307 189 632 15 275 157 359 52 39 8 261 510 564 249 222 352 43 224 221 130 126 45 161 151 30 170 155 229 37 61 191 190 136 49 411 41 153 223 35 366 639 104 127 85 142 456 275 230 113 288 21 120 29 17 214 79 214 255 110 257 77 118 218 13 68 101 63 251 59 183 125 145 170 202 215 109 241 161 267 174 256 90 200 49 76 92 161 276 222 56 463 654 317 130 77 151 144 186 175 292 131 213 86 220 270 108 57 131 395 106 113 85 403 81 189 54 110 144 363 179 368 232 184 400 191 489 231 230 350 465 586 149 317 131 273 50 275 11 97 38 135 368 286 204 137 215 257 418 210 224 110 55 60 75 172 202 525 277 145 337 68 189 117 50 276 313 136 179 423 28 167 194 461 203 601 9 190 227 238 177 113 47 189 451 171 151 181 96 142 129 322 103 83 200 170 55 11 92 243 494 291 121 170 693 104 145 434 141 13 101 57 58 167 42 351 179 195 376 0 245 378 244 79 288 97 116 1082 263 205 374 312 163 126 277 43 154 146 187 138 227 205 91 202 84 124 191 184 214 123 421 234 163 207 69 392 407 805 555 331 459 257 193 152 270 256 205 168 82 44 30 84 134 350 392 468 81 109 138 51 163 98 350 55 219 95 217 185 182 155 54 676 231 99 147 82 316 70 292 251 114 185 205 101 293 88 170 372 143 247 89 183 269 159 159 470 190 148 36 484 150 85 151 135 220 209 180 557 157 313 294 94 360 143 119 227 207 88 71 75 349 843 251 120 51 173 149 253 192 299 302 211 74 152 113 43 85 136 421 115 104 242 228 232 13 59 331 189 125 243 355 398 250 272 145 118 163 1961 152 115 363 91 134 74 378 346 72 160 400 86 256 55 11 200 228 129 162 250 490 56 205 245 156 452 435 409 169 263 194 225 99 561 179 234 364 164 141 80 185 173 8 19 29 132 207 139 64 156 211 16 76 131 93 282 224 71 388 32 160 40 547 120 241 139 286 192 752 249 120 243 218 46 211 120 291 79 257 198 13 1281 166 13 738 488 437 90 370 118 174 163 276 49 154 104 131 124 316 100 468 303 218 68 14 30 260 250 215 829 131 141 45 131 217 166 115 118 64 313 270 608 334 412 132 73 312 298 428 246 226 69 124 375 68 167 62 230 198 165 219 127 190 133 948 357 222 171 259 511 147 80 207 240 212 225 164 53 112 147 121 25 150 149 324 411 249 100 41 133 521 301 150 62 303 411 60 18 110 174 40 394 52 240 331 284 215 421 114 247 430 133 48 109 90 50 18 1198 95 97 208 60 279 137 170 245 280 190 293 117 334 217 91 223 276 271 218 124 173 308 265 129 146 20 159 258 333 162 477 154 438 52 1106 298 479 38 841 127 66 85 66 26 192 67 285 218 115 177 412 385 148 434 168 156 454 403 22 125 84 360 157 205 290 170 429 71 149 131 109 88 156 90 129 330 14 559 412 392 145 155 181 425 252 187 74 157 216 56 231 191 281 147 170 165 579 328 249 278 354 74 559 114 283 243 343 212 193 227 115 79 194 48 410 65 80 109 135 90 215 117 185 53 480 464 62 80 ]
@@@ Loss per-class: [ 0.820659 3.80369 3.51661 3.76722 3.02314 6.61471 3.97403 2.17546 2.62052 1.70299 1.73573 1.99335 3.5232 2.58291 2.82924 4.30378 3.15974 2.81304 3.33899 2.2178 2.14991 3.21126 2.65849 2.64392 3.11638 2.46257 3.07508 2.90468 2.74825 2.34243 4.07821 3.07947 3.50378 4.89895 1.32373 2.68198 1.22107 1.82289 2.32946 4.18333 6.68979 7.49299 2.33046 3.31454 3.32062 2.79459 1.78274 3.12401 1.61487 0.512643 1.03325 3.04218 1.72259 7.26704 1.48124 3.03795 7.19716 4.07364 1.49441 3.58924 2.15831 5.81959 3.22231 1.61677 3.48502 3.19035 2.60134 6.47658 3.37113 4.04011 1.96076 1.82242 1.6285 1.67558 1.00556 2.58155 1.84443 3.72102 1.39176 3.63648 5.41214 5.31536 2.03813 2.48624 2.51676 2.81311 2.43207 3.18324 2.06961 3.54164 4.82077 2.38147 6.70819 2.57116 6.42246 2.44004 2.27362 1.83386 3.002 1.79466 2.1435 3.74683 4.40477 3.13109 4.31829 1.19697 3.54752 3.53634 2.91261 4.14525 3.56302 5.33027 3.63603 4.74366 7.34626 3.46459 2.38112 1.98923 4.13161 2.5742 3.22107 2.93627 3.91815 5.64505 4.93583 3.3449 3.20843 3.38986 2.92545 3.93724 4.83046 2.57034 3.5713 2.49428 3.00821 3.01153 2.3751 2.34499 5.70596 2.19149 4.41156 2.68907 2.04395 2.03194 3.29308 3.58423 2.82883 2.3827 1.78904 4.18753 2.70946 8.67702 1.55567 2.31441 2.93532 3.192 3.80311 3.56415 1.7908 3.18566 4.62372 3.08807 9.47498 3.31291 3.85768 2.94213 3.99183 2.1827 2.82986 2.13636 2.91509 5.73544 2.52203 2.87822 1.50485 2.12984 3.14696 2.2933 3.99508 8.20827 7.11591 3.3846 2.91303 2.9315 3.99835 2.5937 3.22041 1.72895 2.85969 3.23817 2.05422 3.87521 1.50931 3.68678 2.59154 2.12383 7.49018 4.90423 2.03154 3.00958 2.83548 1.37443 3.1394 8.181 4.23643 4.06006 2.83395 4.00799 3.68053 4.19626 5.5608 5.00494 2.4792 2.8533 2.22759 5.09069 1.8826 2.18396 3.50622 2.21103 2.7138 2.22347 4.35091 2.93935 3.01055 3.61948 1.99763 2.55653 3.02203 6.00208 8.60691 2.35368 2.83391 2.93916 4.43604 2.83022 4.19774 2.10084 4.32058 5.03075 3.32429 1.7047 2.58757 3.67862 3.53305 7.9246 2.56257 3.29282 2.91409 2.28089 1.98021 2.94235 3.92939 2.89031 3.34463 1.94778 4.13351 5.21595 3.78627 3.28782 3.41502 2.61774 3.68352 2.28938 6.92137 2.13749 3.7775 2.03894 4.0975 4.11457 7.97666 3.63252 1.95043 4.06229 2.93301 3.41679 1.50993 3.68301 2.45951 1.5978 3.42043 2.89876 3.1629 3.65408 3.90583 5.50267 2.51655 3.07203 7.91047 3.91662 1.75886 4.08349 2.97094 3.13411 2.07231 8.21966 2.69411 4.56176 4.89979 3.22004 3.85921 3.07517 3.22929 4.69537 4.74504 3.10427 3.71262 2.85094 2.96171 3.27062 3.74166 1.54664 3.97034 3.0272 4.44044 3.65555 4.17322 8.27558 3.82095 3.22102 1.73653 1.90092 3.1127 3.74533 3.74535 3.24101 1.68406 3.49012 2.24618 4.20879 2.45817 4.90057 2.61737 5.37187 4.44912 2.67683 4.93511 3.23119 3.59268 4.5217 3.20587 3.01884 2.16098 3.14922 2.35941 2.88493 3.53774 2.20396 2.94007 2.86996 3.86297 3.53963 3.43241 2.70607 2.95276 7.06865 4.3391 2.34988 2.76933 2.11554 6.73654 1.38378 3.52282 3.32904 2.87056 2.88005 7.0205 2.83156 2.60004 2.73022 3.05076 3.05945 3.63878 2.38069 4.73432 2.71597 3.53674 2.96123 2.32738 7.91268 2.43125 4.70931 2.31306 2.7126 3.33015 3.9131 3.09737 4.55639 2.07366 1.92481 3.58903 4.30387 3.12276 4.30717 4.34512 2.31814 3.8892 3.96268 2.54223 4.87063 1.84055 2.84738 1.71662 1.54089 5.30474 2.95733 3.08911 3.29011 2.73149 3.59332 5.46114 8.60462 2.25222 2.32951 2.6808 1.96792 7.49886 4.23935 2.27515 2.61021 2.85917 4.37538 3.89435 4.1248 6.82469 2.45404 2.21739 5.73286 6.07504 2.20049 5.6661 2.85956 2.97184 1.80682 3.35141 2.54241 4.53528 3.30272 4.16812 3.43784 2.78572 1.81814 7.03711 4.68404 2.48233 3.59766 3.43743 3.67614 3.26478 2.19241 3.33367 5.40981 3.84335 5.79579 2.44793 3.69815 2.46617 10.9352 2.94919 4.28869 3.92086 3.24858 3.60114 3.52753 3.35069 5.72562 4.45892 3.37435 4.87527 3.38761 2.97875 3.87038 1.96839 6.39807 3.5899 3.74932 4.50268 3.59368 3.73333 2.48145 3.04845 4.04225 3.85268 2.58802 2.88903 3.35387 2.78516 2.14926 6.2257 2.84557 1.91004 4.08487 2.22298 3.65792 2.72536 7.28952 3.45553 3.73893 3.02669 5.18225 2.42649 2.91145 2.86033 3.75852 3.8457 3.30954 2.6801 2.28574 3.72739 2.84095 3.85727 4.68782 3.25793 3.1856 3.98697 8.41245 3.99723 4.79946 4.1459 6.3554 2.13683 3.37544 3.541 3.37801 2.79891 2.99108 4.18596 3.64949 2.59044 7.16969 4.51507 2.88545 2.71599 3.32826 2.7136 2.27218 4.72952 5.45488 2.47968 3.17109 6.03784 2.13083 4.56981 2.81901 2.64743 3.36815 7.64152 3.19948 2.68017 4.05557 4.12664 3.59976 2.26187 2.10876 3.21144 3.06791 5.97058 6.43823 3.00461 3.37001 3.39308 4.55821 1.70003 3.62133 3.87972 8.82818 4.13994 3.83633 2.71749 3.776 5.12557 4.07446 2.41003 4.58141 3.09438 2.79267 2.37096 7.1679 2.67661 2.41003 2.79389 2.66301 4.41354 4.10214 2.90294 7.18591 2.99225 2.72477 2.64796 3.56505 4.04735 1.98505 4.27757 2.13254 7.11219 3.37503 3.34199 7.28983 4.44097 7.15888 7.21456 1.72305 2.68736 5.75289 3.94752 4.48335 2.63278 3.48545 2.74158 2.95679 4.07418 1.85761 4.65515 2.8959 0 2.86789 3.97739 4.02963 3.26465 5.49389 8.78137 2.67098 3.29886 4.70834 4.67617 3.01222 4.27488 2.16594 3.59543 4.01872 2.75165 3.60711 3.62295 2.49618 2.17451 4.2872 2.05736 2.8366 4.10281 3.5508 3.86777 1.16188 6.18572 1.84456 3.14794 4.38503 6.11925 3.00577 4.41894 4.18689 3.46092 2.97127 7.59929 3.70004 4.47578 1.05741 2.55348 4.15523 3.93489 4.39278 9.73321 4.58948 4.51125 2.49479 2.95129 2.45 3.2302 3.89252 2.24032 3.29376 2.52395 4.13111 4.33632 3.69456 8.14529 5.5477 4.51775 3.28958 3.96292 2.08247 2.33359 5.13425 3.5047 3.80165 4.75921 4.19118 3.16291 3.57372 2.43119 7.27352 3.46451 8.4904 2.47463 2.79878 3.21402 2.25291 3.25168 5.4661 3.8118 5.21298 5.12796 3.96289 3.04308 5.18391 4.13604 2.46762 4.37713 3.1952 3.9654 4.36236 5.0629 2.84073 5.01534 3.39778 4.05827 3.0401 3.37047 2.38837 6.17306 2.60399 4.96835 1.82791 3.03666 3.3343 3.05104 3.14949 3.96163 1.79247 10.8705 5.62161 3.0979 4.45452 5.97453 3.60916 5.68543 4.81241 3.29634 2.56519 3.87201 4.13805 4.47638 2.57057 3.79461 3.70847 4.45306 2.91935 4.12245 2.38151 2.46879 3.39699 5.5254 2.99392 7.80094 3.31076 2.23669 3.23814 5.03637 1.46337 2.82066 3.49839 3.76571 1.3977 4.48962 2.49612 3.61338 1.4829 3.57024 2.91713 4.27327 4.91343 2.79207 2.4675 3.82445 8.39607 1.77597 3.54097 4.30422 2.89758 3.28961 3.64195 2.9106 3.64572 4.44166 5.0858 2.03695 2.88696 4.32964 2.66582 3.62909 2.25672 7.76204 4.3151 4.23092 2.96514 4.89846 5.57461 3.49975 3.49974 4.30693 2.78232 1.59648 4.04663 2.51543 4.603 3.22565 2.61482 2.20318 4.43063 3.40842 3.87172 2.62045 8.78313 4.82539 3.06946 2.59794 2.12679 2.53503 2.90911 3.6973 2.55559 6.10197 3.00882 2.45266 2.70648 6.03181 5.13358 6.81739 2.93616 2.11247 2.48731 2.67617 2.85992 1.91271 5.54625 3.18416 2.91336 3.91966 4.82847 5.7112 3.8697 4.61392 4.74691 2.69897 3.41288 3.3439 5.64424 6.3517 2.7354 3.78639 3.32582 4.83039 2.50107 5.31839 2.69896 2.84706 5.14557 2.04407 1.53526 5.30027 3.83315 3.7717 3.31961 2.89695 2.30999 3.53811 5.45433 3.14153 8.74514 3.56416 7.13501 7.94597 3.51789 4.97655 3.64648 3.13443 5.42173 3.1982 4.61007 3.61036 3.98746 7.04847 5.91471 3.77586 5.28389 3.18546 5.06311 3.33965 4.40129 4.59801 3.05628 3.62094 4.15935 5.53942 3.00431 3.39138 2.52974 2.43736 2.32605 4.30233 3.35842 4.67759 4.93634 3.7725 2.97934 2.84028 2.50755 3.47556 2.9843 1.57449 4.13748 4.74431 3.58571 3.18668 3.02437 2.96567 2.39184 2.95839 3.37466 3.02871 4.37488 2.10639 3.0143 3.28863 4.72574 3.70303 2.453 4.13677 4.14687 4.55101 2.48541 3.46613 2.21315 5.08888 3.64705 3.3781 1.89757 3.43512 2.94481 3.80591 4.13412 2.19439 3.19241 2.01122 3.40364 2.77818 3.00366 2.63403 2.41583 4.59335 3.30758 3.2348 2.88413 5.00641 2.97055 10.2847 4.14692 4.93698 3.59569 2.22434 2.31395 3.17539 3.91765 3.3999 2.98521 2.38381 2.28971 3.41357 3.35259 4.50549 4.08054 5.48064 3.11058 3.05467 2.84064 2.92775 3.04048 1.6413 5.05005 2.81078 3.21386 6.54603 2.87019 2.59292 4.20253 4.06794 2.68418 6.48833 3.85599 2.29126 2.27228 2.51777 2.88434 9.79966 3.31709 2.67654 4.03602 3.17325 4.60357 5.72618 4.4756 2.29042 4.28655 3.22057 3.56448 3.61386 3.34261 3.69806 2.06106 3.85683 5.23232 2.59984 2.98656 5.81801 7.16079 4.41262 3.83841 1.64975 2.82331 4.7777 4.31025 2.21585 3.66743 3.17501 2.20077 3.82719 7.03067 3.97982 4.31731 4.89136 2.82339 5.50157 2.03779 4.16639 3.58821 2.40179 0 4.07882 2.37566 2.24759 5.21274 2.68152 3.48357 3.8966 2.02325 3.66649 2.88437 1.9966 2.3709 3.62877 4.08157 2.32545 5.1273 2.89375 3.68032 2.96841 3.78487 3.16163 3.10305 4.25927 2.58192 4.9228 4.65167 2.60205 3.79952 3.29143 3.20609 2.14213 2.8503 2.49834 2.9915 4.93578 3.19065 3.05688 1.0188 1.70235 2.0341 2.63046 2.80594 3.44835 2.98188 2.57649 3.58694 2.37715 2.94375 4.45753 4.54186 5.92017 4.30929 3.9275 2.3483 2.65632 1.7361 4.84031 4.16086 3.65646 4.536 4.21912 4.10084 2.08531 5.79209 3.18922 4.91378 3.44044 3.71451 3.49975 3.69692 5.88771 2.32247 3.22187 4.76285 4.05722 4.32559 2.03981 4.29601 2.77963 2.70383 4.3937 3.24325 3.50963 3.47959 2.80376 4.7928 4.28745 1.93471 3.36582 2.88787 3.83111 3.17671 3.65699 3.06442 3.80593 2.17292 3.53813 3.43368 6.77258 2.75759 3.22221 4.24982 4.85201 4.33027 2.85824 3.82009 3.3094 2.19578 3.21962 2.98342 2.62298 4.80907 2.05649 3.67542 3.60817 4.18465 2.70813 5.21146 4.62138 4.3763 2.15483 1.68193 3.24801 3.47286 5.80332 4.58651 3.93402 3.03259 3.63225 3.5997 2.46605 2.32904 3.88159 3.54307 3.24004 5.73505 4.02331 4.19787 2.81006 3.39483 3.79332 3.52786 2.81617 3.36939 7.52608 5.75157 2.12563 3.32937 3.20265 2.39416 2.86777 2.81482 2.93917 3.30197 2.96506 4.89193 3.55786 1.29879 3.25267 3.31546 2.15156 5.39912 4.03972 4.71759 2.54088 2.73868 4.58067 4.29899 2.56092 4.66647 3.58131 5.94781 8.54672 3.14123 3.25094 3.99014 3.30556 2.96629 2.48973 4.41904 3.06924 3.42988 2.92366 2.51482 2.22785 1.98812 3.19249 2.53419 3.18675 3.03817 4.04707 2.19289 3.33532 1.98137 2.07456 5.70266 3.13099 4.3583 4.07353 3.21634 8.34652 8.17249 6.19395 3.54071 3.94674 4.91198 4.6022 3.35048 3.46457 6.9416 5.94604 3.39586 4.5455 2.68528 3.18894 5.43968 2.2008 6.22245 3.99842 6.35236 2.22384 2.63907 2.63612 4.05345 3.18839 2.78673 2.30938 2.89353 3.77038 4.38803 4.70425 5.02349 3.80734 4.16012 2.62847 4.98769 2.25307 3.55536 8.05828 1.42656 3.66004 8.18785 2.03111 2.27379 2.09193 5.28687 2.36198 3.64307 3.47312 3.01583 2.62585 4.97776 3.60079 3.20086 5.69534 4.29342 3.32444 3.95062 2.44485 2.27586 2.61863 5.03638 7.40016 6.53838 3.03555 2.61313 2.95344 2.54716 3.4969 2.9179 5.5913 4.33832 2.61211 4.39592 3.7874 3.56165 5.00608 3.49707 2.32111 1.74635 3.038 2.55897 4.24255 4.13783 2.88485 3.18913 2.52984 3.55844 3.76299 5.319 5.04579 3.04036 4.99128 3.40877 4.50657 3.19017 3.77973 2.43274 4.39722 3.99104 3.74282 3.05902 2.35045 2.63601 2.51344 3.48769 2.40545 1.90424 4.06704 4.32438 3.22373 3.05001 4.3014 3.23948 3.70652 5.39885 4.52715 3.36867 3.53467 6.71755 3.32729 3.36168 2.57188 2.79547 2.59098 4.02864 5.90955 3.78265 2.9609 3.52852 3.34105 5.83078 2.8399 2.43061 3.93826 6.77961 4.17319 3.0136 5.27674 2.5303 5.29711 2.55516 2.31123 2.27738 3.03397 2.36432 3.93049 2.49747 2.80959 4.80166 7.00305 4.77216 3.85432 5.21431 7.13838 1.5089 3.46753 3.83057 3.75219 5.80496 3.28359 4.62826 3.5994 2.75107 1.83664 3.98116 2.90015 4.2319 3.14342 3.09614 5.6554 3.13638 2.62055 2.8454 5.12888 3.94467 4.18264 2.78435 3.27338 3.61607 3.39018 8.68204 3.8875 3.78276 2.55997 4.09076 2.27003 2.99963 2.62175 5.51673 1.73156 2.85211 2.291 5.90461 2.12667 4.34855 5.20347 4.87019 4.47048 7.99352 2.97237 4.28147 2.81378 3.466 4.63549 3.15513 3.24882 2.55052 3.88933 2.49634 3.25753 4.64388 2.21192 1.86949 6.16817 3.5233 4.75366 2.06341 3.28244 3.34336 2.36436 4.35513 3.38886 5.49918 4.26744 4.9057 4.62182 3.55484 3.75801 4.19206 3.44819 2.87529 8.12996 2.37672 2.23208 2.94228 3.8935 3.45791 3.39838 2.43323 2.76311 3.84212 3.72979 5.16804 4.26163 5.00731 3.30209 4.28683 2.21934 2.81074 3.60078 3.32967 1.83689 2.2743 2.10288 2.50125 2.62561 5.34142 2.73189 3.63446 2.87642 3.02201 2.81269 2.3775 3.69568 3.0204 4.7418 4.49307 2.80346 4.73833 2.84046 3.99406 4.42659 4.38631 5.28608 5.2539 4.00777 3.48079 2.97503 4.37928 3.11459 1.72132 6.34622 4.83663 ]
@@@ Frame-accuracy per-class: [ 73.8121 17.4274 28.821 22.3176 45.8537 0 16.9492 57.7057 35.1254 56.5252 62.4762 45.5789 28.2723 45.6929 35.6989 12.8 30.1538 44.0318 31.6151 54.0643 54.1738 28.5714 37.5297 38.8119 25.6659 41.3187 20.2817 34.5679 30.8483 43.0913 15.1261 28.0374 28.1588 12.0482 73.2126 32.7759 77.0174 55.6328 38.6124 7.77096 0 0 52.5952 13.4653 28.8066 42.5703 27.9513 34.9345 48.1156 86.0582 67.243 16.1687 67.0201 0 67.668 31.6206 0 5.29101 65.0823 14.6982 49.3359 3.27869 34.2412 76.6102 22.7129 22.7488 37.8687 0 30.9677 14.0127 58.6425 60.9319 59.3083 67.7389 77.7177 36.5217 57.732 10.1545 74.7208 19.5286 16.8675 2.5974 53.5252 49.7258 42.2803 39.4537 49.866 35.7616 51.4523 16.9611 7.92079 44.4444 0 38.9972 0 36.0845 50.8135 63.3377 35.9712 57.3888 53.4125 17.2702 9.81595 24.7934 18.0791 73.2093 20.3046 30.7692 34.891 19.2893 20.7792 1.83486 27.0677 4.8 0 24.2678 30.8131 44.8845 12.6697 38.573 29.249 27.2966 12.7168 3.14961 11.3475 25.7426 31.348 14.8867 36.6102 19.0955 17.4757 35.4098 24.0688 44.2478 25.4826 35.8079 49.0818 46.41 2.89855 49.584 4.52489 42.5974 49.0239 60.4486 13.8702 24.8175 33.9559 46.8172 67.591 7.7193 43.1535 0 63.4421 44.0636 30.6306 29.3839 17.301 18.4615 57.6015 31.8725 6.71141 23.2662 0 27.0784 17.0543 30.8772 15.9204 59.7194 33.7292 41.1658 34.4828 2.8169 41.3115 41.2556 70.8035 51.3317 33.4471 47.4517 15.1659 0 0 29.5652 39.356 33.6414 18.6047 30.2343 36.7347 66.1264 29.8851 34.0058 50.7674 9.97732 72.7711 11.6959 47.8555 51.9658 0 1.50376 50.591 31.9066 40.7311 68.9992 34.8754 0 10.5919 5.99455 31.3376 11.5756 26.4407 14.1844 19.5122 11.1554 43.2285 35.7915 47.0073 9.1954 53.1959 51.0319 20.9366 54.1608 43.573 48.8889 9.96885 41.6076 27.1293 27.027 59.1966 44.665 21.0797 5.97015 0 44.5183 30.3185 32 20.1117 36.5591 16.0804 52.802 18.2741 14.6893 25.1748 63.5983 43.299 40 27.234 0 46.0759 27.4247 28.5714 47.1132 51.1137 20.8877 7.54717 35.3312 22.6244 52.9506 7.25389 6.34921 12.2977 25.2874 13.5021 39.3013 17.4174 49.3827 0 58.7219 14.9254 58.7156 25.1497 14.3646 0 11.4478 55.121 23.6287 27.3458 23.7838 65.0809 20.3209 38.5269 62.0865 17.8138 55.0143 38.2253 21.0863 11.6788 11.5702 40 27.8552 0 20.4778 70.5882 10.4712 25.9366 26.5683 54.6064 0 40.3326 6.98413 6.36943 30.9013 21.7228 32.5581 12.2137 4.31655 9.92908 16.3265 21.6718 42.7397 28.5012 26.6667 27.3973 65.6331 20.6278 32.4706 10.9453 23.7885 13.4969 0 9.38967 29.7082 63.6051 50.727 29.4278 25.9067 16.9279 24.3767 63.6755 15.6584 41.7021 9.46746 47.352 18.1818 35.8974 0.947867 16.0428 38.9982 6.06061 23.493 27.7056 7.36196 34.1333 31.4176 52.6077 31.738 52.3139 30.9368 12.3457 45.4874 28.3048 30.6333 11.8959 28.8372 22.7979 41.9244 31.7169 0 18.7755 44.8733 27.0378 48.0549 0 72.2498 24.7086 19.5965 31.8105 32.6034 0 36.0743 37.7104 40.4301 42.3529 24.2038 16.7089 34.965 2.92683 38.9497 22.8137 34.0694 46.7714 0 39.9386 7.32984 53.4307 36.858 22.3729 19.9203 32.6848 15.5844 56.1331 63.3994 15.1203 18.7919 29.6296 8.20513 22.5166 48.5269 26.3473 9.79021 43.7923 2.64901 64.4217 30.9133 66.4876 69.3467 6.69456 36.7123 29.9595 39.039 25.1816 27.972 4.01146 0 43.843 47.4632 48.2574 50.0476 0 14.4 49.4545 39.7163 29.1022 5.78035 9.77199 14.3885 0 33.1606 45.4746 6.15385 6.15385 58.3501 5.92593 38.8439 39.6501 59.0861 22.4439 44.5714 18.5022 29.1971 11.5556 24.4211 33.6182 64.0324 0 8.05369 47.7922 26.2443 27.1186 23.5294 22.9692 52.573 22.4439 11.2676 13.7405 12.9032 38.3838 15.0725 36.6972 0 35.6808 19.3548 15.0235 23.2019 21.7899 10.7256 26.8398 2.66667 4.87805 22.5681 26.087 21.0526 39.6476 26.087 47.033 9.30233 24.8447 15.3132 10.628 21.1454 32.7869 40 21.6524 7.48663 23.1884 40.3535 32.1951 29.8734 38.5604 49.5479 0 42.3154 61.661 12.4567 51.9291 29.3785 36.5112 0 22.508 15.4341 32.1716 2.24719 45.2883 26.327 32 19.1882 13.7652 28.2744 44.4037 50.3529 26.5734 36.0396 16.7488 12.2066 22.2812 24.8366 22.7642 0 13.3333 0 11.4695 0 44.9848 32.7684 19.9616 19.6868 40.7547 31.4563 11.1675 24.2254 47.0796 0 8.40336 35.5556 35.2304 33.1237 40.8602 47.5712 3.87097 1.24224 33.7522 28.7603 0 49.9391 6.15385 32.4409 42.9799 23.8994 0 25.5452 43.2916 21.3439 15.0852 25.5452 53.3917 50.1435 22.6629 34.4615 0 0 22.9508 22.664 17.0022 13.8728 70.5061 12.2449 12.012 0 13.3891 18.4049 30.5949 26.4151 11.6959 7.30594 49.0501 6.36943 35.0404 42.1818 44.8133 0 35.1079 23.4731 33.4705 42.6593 7.01754 21.2291 26.3451 0 24.0803 37.4532 41.1523 32.3353 14.9533 49.6955 5.2356 57.4491 5.71429 34.4828 22.6804 0 15.0376 0 0 61.8938 37.3757 2.13523 31.5789 15.2381 36.5325 23.435 34.6555 32.9588 4.21053 63.3147 1.86047 29.703 0 24.4726 8.46561 10.219 27.0784 2.35294 0 40.3361 24.0964 16.8421 8.48485 39.7022 12.5874 46.677 27.7551 18.5915 37.0203 18.543 18.0095 52.8926 32.9859 12.6984 53.3333 27.1468 16.9697 20.4724 15.8996 75.7543 0 52.6174 37.5546 6.34921 0 21.3084 1.8018 15.0943 20.4724 37.3928 0 11.2957 6.03015 79.7342 37.744 11.7302 16.3701 8.1448 0 19.2893 11.6364 39.5869 35.8879 41.0901 23.9554 16.4103 51.357 28.0543 40.2402 16.3009 19.2 20.2429 0 3.27869 4.57143 17.8723 15.9204 57.5035 36.0176 7.61905 24.0838 16.0279 25.2427 6.60066 26.087 23.7443 49.5186 0 16.3339 0 43.5208 38.5185 22.0065 37.9085 29.1815 1.24224 20.7792 7.61905 4.3956 14.8282 25.9023 2.3166 14.425 41.2574 12.6984 27.673 15.3425 9.12548 2.31214 38.3142 3.06513 23.8342 15.7761 35.6846 31.6498 52.3282 0 32.0988 11.0092 55.2846 23.6311 26.8585 24.2991 27.2232 16.4794 63.2175 0 15.3846 28.385 5.04202 8.79121 26.3305 21.0526 6.22222 27.4052 43.1779 16.5803 12.5561 6.29921 40.8342 20.4778 25.1208 7.74194 38.3838 6.69145 47.619 41.6171 25.3165 1.94175 25.4682 0 38.5675 44.6897 30.2041 0 60.5378 37.1638 18.3784 22.0532 65.4427 10.596 47.9784 27.6423 70.1031 23.2932 31.7125 7.45342 5.12821 33.7553 35.8814 11.2554 0 65.4405 25.3521 18.3486 42.5121 31.016 17.4648 31.2684 25.2927 18.7192 12.3894 48.4623 31.9039 14.5923 41.1911 16.8798 48.4398 0 7.17489 11.3475 34.7032 6.28931 10.6667 18.8341 18.6667 11.6732 32.2581 62.1715 21.9895 42.4044 5.59441 30.1639 36.4465 52.3732 17.6166 26.1097 13.7931 35.2442 0 2.7972 43.7299 38.1375 44.4906 37.467 23.7398 15.3034 41.7391 0 38.8385 48.8889 34.2142 5.71429 5.06329 0 27.5335 50.5387 48.1842 44.489 41.3483 59.2908 2.29885 32.5167 36.5688 12.2605 3.95257 0 18.5759 14.5215 19.6721 29.3255 24.4373 29.1939 10.6667 11.3821 38.1201 18.3727 17.5824 2.0202 38.3961 4.81928 32.5733 29.9776 0 52.6603 65.3636 4.78469 14.902 29.2398 14.0351 37.678 52.6316 23.4273 8.81057 35.3553 0 24.8963 10.1695 0 22.8438 8.80503 18.1818 18.7867 3.61991 29.9029 11.6129 27.0042 7.32265 0 0 23.6453 3.14961 21.8688 3.36134 35.9673 8.76494 5.49828 29.912 17.284 14.8492 5.47945 32.2981 28.483 41.1215 42.9799 47.9532 7.73481 21.4464 18.1818 1.30719 14.0541 27.2446 28.9331 40.4494 21.2389 39.698 62.3377 19.5276 3.83142 30.9677 28.3828 22.1453 21.9839 46.1538 26.3248 16.73 30.9133 10.4046 53.9683 31.793 24.8848 17.3913 20.5323 45.2592 7.51174 12.3348 2.33918 44.114 25.7669 42.2164 9.17431 21.7195 24.9135 58.0468 19.4986 31.2076 21.5054 10.8401 50.1873 25.5875 50.4597 14.6868 39.0456 28.2454 33.5124 45.6948 2.67559 27.0866 17.4905 32.1755 9.90099 30.127 0 12.3077 2.5974 10.3321 52.3745 46.4223 22.9829 12.3636 16.7053 34.5631 48.2676 57.4822 21.8263 19.0045 7.20721 19.8347 9.27152 36.5217 36.0494 27.4025 34.955 26.8041 62.2222 1.45985 32.19 19.5745 1.9802 27.8481 40.1914 16.1172 17.2702 35.4191 0 18.5075 43.7018 49.4041 45.2088 38.0715 0 18.3727 34.7253 10.9015 29.2958 13.2159 4.21053 10.5541 55.814 7.58017 19.1419 18.1818 22.7979 22.4561 12.3552 59.5349 15.4589 1.1976 33.4165 34.6041 1.8018 0 6.48649 17.6591 66.3296 35.6775 6.58436 11.7302 44.124 29.6651 24.7423 51.7837 21.2014 0 11.8227 17.3913 6.83761 37.0149 9.41176 51.7781 11.6992 23.0179 49.9336 0 18.3299 34.8745 43.7628 3.77358 43.6742 18.4615 9.44206 34.5497 17.4573 30.1703 47.7971 38.4 11.6208 17.3913 42.8829 4.5977 38.1877 20.4778 39.4667 15.8845 36.4835 35.5231 26.2295 35.0617 2.36686 6.4257 43.342 15.7182 30.7692 27.5304 55.7533 30.2772 55.6575 36.6265 1.43885 27.2611 31.1656 76.5984 63.7264 60.6335 41.7845 32.6214 19.1214 31.4754 38.4473 19.4932 40.3893 28.4866 15.7576 26.9663 3.27869 5.91716 12.6394 48.7874 43.3121 54.429 0 11.8721 12.9964 13.5922 15.9021 14.2132 45.9344 3.6036 25.9681 14.6597 16.5517 6.469 15.3425 16.0772 0 50.7021 24.1901 9.04523 18.9831 16.9697 50.237 8.51064 46.4957 38.171 10.4803 24.2588 17.0316 24.6305 30.3237 4.51977 12.9032 48.3221 34.8432 34.7475 36.8715 33.2425 12.987 34.4828 18.1818 46.9713 31.4961 25.5892 2.73973 41.2797 33.887 11.6959 6.60066 14.0221 33.1066 19.0931 20.4986 41.9731 22.2222 35.7257 37.0119 6.34921 59.6394 23.6934 19.2469 11.8681 37.1084 3.38983 11.1888 10.596 53.2189 59.0397 23.8569 23.2365 0 13.2565 6.68896 25.2465 16.1039 19.6995 36.3636 46.8085 18.7919 28.8525 20.2643 4.5977 5.84795 16.1172 24.4365 36.3636 24.8804 22.6804 40.7002 26.2366 0 1.68067 47.6621 23.219 33.4661 47.6386 26.7229 33.6261 35.9281 21.6514 32.9897 0 25.0765 67.1935 31.4754 31.1688 48.9684 1.0929 12.6394 20.1342 30.1189 32.6118 11.0345 11.215 37.7029 6.93642 14.425 3.6036 0 36.9077 25.8206 9.26641 25.8462 26.7465 40.7747 14.1593 35.5231 26.0692 40.8946 46.8508 42.7095 58.8523 34.8083 44.7818 23.6504 35.9202 2.01005 52.1817 18.9415 44.7761 56.7901 4.25532 28.2686 22.3602 14.0162 17.2911 0 0 3.38983 16.6038 9.63855 5.01792 13.9535 28.754 20.331 0 3.92157 22.0532 16.0428 35.3982 28.5078 2.7972 52.5097 0 18.6916 0 30.5023 44.8133 35.6108 19.3548 27.5742 36.3636 26.9767 34.8697 17.4274 7.3922 5.03432 0 11.8203 10.7884 34.3053 11.3208 52.4272 20.1511 0 65.158 11.4114 0 53.7576 45.6499 52.8 4.41989 45.614 7.59494 28.0802 27.5229 34.7197 10.101 25.2427 37.3206 0 16.8675 17.3776 21.8905 46.1046 48.7644 40.2746 4.37956 0 0 30.3263 50.2994 34.3387 16.1543 19.0114 27.5618 6.59341 12.9278 48.2759 9.60961 24.2424 32.0675 4.65116 21.6906 40.2957 65.2424 18.5351 36.8485 15.8491 6.80272 34.56 28.1407 37.8063 19.4726 20.3091 5.7554 2.40964 31.6911 1.45985 17.9104 27.2 35.141 22.1662 38.0665 9.5672 10.1961 12.5984 34.4569 41.7501 39.1608 41.3483 13.9942 48.1696 56.1095 15.5932 21.118 34.2169 29.106 5.64706 22.1729 10.9422 0 4.44444 30.5085 30.4527 0 22.5914 22.0736 44.0678 43.9854 33.6673 11.9403 0 15.7303 38.1592 23.8806 23.2558 0 41.8451 44.2284 24.7934 0 9.95475 32.0917 2.46914 45.3739 0 46.9854 53.997 62.9174 23.2019 51.2456 20.9607 44.8485 25.0871 8.98876 0 8.21918 23.2044 9.90099 0 63.9132 13.6126 14.359 23.5012 3.30579 24.6869 5.09091 25.8065 35.0305 51.6934 11.5486 40.2044 12.766 26.9058 26.6667 3.27869 27.2931 37.9747 37.2007 2.28833 16.8675 13.2565 44.4084 20.339 22.3938 34.1297 0 16.3009 11.2186 39.2804 6.76923 51.7277 36.8932 43.3295 1.90476 56.7555 47.9062 50.4692 0 60.2496 7.05882 0 5.84795 9.02256 0 38.4416 19.2593 39.9299 14.1876 6.06061 32.1127 24.9697 43.5798 16.835 43.9586 33.2344 12.7796 56.9857 60.9665 0 16.7331 10.6509 48.5437 34.2857 20.438 37.5215 11.7302 21.1874 5.59441 8.69565 10.6464 2.73973 32.7684 14.0575 13.2597 28.5714 32.9803 0 48.9723 48.2424 25.2229 17.8694 19.9357 34.1598 49.8237 43.5644 20.8 24.1611 3.80952 10.1617 3.53982 30.2376 15.1436 57.9041 33.2203 17.0088 33.8369 47.4547 57.8387 53.7074 52.0646 33.0042 0 39.857 21.8341 31.0406 40.2464 41.048 43.2941 17.5711 22.4176 6.92641 5.03145 42.6735 12.3711 40.1949 6.10687 14.9068 10.0457 1.47601 1.10497 16.2413 28.9362 31.8059 3.73832 26.847 65.8773 1.6 6.21118 ]

LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:406) AvgLoss: 2.656 (Xent), [AvgXent: 2.656, AvgTargetEnt: 0]
progress: []
FRAME_ACCURACY >> 40.838% <<

