nsclab-gpu
nnet-train-frmshuff --cross-validate=true --randomize=false --verbose=0 --minibatch-size=256 --randomizer-size=32768 --randomizer-seed=777 --feature-transform=exp/dnn4_pretrain-dbn_dnn/final.feature_transform 'ark:copy-feats scp:exp/dnn4_pretrain-dbn_dnn/cv.scp ark:- |' 'ark:ali-to-pdf exp/tri3_ali/final.mdl "ark:gunzip -c exp/tri3_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- |' exp/dnn4_pretrain-dbn_dnn/nnet_dbn_dnn.init 
WARNING (nnet-train-frmshuff[5.5.294-06484]:SelectGpuId():cu-device.cc:221) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:349) Selecting from 2 GPUs
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:364) cudaSetDevice(0): TITAN Xp	free:12023M, used:173M, total:12196M, free/total:0.985815
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:364) cudaSetDevice(1): TITAN Xp	free:11888M, used:304M, total:12192M, free/total:0.975032
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:411) Trying to select device: 0 (automatically), mem_ratio: 0.985815
LOG (nnet-train-frmshuff[5.5.294-06484]:SelectGpuIdAuto():cu-device.cc:430) Success selecting device 0 free mem ratio: 0.985815
LOG (nnet-train-frmshuff[5.5.294-06484]:FinalizeActiveGpu():cu-device.cc:284) The active GPU is [0]: TITAN Xp	free:11957M, used:239M, total:12196M, free/total:0.980403 version 6.1
copy-feats scp:exp/dnn4_pretrain-dbn_dnn/cv.scp ark:- 
LOG (nnet-train-frmshuff[5.5.294-06484]:Init():nnet-randomizer.cc:32) Seeding by srand with : 777
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:163) CROSS-VALIDATION STARTED
ali-to-post ark:- ark:- 
ali-to-pdf exp/tri3_ali/final.mdl 'ark:gunzip -c exp/tri3_ali/ali.*.gz |' ark:- 
LOG (ali-to-pdf[5.5.294-06484]:main():ali-to-pdf.cc:68) Converted 1232 alignments to pdf sequences.
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:354) ### After 0 frames,
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:355) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -5.55469, max 5.62657, mean 0.0284201, stddev 1.02852, skewness 0.165764, kurtosis 1.87554 ) 
[1] output of <AffineTransform> ( min -19.5095, max 11.1738, mean -3.18688, stddev 2.42036, skewness 0.0899909, kurtosis 1.9239 ) 
[2] output of <Sigmoid> ( min 3.36599e-09, max 0.999986, mean 0.135318, stddev 0.225692, skewness 2.29326, kurtosis 4.5301 ) 
[3] output of <AffineTransform> ( min -27.6438, max 14.671, mean -3.63402, stddev 2.43802, skewness 0.0526152, kurtosis 3.04288 ) 
[4] output of <Sigmoid> ( min 9.87255e-13, max 1, mean 0.100174, stddev 0.19095, skewness 3.00661, kurtosis 8.98713 ) 
[5] output of <AffineTransform> ( min -14.6347, max 11.9031, mean -3.41426, stddev 2.04313, skewness 0.770303, kurtosis 2.58622 ) 
[6] output of <Sigmoid> ( min 4.40782e-07, max 0.999993, mean 0.0991913, stddev 0.191272, skewness 3.06813, kurtosis 9.29627 ) 
[7] output of <AffineTransform> ( min -24.7681, max 15.0543, mean -3.66402, stddev 2.07463, skewness 0.777725, kurtosis 4.69726 ) 
[8] output of <Sigmoid> ( min 1.7512e-11, max 1, mean 0.083892, stddev 0.180425, skewness 3.47263, kurtosis 12.0825 ) 
[9] output of <AffineTransform> ( min -14.0879, max 10.826, mean -3.75573, stddev 1.89634, skewness 1.65172, kurtosis 5.46733 ) 
[10] output of <Sigmoid> ( min 7.61588e-07, max 0.99998, mean 0.0770531, stddev 0.185865, skewness 3.60672, kurtosis 12.6283 ) 
[11] output of <AffineTransform> ( min -25.7688, max 15.0842, mean -3.89112, stddev 1.94803, skewness 1.36469, kurtosis 7.50824 ) 
[12] output of <Sigmoid> ( min 6.438e-12, max 1, mean 0.0685828, stddev 0.174642, skewness 3.95863, kurtosis 15.4864 ) 
[13] output of <AffineTransform> ( min -3.13877, max 3.30089, mean -0.00528803, stddev 0.591972, skewness 0.0176942, kurtosis 0.329142 ) 
[14] output of <Softmax> ( min 2.20653e-05, max 0.0143427, mean 0.000657896, stddev 0.000433617, skewness 2.73203, kurtosis 18.8093 ) 
### END FORWARD

LOG (copy-feats[5.5.294-06484]:main():copy-feats.cc:143) Copied 120 feature matrices.
LOG (ali-to-post[5.5.294-06484]:main():ali-to-post.cc:73) Converted 1232 alignments.
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:384) ### After 37632 frames,
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:385) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -6.99253, max 6.66255, mean -0.0182415, stddev 1.07086, skewness 0.150785, kurtosis 2.22078 ) 
[1] output of <AffineTransform> ( min -22.4899, max 11.4717, mean -3.19786, stddev 2.49728, skewness 0.1522, kurtosis 1.53222 ) 
[2] output of <Sigmoid> ( min 1.70915e-10, max 0.99999, mean 0.141444, stddev 0.236291, skewness 2.17454, kurtosis 3.81937 ) 
[3] output of <AffineTransform> ( min -24.6885, max 13.8813, mean -3.67232, stddev 2.51964, skewness 0.0133207, kurtosis 2.32115 ) 
[4] output of <Sigmoid> ( min 1.89627e-11, max 0.999999, mean 0.104443, stddev 0.198093, skewness 2.86073, kurtosis 7.93866 ) 
[5] output of <AffineTransform> ( min -15.0584, max 10.0289, mean -3.45346, stddev 2.10891, skewness 0.744903, kurtosis 2.12474 ) 
[6] output of <Sigmoid> ( min 2.88553e-07, max 0.999956, mean 0.102454, stddev 0.19823, skewness 2.92484, kurtosis 8.22575 ) 
[7] output of <AffineTransform> ( min -19.6248, max 13.7519, mean -3.6861, stddev 2.13247, skewness 0.704534, kurtosis 3.7127 ) 
[8] output of <Sigmoid> ( min 2.99969e-09, max 0.999999, mean 0.0872628, stddev 0.186184, skewness 3.31628, kurtosis 10.8595 ) 
[9] output of <AffineTransform> ( min -14.4946, max 10.4086, mean -3.78267, stddev 1.93874, skewness 1.57975, kurtosis 4.88317 ) 
[10] output of <Sigmoid> ( min 5.07083e-07, max 0.99997, mean 0.0792538, stddev 0.189793, skewness 3.47378, kurtosis 11.5988 ) 
[11] output of <AffineTransform> ( min -19.7193, max 13.4074, mean -3.90666, stddev 1.982, skewness 1.27348, kurtosis 6.29958 ) 
[12] output of <Sigmoid> ( min 2.72911e-09, max 0.999999, mean 0.070977, stddev 0.178411, skewness 3.80343, kurtosis 14.2181 ) 
[13] output of <AffineTransform> ( min -3.5365, max 2.99471, mean -0.00588537, stddev 0.606163, skewness 0.0124178, kurtosis 0.147214 ) 
[14] output of <Softmax> ( min 1.51979e-05, max 0.010338, mean 0.000657894, stddev 0.000441611, skewness 2.44438, kurtosis 12.5079 ) 
### END FORWARD

LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:395) Done 120 files, 0 with no tgt_mats, 0 with other errors. [CROSS-VALIDATION, NOT-RANDOMIZED, 0.00752409 min, processing 83359 frames per sec; i/o time 24.2393%]
LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:405) PER-CLASS PERFORMANCE:
@@@ Frames per-class: [ 2079 9 10 3 20 0 15 49 19 73 34 83 6 21 29 19 24 39 22 27 28 28 31 36 14 8 24 5 15 34 14 33 16 0 60 32 7 34 48 25 2 27 22 18 18 8 107 5 558 1916 138 47 43 0 53 27 0 19 35 39 30 0 2 43 22 12 67 0 6 37 52 46 80 64 83 13 109 32 47 8 2 3 57 24 17 26 12 15 30 24 7 31 0 15 0 31 43 46 19 40 8 16 5 19 15 130 11 16 21 13 6 16 0 2 0 18 10 16 15 50 7 22 9 4 0 17 12 18 11 4 3 48 30 56 13 15 27 55 6 32 10 30 32 46 6 0 42 27 44 14 19 0 90 53 29 18 9 18 45 12 5 26 0 4 10 6 16 12 21 54 0 0 20 14 59 16 19 24 8 0 0 16 23 25 0 29 16 35 16 17 30 23 76 9 25 19 0 7 21 24 15 63 5 0 24 6 43 17 20 6 0 6 40 65 27 4 26 25 20 29 35 48 18 21 27 14 27 21 22 0 0 53 28 18 0 15 7 51 2 15 12 46 40 0 21 0 0 26 10 25 45 19 17 19 12 36 5 21 20 27 21 19 19 14 0 62 20 30 0 19 0 19 30 13 16 14 54 12 33 23 10 9 19 18 3 9 30 14 0 13 37 4 17 16 31 10 26 14 8 15 19 10 25 6 8 17 17 6 26 4 9 57 10 21 4 13 11 17 10 20 38 39 28 16 13 16 78 21 13 4 7 20 10 4 22 20 5 37 11 7 21 17 12 30 32 13 23 17 45 34 4 10 25 14 12 1 36 63 17 20 2 67 38 18 31 21 25 16 8 14 23 11 28 31 19 11 17 8 45 0 32 11 34 17 35 18 20 0 26 68 18 0 9 17 13 26 0 7 27 8 39 18 80 56 26 12 10 16 14 5 9 0 21 58 21 49 0 8 14 28 25 12 21 2 10 23 19 20 0 25 11 40 23 26 20 9 8 11 16 19 46 68 1 8 17 12 19 2 6 26 20 0 26 0 3 23 13 0 2 15 4 28 10 42 2 17 6 9 0 24 12 14 15 0 5 53 8 10 2 41 17 5 7 39 9 17 29 15 14 0 37 12 58 18 7 10 16 20 15 0 34 21 25 27 10 11 24 23 8 32 11 9 30 12 0 0 9 6 17 0 12 13 42 25 17 30 13 13 49 4 13 35 23 38 3 54 12 6 46 26 1 34 24 41 15 11 0 26 25 34 25 9 37 75 23 25 0 7 21 23 29 0 12 28 17 0 10 5 16 18 3 11 31 9 17 13 51 21 36 39 51 33 0 12 31 28 22 9 8 24 23 41 21 39 0 10 6 18 11 1 23 24 29 5 6 9 19 31 28 32 0 18 4 18 5 10 6 5 40 7 0 23 17 17 8 15 2 29 6 19 31 13 12 11 60 13 58 17 3 18 13 147 18 24 16 20 0 29 8 10 9 44 0 9 17 54 25 15 24 37 0 6 24 32 48 26 18 10 13 10 24 12 0 13 0 0 9 12 13 28 31 9 14 12 9 0 19 4 22 0 46 0 38 15 20 27 6 12 6 5 5 33 21 18 16 29 8 17 13 20 7 12 8 14 19 17 14 35 0 25 5 53 22 16 25 45 17 57 0 8 49 17 13 18 0 16 16 29 13 11 5 65 11 14 15 18 12 36 53 18 19 62 0 24 38 11 6 96 25 17 10 164 5 28 0 66 3 19 13 3 6 39 14 0 77 19 11 6 34 1 7 26 3 5 56 24 31 15 25 37 0 11 9 11 8 7 5 42 29 15 120 13 60 41 18 35 71 12 21 5 32 0 9 1 9 36 20 35 35 87 0 27 26 25 7 8 0 27 31 64 23 8 43 5 20 29 25 11 0 20 12 0 11 10 24 9 0 18 32 8 7 42 0 6 35 8 30 70 16 47 8 11 57 43 16 8 16 14 3 0 0 22 0 16 22 4 43 3 8 17 0 1 34 14 18 0 30 13 8 16 26 42 16 25 32 51 17 4 3 18 0 7 16 20 33 9 2 51 55 28 0 0 14 19 18 14 38 21 31 0 24 41 19 14 0 38 7 11 14 32 6 14 0 16 18 25 20 45 12 24 52 17 55 38 36 38 42 59 1 31 9 22 7 19 0 13 3 23 45 2 21 7 23 13 65 5 26 18 5 16 4 19 17 44 40 22 29 10 19 8 0 35 35 15 20 21 0 9 14 51 26 81 0 19 29 32 25 19 0 6 59 5 17 30 16 18 8 45 11 12 9 13 0 0 15 30 50 26 18 11 63 20 5 43 18 0 5 10 0 24 0 58 27 25 39 14 20 39 26 23 18 15 7 94 13 15 26 23 20 5 23 3 22 10 28 26 10 10 18 29 14 22 42 29 20 14 41 21 30 17 6 54 29 70 57 31 54 21 1 9 31 18 21 25 4 0 0 0 18 42 44 51 5 28 12 0 9 8 24 0 26 16 31 0 9 9 8 58 21 7 31 4 40 6 51 18 11 24 29 9 25 27 14 81 8 26 4 32 36 16 21 37 9 12 1 60 23 29 7 4 16 13 22 57 13 36 32 4 46 6 32 20 22 4 17 21 54 57 38 31 0 14 26 34 28 31 24 28 6 16 9 0 12 2 78 0 12 26 26 25 0 6 30 27 3 5 83 29 41 9 8 18 36 200 15 20 27 51 17 1 36 47 25 17 34 13 25 1 0 50 26 24 40 19 31 0 11 8 16 47 65 49 21 36 18 20 7 83 12 32 44 16 24 1 33 17 0 0 0 11 26 11 13 11 19 0 5 40 9 36 25 4 31 12 38 4 54 5 21 0 29 7 69 40 11 19 34 11 30 36 31 20 15 15 0 83 22 0 85 50 49 0 28 56 17 16 28 22 20 14 13 14 60 4 49 35 22 4 10 0 31 46 17 80 21 14 7 8 33 10 14 11 11 26 34 125 33 58 35 5 47 38 39 29 40 0 5 22 0 23 0 13 12 16 15 19 21 22 117 49 6 15 19 88 30 10 19 22 32 30 4 0 38 9 19 15 16 4 20 56 40 0 3 4 67 20 26 6 37 33 13 0 13 20 7 28 1 21 14 9 14 40 19 22 52 7 5 31 4 0 6 123 3 4 51 11 28 13 2 17 44 19 35 2 29 24 11 18 36 16 23 9 23 22 6 19 0 0 1 15 60 21 57 17 46 9 58 13 73 15 69 30 5 6 2 0 25 0 22 29 26 22 16 31 2 40 22 19 35 39 0 11 2 29 51 16 18 20 47 9 12 20 8 10 17 16 18 34 0 52 51 32 14 19 50 37 23 35 2 15 14 15 27 9 17 18 43 17 74 54 17 41 44 10 60 5 35 41 52 24 22 17 12 10 28 15 38 3 5 5 36 13 24 9 20 13 60 43 6 4 ]
@@@ Loss per-class: [ 7.65563 7.14543 6.69395 6.00026 6.91935 0 7.22603 7.56924 7.39015 7.91037 7.88781 7.08074 7.46713 6.90788 7.14186 6.74943 7.5732 8.07115 7.71967 7.47701 7.5138 7.80703 7.63713 7.27384 7.48109 7.10718 6.44499 6.91543 7.48908 7.40344 6.76184 7.42727 7.01027 0 7.4317 7.31658 7.87194 7.2876 7.00756 7.32023 6.16334 7.18834 8.02883 7.5841 7.04265 6.90029 7.31157 6.417 7.5191 7.18987 7.52277 7.08843 7.33649 0 7.77054 6.73768 0 7.32799 7.02473 6.80216 6.8904 0 6.34331 7.02906 7.78865 7.26572 7.06604 0 6.02085 6.91815 7.39714 7.72688 7.70077 6.92949 7.15501 7.86474 6.74308 7.14754 7.28515 8.00082 6.26306 6.11225 6.78346 7.92152 7.95861 6.97572 7.43414 7.12706 7.35924 6.20308 8.07791 7.49135 0 7.65601 0 7.97485 7.30431 7.27725 6.60093 7.62615 7.05291 6.73535 7.16893 7.51963 7.65129 7.37286 7.15998 7.8672 8.05417 6.89432 6.91007 7.23083 0 6.05659 0 7.13689 7.6011 6.22889 7.64862 7.78504 7.55359 7.1371 7.01138 6.2753 0 7.71167 7.65841 7.32829 6.78826 6.57427 7.13855 7.99638 7.04115 7.63694 6.37846 6.76134 7.27003 6.71311 7.28965 7.05913 8.39344 7.26982 6.92962 8.30361 6.92894 0 6.67242 8.19446 7.47882 7.51171 6.71996 0 7.63317 7.13782 7.4355 7.83556 6.37786 7.12841 7.30241 6.82515 7.0413 7.50866 0 6.51137 7.05053 7.22787 7.34551 7.07984 7.1407 7.04481 0 0 6.68328 7.42494 8.11762 6.85381 8.22017 8.24068 7.56553 0 0 7.82799 6.63045 7.3379 0 7.73289 7.33676 6.37874 7.01667 6.97345 7.38359 7.53654 7.33915 7.77106 7.22758 7.21272 0 6.44086 7.38829 7.24125 6.68902 7.63389 6.43349 0 7.82095 6.58981 7.32278 7.1674 7.47778 7.91902 0 7.7164 6.94356 7.13494 7.03007 6.0289 7.68008 7.74704 7.96711 7.54946 7.35698 8.13713 7.54083 7.33131 6.75636 8.20566 7.8553 7.34344 7.33144 0 0 7.59486 7.11302 6.4588 0 6.90878 7.02406 8.08162 6.30785 8.08413 6.85139 7.39609 6.75979 0 7.06654 0 0 7.7969 6.61309 7.54754 8.67986 7.22836 7.12228 7.55802 7.11992 7.70526 6.91732 7.50408 8.0074 7.32945 6.69592 7.47192 7.8338 7.2299 0 7.65573 7.07334 7.7633 0 7.72336 0 6.43508 7.66353 8.12434 7.38752 7.34259 7.49546 7.29319 7.11615 6.54884 7.63465 7.72293 8.20531 7.18339 6.32346 6.4044 6.47627 7.79282 0 7.67346 7.59512 6.33187 7.57653 6.93109 7.48091 7.10182 7.53594 7.12586 7.06274 8.07832 7.15096 7.47797 7.36947 7.28368 8.10726 7.27495 7.27527 7.28869 6.79791 6.32318 7.33832 7.37261 6.85495 6.69046 6.13416 7.34949 6.79271 7.32856 7.44793 7.76553 7.47938 7.38425 7.79905 7.37457 6.71947 6.98962 7.67352 7.32408 7.08588 5.7637 6.99141 8.3247 6.84575 6.81719 7.39779 7.45154 7.19954 7.42629 7.02374 7.20776 7.87291 6.63906 7.15561 6.97905 7.32186 7.87328 7.03597 6.6415 7.88978 7.65518 7.01891 7.41556 7.08285 8.05715 8.06255 4.59525 6.80724 7.86777 7.4118 6.73856 6.62941 7.75737 7.23312 6.18327 7.72977 6.84678 7.83338 7.40632 6.37927 7.48651 7.10475 6.25656 6.88381 7.11715 6.82964 7.11647 8.25026 7.58373 8.12531 0 7.44893 6.86171 7.24384 7.33349 7.3044 8.20763 6.88634 0 7.14449 6.96834 7.58794 0 6.84521 6.92661 7.66101 6.89119 0 6.56748 7.05222 7.37317 6.99018 6.62022 7.6539 7.49294 6.97263 7.65463 6.87614 7.20414 6.79063 6.28683 7.72993 0 7.42729 7.4787 6.73494 7.20352 0 7.20006 7.56857 7.57035 6.86442 7.57236 6.84469 6.46988 7.35836 7.36846 7.42213 7.0864 0 8.12004 7.08118 7.03981 7.88319 6.83949 7.16342 7.48138 7.13877 7.0471 7.73645 8.38974 7.23409 7.95743 4.82559 7.09941 7.44439 6.97423 8.27401 6.7004 6.68458 6.65817 7.98953 0 6.78904 0 6.01634 7.50014 6.30512 0 6.89351 6.33495 6.49564 7.46821 7.36483 7.40155 5.34807 6.91882 7.38597 7.55193 0 7.22214 7.03183 7.33009 7.43803 0 6.3816 7.34384 7.09882 7.05945 6.66414 6.58405 8.08767 6.1603 7.19235 7.04636 6.35286 7.99406 7.46942 7.17266 7.58883 0 7.45682 7.09737 6.52878 7.03306 7.28717 7.06868 7.77811 6.96461 6.88707 0 7.36496 7.58952 7.89752 7.37099 7.42826 7.04021 7.50469 7.66257 7.02934 7.78088 6.86231 7.14808 6.91559 6.66486 0 0 6.81367 6.14805 7.33789 0 7.01462 7.85177 7.61318 8.31141 7.24946 7.20229 7.01355 6.9691 8.0152 5.98398 7.1876 7.30718 7.28362 7.24314 6.12612 7.26421 7.34083 6.1624 7.22917 7.02477 5.24864 7.208 7.10687 8.09657 7.36488 7.10838 0 7.36219 7.48714 6.08406 7.67574 7.38831 7.16028 7.3792 7.37533 7.07909 0 6.87135 7.42917 7.20209 7.33451 0 7.25606 7.21709 8.55013 0 7.49055 6.87364 6.56291 7.48993 5.96697 7.95543 7.10082 6.84325 7.0357 6.98442 8.18927 6.90512 7.17875 6.55925 7.3865 7.44774 0 7.51301 7.32355 7.60216 6.94076 6.41972 7.51585 6.73113 7.44891 7.98748 7.21637 7.28941 0 7.41656 8.42865 7.21376 7.0268 4.94604 6.20341 6.90441 7.06284 6.98183 6.70709 7.55718 6.31333 7.01336 6.92538 7.19392 0 8.01742 6.25652 7.0933 6.26452 6.66476 7.20777 6.67263 6.75921 6.94608 0 7.39894 7.65776 6.43229 6.45771 7.38583 6.11203 7.23668 6.932 8.03002 7.85822 7.94729 7.82146 7.40055 6.99766 7.0963 7.65788 6.54188 6.70864 7.36539 7.57685 7.62979 7.04407 8.01933 7.46621 7.4787 0 7.89917 6.69402 7.95829 7.08482 8.09253 0 7.52174 8.11863 6.72531 7.97094 7.26059 6.53374 6.96436 0 6.90954 6.47617 7.58866 8.13782 7.15098 7.84835 7.54193 7.53163 7.16697 7.29939 6.90391 0 7.36411 0 0 7.57012 6.51368 7.86878 7.16122 7.56517 7.16809 7.30706 7.6295 7.16417 0 7.42798 7.91315 7.07313 0 7.63831 0 7.44344 8.03047 6.82063 7.8324 6.7086 7.38378 7.4191 7.16875 6.62812 7.15357 7.05335 6.86665 7.45109 8.32479 7.15549 7.98494 6.75111 6.82872 7.15148 6.75413 7.79437 7.08504 6.9023 7.1583 7.1094 7.168 0 7.46782 7.46861 7.74199 6.75609 6.88029 7.40427 7.93938 7.95784 7.30596 0 6.83988 7.62664 7.72635 7.45185 7.71495 0 7.25286 7.31971 7.11126 8.22181 7.5826 6.63624 8.08046 6.91529 6.83773 7.127 7.44467 7.11238 7.21688 6.11512 7.29245 6.65179 7.78429 0 7.52895 8.12218 7.82015 7.71544 7.79728 6.9853 6.92714 7.08835 7.36393 7.19676 6.96013 0 7.28699 6.364 6.84682 7.1462 6.71592 6.85078 7.0069 6.83601 0 7.60573 6.88486 6.9716 7.24244 7.03125 5.26013 7.19786 7.24059 7.01857 7.5302 7.16414 7.63575 7.63152 7.36016 7.70097 7.35804 0 7.58378 7.9211 7.43294 7.33953 6.82723 6.07781 7.42496 8.04629 7.22575 7.82027 7.51334 7.33489 7.49679 7.75712 7.27524 7.56946 7.01576 6.91441 6.6851 7.38106 0 6.84502 4.65043 7.37936 6.99896 7.07776 6.80947 7.29073 7.33247 0 7.28772 7.10893 7.476 8.00637 6.50441 0 8.01235 6.87685 7.03548 7.56725 6.42623 7.87875 6.52538 8.0414 8.26934 7.07204 7.01345 0 7.26623 6.89876 0 7.59274 7.96852 7.82875 7.10988 0 7.71776 7.31355 7.37231 6.98326 6.8939 0 7.22539 7.22523 6.67078 6.87662 7.32458 7.01509 6.51029 6.81421 6.88139 7.35076 7.56623 7.62722 7.10401 8.47717 7.98604 6.23832 0 0 7.35622 0 6.74638 6.92838 7.30541 7.55127 5.68366 6.62971 7.28111 0 4.65911 8.43354 7.23253 7.87434 0 7.82738 7.08836 6.68556 6.69852 7.07928 6.87726 6.88929 7.16093 7.28209 7.26569 7.65918 6.44285 5.8725 7.14552 0 7.69685 6.89466 7.25194 6.73595 7.01867 5.19143 7.15251 7.70587 7.12336 0 0 7.35659 7.56589 7.74117 6.66715 7.01138 7.68701 7.21479 0 6.96047 7.69519 6.99133 6.80025 0 7.36062 7.44464 7.35453 7.20696 7.47678 6.73378 7.29503 0 7.52918 7.06026 8.01103 7.12856 7.74192 7.5857 8.26561 6.7548 6.67436 8.1465 7.42205 6.89035 7.271 6.34209 6.54628 5.21909 7.4292 6.20826 7.55908 7.39944 6.97965 0 7.29553 6.01664 7.21031 7.34067 5.82453 7.57052 6.71721 7.32308 6.8063 7.19797 7.10757 6.87302 7.47431 7.08539 7.87598 6.29242 7.30164 7.55476 7.16284 7.11445 7.25578 7.34738 6.73518 7.00309 6.5276 0 6.93974 7.7665 7.13535 7.50052 7.70295 0 6.88153 7.28886 7.08478 6.82193 7.46615 0 7.31078 7.22011 7.95028 7.42885 7.62979 0 6.82132 7.80887 6.652 7.13302 7.45465 8.28368 7.59329 6.91621 6.45156 7.30393 7.31815 6.94822 7.67001 0 0 6.18343 7.73988 7.68753 7.6627 6.86387 6.88553 7.03742 7.19323 6.16398 7.35644 7.38829 0 6.17391 6.82563 0 6.85919 0 7.28568 7.19015 8.06094 7.46268 8.51226 6.6798 8.16821 6.77738 7.21541 7.36069 7.60524 6.98134 6.79799 7.03688 7.42228 8.12836 7.19388 6.90136 6.889 7.55105 7.07647 8.54676 7.25333 8.00589 7.62111 6.93372 6.43274 7.58532 7.9244 7.09554 7.40079 7.36308 7.13352 6.29694 7.14763 7.53057 7.34416 7.26657 6.8795 7.75394 6.89347 6.62685 6.54099 7.21442 7.11141 7.83889 7.23776 5.2029 6.48348 7.45084 6.94345 6.86942 7.15556 7.179 0 0 0 7.14401 7.16835 7.06211 7.17068 6.52063 7.52663 6.89853 0 7.27156 7.04948 8.32343 0 7.62816 8.41152 6.46175 0 7.06765 7.19401 6.78553 6.81699 7.3292 7.13767 7.51231 6.67626 7.44065 6.56476 7.61236 7.7393 7.31582 7.75407 7.50803 6.91599 7.50942 6.94008 7.21639 6.83988 7.23487 6.89343 6.88531 6.83028 8.19554 7.60116 7.42488 7.6827 6.67777 6.94877 5.61316 7.24043 7.25876 6.94123 7.01161 5.90961 7.37774 7.32842 8.12032 7.42515 6.98946 7.93214 7.18186 6.06398 7.97265 7.57239 7.66613 7.23907 6.62202 7.08573 8.05907 8.93012 6.80472 7.47347 7.35077 7.48826 0 7.09446 7.32529 6.96393 7.38218 7.79851 8.72159 7.6883 6.00331 7.72999 6.49639 0 7.45615 6.91264 7.14217 0 6.94385 7.77545 7.46647 7.43205 0 6.66033 6.45369 7.5611 6.56222 6.94499 6.84874 7.17838 8.61366 6.38841 6.91038 7.55946 7.2514 7.91147 6.83364 7.69893 7.4708 7.14552 7.05919 5.23642 7.03777 8.04573 8.12814 8.16379 7.12837 7.07017 7.07512 4.82268 0 7.64312 7.06049 8.16332 7.58883 7.58233 6.86502 0 7.54309 7.4898 7.54798 7.54804 8.42713 7.21072 7.87094 7.02923 8.25193 7.31898 6.25209 7.31735 6.91754 8.5743 8.01456 7.96121 6.71217 4.53493 7.76149 7.01544 0 0 0 7.19586 7.21644 7.37132 7.67034 7.56918 7.3745 0 6.91928 6.55881 7.82373 7.06385 6.82399 7.02678 7.84968 6.98108 7.06781 7.22025 7.37255 5.88286 7.15589 0 6.78844 7.75604 7.64787 7.29435 7.29986 7.60412 7.86944 6.93262 7.02921 7.65562 7.48574 7.17795 7.89899 7.5886 0 7.13071 7.6729 0 7.3834 7.43166 8.29058 0 6.89036 6.81202 8.481 6.7861 6.70873 6.73834 7.37788 7.49453 7.40871 6.93708 7.63954 6.67213 7.51027 7.82845 7.44389 7.56747 7.53197 0 8.63635 7.88681 6.93529 7.47924 7.11619 6.68379 6.85754 7.09998 7.44662 6.59574 7.71322 6.71601 8.04329 7.40706 7.97306 7.64777 7.93916 7.26391 7.72083 6.43565 7.60734 7.6663 7.30906 7.73116 6.95561 0 6.8575 7.05416 0 7.13153 0 7.69311 6.85 6.44295 8.04389 7.36142 7.36147 7.36285 8.18664 7.84256 6.73118 6.73612 6.95253 8.28989 7.96894 6.95198 7.72061 7.67545 7.61932 7.44438 5.4592 0 7.62439 6.7893 7.61767 6.60373 7.31689 6.53025 8.22371 8.00053 6.90825 0 6.42378 5.87428 7.8381 7.13108 6.72398 6.71004 7.17413 7.31308 6.36822 0 7.63051 6.7533 5.63373 7.14141 5.16301 7.21769 7.81997 6.64331 7.57352 7.52715 7.79818 7.18507 8.16941 6.94736 6.83165 7.37314 7.07052 0 6.44033 7.22952 6.06378 6.78613 7.12738 7.65312 7.19705 7.6281 6.35399 7.20369 7.52304 7.16098 7.6371 6.28532 7.40214 7.06142 7.78444 7.9514 7.14334 7.52375 7.52657 7.11014 7.31903 7.64626 6.04277 7.18254 0 0 4.36918 6.81182 8.02207 7.71269 7.91696 7.29455 7.55989 7.47607 7.3679 7.52859 7.78067 7.20455 7.41483 7.34416 6.5603 6.17239 5.73125 0 7.75569 0 7.54945 7.29598 7.20602 7.69084 6.87522 7.08554 6.07488 7.19651 7.15047 7.26775 7.75827 7.54842 0 6.45033 6.04806 7.64443 6.72823 7.52703 7.31683 7.19552 7.48717 6.89708 7.08661 6.95188 7.93646 8.06342 7.44664 7.31101 7.37939 7.15794 0 8.02619 7.33975 6.87829 7.31443 7.91976 7.31588 8.0946 7.55371 6.89064 6.29082 6.75001 7.19246 7.40772 6.81753 7.07819 7.66996 6.99633 7.48298 7.74248 6.5329 7.23712 8.11474 7.80966 7.26297 7.95913 8.08821 6.62481 7.13389 7.74433 7.42164 6.96712 7.75221 6.85394 7.23672 7.47952 7.485 7.10319 7.63752 5.77827 6.05224 6.66046 7.49645 7.08231 7.26501 7.15683 6.27869 6.99235 7.34725 6.98813 6.80271 6.99738 ]
@@@ Frame-accuracy per-class: [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2.66667 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 12.2449 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2.46914 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1.7094 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1.86916 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5.12821 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4.87805 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.1746 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 22.2222 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 13.3333 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1.34228 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ]

LOG (nnet-train-frmshuff[5.5.294-06484]:main():nnet-train-frmshuff.cc:406) AvgLoss: 7.49555 (Xent), [AvgXent: 7.49555, AvgTargetEnt: 0]
progress: []
FRAME_ACCURACY >> 0.0345451% <<

